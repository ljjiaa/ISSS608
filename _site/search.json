[
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The aim of this exercise is to perform a makeover to improve the original visualisation completed by one of the classmates on exercise 1. In terms of the makeover, our approach is to critic the submission in terms of the clarity and aesthetic then remake the original design by using the data visualisation design principles and best practices we had learnt."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#overview",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#overview",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The aim of this exercise is to perform a makeover to improve the original visualisation completed by one of the classmates on exercise 1. In terms of the makeover, our approach is to critic the submission in terms of the clarity and aesthetic then remake the original design by using the data visualisation design principles and best practices we had learnt."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-preparation",
    "title": "Take-home Exercise 2",
    "section": "2 Data preparation",
    "text": "2 Data preparation\n\n2.1 Loading R packages\n\npacman::p_load(tidyverse, haven, patchwork, ggdist, ggrain, ggridges, quarto)\n\n\n\n2.2 Importing PISA data\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\nselect_df &lt;- stu_qqq_SG[c(3:4,7,26,1039,1167:1196)]\n\n\n\n2.3 Data cleaning\nTo ensure consistency in the dataset employed for make over, I refer to the code chunk used by the original author. However, I will not go into details as it is not the focus of this exercise.\nExpand the following session to show the code chunk.\n\n\nCode\n#Convert wide-format into long-format\nmath_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"MATH\"),\n    names_to = \"MATH\",\n    values_to = \"MATH_score\"\n  )\nmath_long &lt;- math_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nmath_long_selected &lt;- select(math_long, -contains(\"READ\"), -contains(\"SCIE\"))\n\nread_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"READ\"),\n    names_to = \"READ\",\n    values_to = \"READ_score\"\n  )\nread_long &lt;- read_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nread_long_selected &lt;- select(read_long, -contains(\"MATH\"), -contains(\"SCIE\"), -one_of(names(math_long_selected)))\n\nscie_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"SCIE\"),\n    names_to = \"SCIENCE\",\n    values_to = \"SCIENCE_score\"\n  )\nscie_long &lt;- scie_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nscie_long_selected &lt;- select(scie_long, -contains(\"MATH\"), -contains(\"READ\"), -one_of(names(math_long_selected)))\n\ncombined_long_df &lt;- bind_cols(math_long_selected, read_long_selected, scie_long_selected)\n\n\n\n\nCode\n#translate column names\nschool_map &lt;- c(\"SGP01\" = \"Public/Secondary\",\n                \"SGP02\" = \"Public/Post-secondary\",\n                \"SGP03\" = \"Private/Secondary\",\n                \"SGP97\" = \"Undisclosed\")\ngender_map &lt;- c(\"1\" = \"Female\",\n                \"2\" = \"Male\")\nclean_df &lt;- combined_long_df %&gt;%\n  mutate(SCHOOL = school_map[STRATUM],\n         GENDER = gender_map[ST004D01T],\n         Math = as.numeric(MATH_score),\n         Read = as.numeric(READ_score),\n         Science = as.numeric(SCIENCE_score),\n         ESC_status = round(((ESCS - min(ESCS, na.rm = TRUE))/\n                              (max(ESCS, na.rm = TRUE)-min(ESCS, na.rm = TRUE)))*100, digits = 0),\n         ESC_status = case_when(\n           ESC_status &gt;= 0 & ESC_status &lt; 25 ~ \"Low\",\n           ESC_status &gt;= 25 & ESC_status &lt; 50 ~ \"Lower-Middle\",\n           ESC_status &gt;= 50 & ESC_status &lt; 75 ~ \"Upper-Middle\",\n           ESC_status &gt;= 75 & ESC_status &lt;= 100 ~ \"High\",\n           TRUE ~ as.character(ESC_status)\n         ))\n\n#remove columns which are not used to plot\nclean_short_df &lt;- clean_df %&gt;%\n  select(-c(CNTSCHID, STRATUM, ST004D01T, ESCS, MATH_score, READ_score, SCIENCE_score))\n\n\n\n\nCode\n#translate column names\nschool_map &lt;- c(\"SGP01\" = \"Public/Secondary\",\n                \"SGP02\" = \"Public/Post-secondary\",\n                \"SGP03\" = \"Private/Secondary\",\n                \"SGP97\" = \"Undisclosed\")\ngender_map &lt;- c(\"1\" = \"Female\",\n                \"2\" = \"Male\")\nclean_df &lt;- combined_long_df %&gt;%\n  mutate(SCHOOL = school_map[STRATUM],\n         GENDER = gender_map[ST004D01T],\n         Math = as.numeric(MATH_score),\n         Read = as.numeric(READ_score),\n         Science = as.numeric(SCIENCE_score),\n         ESC_status = round(((ESCS - min(ESCS, na.rm = TRUE))/\n                              (max(ESCS, na.rm = TRUE)-min(ESCS, na.rm = TRUE)))*100, digits = 0),\n         ESC_status = case_when(\n           ESC_status &gt;= 0 & ESC_status &lt; 25 ~ \"Low\",\n           ESC_status &gt;= 25 & ESC_status &lt; 50 ~ \"Lower-Middle\",\n           ESC_status &gt;= 50 & ESC_status &lt; 75 ~ \"Upper-Middle\",\n           ESC_status &gt;= 75 & ESC_status &lt;= 100 ~ \"High\",\n           TRUE ~ as.character(ESC_status)\n         ))\n\n#remove columns which are not used to plot\nclean_short_df &lt;- clean_df %&gt;%\n  select(-c(CNTSCHID, STRATUM, ST004D01T, ESCS, MATH_score, READ_score, SCIENCE_score))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-critique-and-remake",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-critique-and-remake",
    "title": "Take-home Exercise 2",
    "section": "3 Visualization Critique and Remake",
    "text": "3 Visualization Critique and Remake\n\n3.1 Plot 1\nThe following plot illustrates the distribution of students’ performance across Mathematics, Science and English.\n\n\n3.1.1 Critique\nClarity\nMisleading Layout: Two of the plots are stacked on the left with one larger plot on the right. The arrangement may confuse the focus and hinder direct comparison, stacking them vertically would provide a clearer comparison across all subjects.\nInconsistencies in Axis Values: The x-axis and y-axis values are inconsistent across the plots, it hampers the ability to make direct comparisons, affecting the clarity of the comparative analysis.\nIndistinguishable Color Usage: The line colors for median, Q1, and Q3 are too similar, which make it hard to differentiate between these statistics.\nOverlapping Annotations: While displaying annotations and statistics on the graph can be informative, it is difficult for the user to read when the annotations overlap with the histogram\nAesthetics\nPlot Title Font Size: The plot title is in a bigger font size, which enhances the emphasis and readability.\nMinimalist Design: A minimalist approach ensures a clean look. Yet, using only monochromatic tone is less visually engaging.\nAnnotation Colors: Distinct colors for annotations enhance visual interest and draw attention to important figures.\n\n\n3.1.2 Sketch\n\n\n\n3.1.3 Make Over\n\n\nCode\n#Plot composite histograms for Math, Read, Science\n\np1 &lt;- ggplot(data = clean_short_df,\n             aes(x= Math))+\n  geom_histogram(bins=20,\n                 color = \"grey30\",\n                 fill=\"slategray1\",\n                 size = 0.5,\n                 alpha = 0.7) +\n  geom_vline(aes(xintercept=mean(Math)), \n             color=c(\"red\"), \n             size=1, \n             linetype=\"dashed\") +\n  annotate(\"text\", x = 400, y = 10000,\n           label = paste(\"Mean:\", \n                         round(mean(clean_short_df$Math), 2)),\n           color = \"red\", size = 4) +    \n  geom_vline(aes(xintercept=median(Math)), \n             color=c(\"blue\"), \n             size=1, \n             linetype=\"solid\") +\n  annotate(\"text\", x = 800, y = 10000,\n           label = paste(\"Median:\", \n                         round(median(clean_short_df$Math), 2)),\n           color = \"blue\", size = 4) +    \n  coord_cartesian(xlim=c(0,1000), ylim=c(0,12000)) +\n  geom_boxplot(width = 800,\n               fill = \"white\", \n               color = \"black\",\n               alpha = 0.6,\n               outlier.colour = \"grey30\",\n               outlier.fill = \"white\",\n               outlier.size = 2,\n               outlier.alpha = 0.3) +\n  theme_minimal() +\n  theme(axis.title.y = element_blank())\n\n\np2 &lt;- ggplot(data = clean_short_df,\n             aes(x= Read))+\n  geom_histogram(bins=20,\n                 color = \"grey30\",\n                 fill=\"slategray1\",\n                 size = 0.5,\n                 alpha = 0.7) +\n  geom_vline(aes(xintercept=mean(Read)), \n             color=c(\"red\"), \n             size=1, \n             linetype=\"dashed\") +\n  annotate(\"text\", x = 400, y = 10000,\n           label = paste(\"Mean:\", \n                         round(mean(clean_short_df$Read), 2)),\n           color = \"red\", size = 4) +    \n  geom_vline(aes(xintercept=median(Read)), \n             color=c(\"blue\"), \n             size=1, \n             linetype=\"solid\") +\n  annotate(\"text\", x = 800, y = 10000,\n           label = paste(\"Median:\", \n                         round(median(clean_short_df$Read), 2)),\n           color = \"blue\", size = 4) +    \n  coord_cartesian(xlim=c(0,1000), ylim=c(0,12000)) +\n  geom_boxplot(width = 800,\n               fill = \"white\", \n               color = \"black\",\n               alpha = 0.6,\n               outlier.colour = \"grey30\",\n               outlier.fill = \"white\",\n               outlier.size = 2,\n               outlier.alpha = 0.3) +\n  labs(y= \"No. of Students\") +\n  theme_minimal() \n\np3 &lt;- ggplot(data = clean_short_df,\n             aes(x= Science))+\n  geom_histogram(bins=20,\n                 color = \"grey30\",\n                 fill=\"slategray1\",\n                 size = 0.5,\n                 alpha = 0.7) +\n  geom_vline(aes(xintercept=mean(Science)), \n             color=c(\"red\"), \n             size=1, \n             linetype=\"dashed\") +\n  annotate(\"text\", x = 400, y = 10000,\n           label = paste(\"Mean:\", \n                         round(mean(clean_short_df$Science), 2)),\n           color = \"red\", size = 4) +    \n  geom_vline(aes(xintercept=median(Science)), \n             color=c(\"blue\"), \n             size=1, \n             linetype=\"solid\") +\n  annotate(\"text\", x = 800, y = 10000,\n           label = paste(\"Median:\", \n                         round(median(clean_short_df$Science), 2)),\n           color = \"blue\", size = 4) +    \n  coord_cartesian(xlim=c(0,1000), ylim=c(0,12000)) +\n  geom_boxplot(width = 800,\n               fill = \"white\", \n               color = \"black\",\n               alpha = 0.6,\n               outlier.colour = \"grey30\",\n               outlier.fill = \"white\",\n               outlier.size = 2,\n               outlier.alpha = 0.3) +\n  theme_minimal() +\n  theme(axis.title.y = element_blank())\n\n(p1 /  p2 / p3) + plot_annotation(title= \"Distribution of Performance across Subjects\",\n                    theme = theme(plot.title=element_text(size= 14, hjust= 0.5)))\n\n\n\n\n\nLayout improvement and consistent axis value: All three plots are stacked vertically with consistent x-axis and y-axis value. This provide a clear and direct comparison across all subjects.\nIncorporate box plot: Instead of changing the line colour for median, Q1 and Q3, a box plot is added along the x-axis, along with a mean line, simplifies the interpretation of central tendency, spread, and outliers, enhancing the visualization of key statistical data.\nAnnotations: To enhance readability and interpretation, annotations have been placed above the histogram with distinct colors, ensuring they do not overlap with the data and remain clearly visible to users.\nImproved Color Scheme: While maintaining a minimalist theme, the introduction of a blue fill to the histogram created color contrast, improving both visual appeal and data differentiation.\n\n\n\n3.2 Plot 2\nThis combined density plot overlays the distributions of math performance for different genders across three subjects.\n\n\n3.2.1 Critique\nClarity\nTitle : The title is too generic and does not provide enough context about the plot.\nPlot type: Good choice of density plot for understanding the distribution of data.\nMisleading Layout: Two plots are stacked on the left with one larger plot on the right. The arrangement may confuse the focus and hinder direct comparison, stacking them vertically would provide a clearer comparison across all subjects.\nAxis Label: When the plots are stacked together, using only one label enhances the clarity and neatness of the plot\nAxis scale: Well-used of consistent x-axis scale which enhances the comparability between subjects.\nConsistency: Ensure all plots consistently include or exclude annotations for comparative analysis.\nAesthetic\nMinimalist Design: A minimalist approach ensures a clean look, focusing on essential data.\nColor Choice: Effective use of colour to distinguish between male and female performance distribution.\n\n\n3.2.2 Sketch\n\n\n\n3.2.3 Make Over\n\n\nCode\n##Gender and Subject\n#1. Math\n\n\np4 &lt;- ggplot(data = clean_short_df, \n             aes(x = Math, \n                 fill = GENDER)) +\n  geom_density(alpha = 0.3) +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 0.005)) +\n  theme_minimal()+\n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_blank())\n\n\np5 &lt;- ggplot(data = clean_short_df, \n             aes(x = Read,\n                 fill = GENDER)) +\n  geom_density(alpha=0.3) +\n  coord_cartesian(xlim = c(0, 1000)) +\n  theme_minimal() \n\n\np6 &lt;- ggplot(data = clean_short_df, \n             aes(x = Science,\n                 fill = GENDER)) +\n  geom_density(alpha=0.3) +\n  coord_cartesian(xlim = c(0, 1000)) +\n  theme_minimal()+\n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_blank())\n\n\n\n(p4 / p5 /p6) + plot_annotation(title= \n                                  \"Gender-Based Performance Comparison Across Subjects\",\n                    theme = theme(plot.title=element_text(size= 14, hjust= 0.5)))\n\n\n\n\n\nClarity of Title: Improved the title to provide more context about the plot, making it more informative and engaging.\nLayout improvement and consistent axis value: Reorganized the layout by stacking all plots vertically with consistent x-axis and y-axis value. This provide a clear and direct comparison across all subjects.\nConsistency in Annotations: Ensured that all plots consistently exclude annotations for comparative analysis. This consistency allows users to better understand and interpret the data.\n\n\n\n3.3 Plot 3\nIn the following plot, boxplot is used to examine performance on each subject in terms of school types.\n\n\n3.3.1 Critique\nClarity\nTitle : The title is too generic and does not provide enough context about the plot.\nPlot type and layout : Good choice of box plot and side-by-side arrangement for comparing the performance of private and public school across different subjects.\nAxis Title and label: The x-axis label is repetitive, consider simplifying it to enhance the clarity and help in comparison.\nAxis Values: The y-axis values are inconsistent across the plots, which limits the ability to make direct comparison.\nSubtitle : Instead of using subject as y-axis label, placing it on top of each plot as subtitle would enhance overall clarity.\nData Points: Outliers are identified, which is helpful to understand the distribution. However, as they are stacked together, it’s difficult to interpret the outliers’ distribution and significance.\nAesthetic\nSimplicity: The design is simple, which is good for clarity but can be less engaging.\nConsistency: The styling is consistent across all plots.\n\n\n3.3.2 Sketch\n\n\n\n3.3.3 Make Over\n\n\nCode\n# Deriving mean value\nMath_school &lt;- clean_short_df %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise(\n    Freq = n(),\n    Mean = mean(Math, na.rm= TRUE))\n\nRead_school &lt;- clean_short_df %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise(\n    Freq = n(),\n    Mean = mean(Read, na.rm= TRUE))\n\nScience_school &lt;- clean_short_df %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise(\n    Freq = n(),\n    Mean = mean(Science, na.rm= TRUE))\n\n# Ploting boxplot\n\np7 &lt;- ggplot(data= clean_short_df,\n       aes(x= SCHOOL, y= Math, color = SCHOOL)) +\n  geom_boxplot(width= 0.3, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.3, outlier.shape = 19) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=2) +  coord_cartesian(ylim = c(0,1000)) +\n  geom_text(data = Math_school,\n            aes(x = SCHOOL, y=Mean, label = paste(\"Mean:\\n\", round(Mean,2))),\n            color = \"red\", \n            hjust = -0.3, \n            vjust = -2, \n            size= 2.75)+  \n  scale_color_manual(values=c(\"steelblue\", \"goldenrod\")) +\n  theme_minimal() +\n  labs(title=\"Mathematics\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 8)) \n\np8 &lt;- ggplot(data= clean_short_df,\n       aes(x= SCHOOL, y= Read, color = SCHOOL)) +\n  geom_boxplot(width= 0.3, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.3, outlier.shape = 19) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=2) +  coord_cartesian(ylim = c(0,1000)) +\n  geom_text(data = Read_school,\n            aes(x = SCHOOL, y=Mean, label = paste(\"Mean:\\n\", round(Mean,2))),\n            color = \"red\", \n            hjust = -0.3, \n            vjust = -2, \n            size= 2.75)+  \n  scale_color_manual(values=c(\"steelblue\", \"goldenrod\")) +\n  theme_minimal() +\n  labs(title=\"Reading\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10))   \n\np9 &lt;- ggplot(data= clean_short_df,\n       aes(x= SCHOOL, y= Science, color = SCHOOL)) +\n  geom_boxplot(width= 0.3, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.3, outlier.shape = 19) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=2) +  coord_cartesian(ylim = c(0,1000)) +\n  geom_text(data = Science_school,\n            aes(x = SCHOOL, y=Mean, label = paste(\"Mean:\\n\", round(Mean,2))),\n            color = \"red\", \n            hjust = -0.3, \n            vjust = -2, \n            size= 2.75)+  \n  scale_color_manual(values=c(\"steelblue\", \"goldenrod\")) +\n  theme_minimal() +\n  labs(title=\"Science\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10))  \n\n\np7 + p8 + p9 + \n  plot_layout(guides = \"collect\") +\n  plot_annotation(title= \"School-Based Performance Comparison Across Subjects\",\n                    theme = theme(plot.title=element_text(size= 14, hjust= 0.5)))\n\n\n\n\n\nTitle: The title has been revised to provide more context about the plot.\nAesthetic and Engagement: Different colors are assigned to each school type, not only enhancing visual appeal but also make it easier to distinguish between the two categories.\nAxis Title and label: Repetitive x-axis labels have been removed and a clear legend has been used to improve clarity and facilitate comparison.\nConsistency in Axes: The y-axis values were standardized for all plots, which enabled direct comparison across all subjects.\nSubtitle: Placing subject names as subtitles provides clear identification for each plot.\nData Points: The opacity of the outliers are adjusted to enhance clarity, and mean values are highlighted with red dots and annotation for easy reference.\n\n\n\n3.4 Plot 4\nThe following plot reveals the relationship between student performance and socioeconomic status across different subjects.\n\n\n3.4.1 Critique\nClarity\nTitle : The title is too generic and does not provide enough context about the plot.\nPlot type and layout: Good choice of box plot and side-by-side arrangement for comparing the performance of various socioeconomic status across different subjects.\nAxis Title and label: The x-axis label is repetitive, consider simplifying it to enhance the clarity and help in comparison. Incorrectly ordered of x-axis data, can mislead users and leading to potential misinterpretations.\nAxis Values: The y-axis values are inconsistent across the plots, which limits the ability to make direct comparisons.\nSubtitle : Instead of using subject as y-axis label, placing it on top of each plot as subtitle would enhance overall clarity.\nData Points: Outliers are identified, which is helpful to understand the distribution. However, as they are stacked together, it’s difficult to interpret the outliers’ distribution and significance.\nAesthetic\nSimplicity: The design is simple, which is good for clarity but can be less engaging.\nConsistency: The styling is consistent across all plots.\n\n\n3.4.2 Sketch\n\n\n\n3.4.3 Make Over\n\n\nCode\nclean_short_df$ESC_status &lt;- factor(clean_short_df$ESC_status, levels = c(\"Low\", \"Lower-Middle\", \"Upper-Middle\", \"High\"))\n\np10 &lt;- ggplot(data= na.omit(clean_short_df),\n       aes(x= ESC_status, y= Math, color = ESC_status)) +\n  geom_boxplot(width= 0.3, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.3, outlier.shape = 19) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=2) +\n  coord_cartesian(ylim = c(0,1000)) +\n  theme_minimal() +\n  labs(title=\"Mathematics\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 8)) + \n  labs(color = \"ESCS status\") \n\np11 &lt;- ggplot(data= na.omit(clean_short_df),\n       aes(x= ESC_status, y= Read, color = ESC_status)) +\n  geom_boxplot(width= 0.3, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.3, outlier.shape = 19) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=2) +  \n  coord_cartesian(ylim = c(0,1000)) +\n  theme_minimal() +\n  labs(title=\"Reading\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) + \n  labs(color = \"ESCS status\")   \n\np12 &lt;- ggplot(data= na.omit(clean_short_df),\n       aes(x= ESC_status, y= Science, color = ESC_status)) +\n  geom_boxplot(width= 0.3, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.3, outlier.shape = 19) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=2) +  \n  coord_cartesian(ylim = c(0,1000)) +\n  theme_minimal() +\n  labs(title=\"Science\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) + \n  labs(color = \"ESCS status\")\n\n\np10 + p11 + p12 + \n  plot_layout(guides = \"collect\") +\n  plot_annotation(title= \"Socioeconomic-Based Performance Comparison Across Subjects\",\n                    theme = theme(plot.title=element_text(size= 14, hjust= 0.5)))\n\n\n\n\n\nTitle: The title has been revised to provide more context about the plot.\nAesthetic and Engagement: Different colors are assigned to each socioeconomic status, not only enhancing visual appeal but also make it easier to distinguish between the categories.\nAxis Title and label: Repetitive x-axis labels have been removed and a clear legend has been used to improve clarity and facilitate comparison. The x-axis data has been corrected to represent the categories in a logical sequence, ensuring clear and accurate comparisons and preventing any potential misinterpretation.\nConsistency in Axes: The y-axis values were standardized for all plots, which enabled direct comparison across all subjects.\nSubtitle: Placing subject names as subtitles provides clear identification for each plot.\nData Points: The opacity of the outliers are adjusted to enhance clarity, and mean values are highlighted with red dots for easy reference.\n\n\nCode\n&lt;!-- Include the pop-up template --&gt;\n&lt;iframe src=\"popup_template.html\" frameborder=\"0\" width=\"100%\" height=\"400\"&gt;&lt;/iframe&gt;\n\n\nTo view the comparison, click here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-Visual Analytics and Applications",
    "section": "",
    "text": "Welcome to my VAA learning space\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIn-class Exercise 9\n\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nIn-class Exercise 7\n\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nTake-home Exercise 4\n\n\nPrototyping Modules for Visual Analytics Shiny Application\n\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 8\n\n\nModelling, Visualising and Analysing Network Data with R\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 7a\n\n\nChoropleth Mapping with R\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 7b\n\n\nVisualising Geospatial Point Data\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 7c\n\n\nAnalytical Mapping\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nIn-class Exercise 6: Horizon Plot\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nLim Jia Jia\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 6\n\n\nVisualising and Analysing Time-oriented Data\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n  \n\n\n\n\nTake-home Exercise 3\n\n\nBe Weatherwise or Otherwise\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 5c\n\n\nHeatmap for Visualising and Analysing Multivariate Data\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 5d\n\n\nVisual Multivariate Analysis with Parallel Coordinates Plot\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 5e\n\n\nTreemap Visualisation with R\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 5\n\n\nVisual Multivariate Analysis\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 5b\n\n\nVisual Correlation Analysis\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 4\n\n\nFundamentals of Visual Analytics\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\n\n\n\n\n  \n\n\n\n\nTake-home Exercise 2\n\n\nDataVis Makeover\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 3\n\n\nProgramming Interactive Data Visualisation and Animated Statistical Graphics with R\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\n\n\n\n\n  \n\n\n\n\nTake-home Exercise 1\n\n\nCreating data visualisation beyond default\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 2\n\n\nBeyond ggplot2 Fundamentals\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n  \n\n\n\n\nIn-class Exercise 1: Now You See It!\n\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 1\n\n\nA Layered Grammar of Graphics: ggplot2 methods\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "In this hands-on exercise, we learn to design treemap using appropriate R packages. The hands-on exercise consists of three main section:\n\nhow to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package.\nhow to plot static treemap by using treemap package.\nhow to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#overview",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "In this hands-on exercise, we learn to design treemap using appropriate R packages. The hands-on exercise consists of three main section:\n\nhow to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package.\nhow to plot static treemap by using treemap package.\nhow to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#loading-r-packages",
    "title": "Hands-on Exercise 5e",
    "section": "2 Loading R packages",
    "text": "2 Loading R packages\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#data-wrangling",
    "title": "Hands-on Exercise 5e",
    "section": "3 Data Wrangling",
    "text": "3 Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal of Urban Redevelopment Authority (URA).\n\n3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nRows: 23205 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): Project Name, Address, Type of Area, Nett Price($), Sale Date, Pro...\ndbl  (8): No. of Units, Area (sqm), Transacted Price ($), Unit Price ($ psm)...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nrealis2018\n\n# A tibble: 23,205 × 20\n   `Project Name`  Address            `No. of Units` `Area (sqm)` `Type of Area`\n   &lt;chr&gt;           &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;         \n 1 ADANA @ THOMSON 8 Old Upper Thoms…              1           52 Strata        \n 2 ALANA           156 Sunrise Terra…              1          284 Strata        \n 3 ALANA           104 Sunrise Terra…              1          256 Strata        \n 4 ALANA           126 Sunrise Terra…              1          256 Strata        \n 5 ATELIER VILLAS  43 Yio Chu Kang D…              1          277 Strata        \n 6 ATELIER VILLAS  11 Yio Chu Kang D…              1          285 Strata        \n 7 BANYAN VILLAS   30 Lentor Plain                 1          234 Land          \n 8 BANYAN VILLAS   18 Lentor Plain                 1          155 Land          \n 9 BULLION PARK    170 Lentor Loop  …              1          115 Strata        \n10 BULLION PARK    166 Lentor Loop  …              1          117 Strata        \n# ℹ 23,195 more rows\n# ℹ 15 more variables: `Transacted Price ($)` &lt;dbl&gt;, `Nett Price($)` &lt;chr&gt;,\n#   `Unit Price ($ psm)` &lt;dbl&gt;, `Unit Price ($ psf)` &lt;dbl&gt;, `Sale Date` &lt;chr&gt;,\n#   `Property Type` &lt;chr&gt;, Tenure &lt;chr&gt;, `Completion Date` &lt;chr&gt;,\n#   `Type of Sale` &lt;chr&gt;, `Purchaser Address Indicator` &lt;chr&gt;,\n#   `Postal District` &lt;dbl&gt;, `Postal Sector` &lt;dbl&gt;, `Postal Code` &lt;dbl&gt;,\n#   `Planning Region` &lt;chr&gt;, `Planning Area` &lt;chr&gt;\n\n\nThe output tibble data.frame is called realis2018.\n\n\n3.2 Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame. Refer to Introduction to dplyr for more details.\n\n\n3.3 Grouped summaries without the Pipe\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\n\nAggregation functions such as sum() and median() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n3.4 Grouped summaries with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%: To learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 5e",
    "section": "4 Designing Treemap with treemap Package",
    "text": "4 Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments.\n\n4.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n4.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely:\n\nindex\nvSize\nvColor\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the three arguments used\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n4.3 Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the code chunk above\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\n\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\n4.4 Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n4.4.1 The “value” type treemap\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the code chunk above\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\n4.4.2 The “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the code chunk above\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n4.5 Treemap Layout\ntreemap() supports two popular treemap layouts, namely:\n\nsquarified\npivotSize (default)\n\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n4.6 Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n4.7 Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 5e",
    "section": "5 Designing Treemap using treemapify Package",
    "text": "5 Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Refer to Introduction to “treemapify” and user guide for more details.\n\n5.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n5.2 Defining hierarchy\n\nGroup by Planning RegionGroup by Planning AreaAdding boundary line\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 5e",
    "section": "6 Designing Interactive Treemap using d3treeR",
    "text": "6 Designing Interactive Treemap using d3treeR\n\n6.1 Installing d3treeR package\nThis slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\ninstall.packages(\"devtools\")\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n6.2 Designing An Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rows and colouring the cells within the table.\nHeatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#overview",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rows and colouring the cells within the table.\nHeatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#loading-r-packages",
    "title": "Hands-on Exercise 5c",
    "section": "2 Loading R packages",
    "text": "2 Loading R packages\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#data-preparation",
    "title": "Hands-on Exercise 5c",
    "section": "3 Data preparation",
    "text": "3 Data preparation\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe output tibbled data frame is called wh.\n\n\n3.2 Preparing the data\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\n\nwh\n\n# A tibble: 156 × 12\n   Country        Region `Happiness score` `Whisker-high` `Whisker-low` Dystopia\n * &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 Albania        Centr…              4.59           4.70          4.48     1.46\n 2 Bosnia and He… Centr…              5.13           5.22          5.04     1.88\n 3 Bulgaria       Centr…              4.93           5.02          4.84     1.22\n 4 Croatia        Centr…              5.32           5.40          5.24     1.77\n 5 Czech Republic Centr…              6.71           6.78          6.64     2.49\n 6 Estonia        Centr…              5.74           5.82          5.66     1.46\n 7 Hungary        Centr…              5.62           5.70          5.54     1.97\n 8 Kosovo         Centr…              5.66           5.76          5.57     2.26\n 9 Latvia         Centr…              5.93           6.00          5.86     2.14\n10 Lithuania      Centr…              5.95           6.04          5.87     2.13\n# ℹ 146 more rows\n# ℹ 6 more variables: `GDP per capita` &lt;dbl&gt;, `Social support` &lt;dbl&gt;,\n#   `Healthy life expectancy` &lt;dbl&gt;, `Freedom to make life choices` &lt;dbl&gt;,\n#   Generosity &lt;dbl&gt;, `Perceptions of corruption` &lt;dbl&gt;\n\n\nNotice that the row number has been replaced into the country name.\n\n\n3.3 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#static-heatmap-using-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#static-heatmap-using-heatmap",
    "title": "Hands-on Exercise 5c",
    "section": "4 Static Heatmap using heatmap()",
    "text": "4 Static Heatmap using heatmap()\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\n\nWe will be focused on using heatmap() of R Stats package.\n\n4.1 heatmap() of R Stats\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",           # define the scale, can be column or row\n                      cexRow = 0.6,            # adjust the row label font size\n                      cexCol = 0.6,            # adjust the column label font size\n                      margins = c(8, 4),    # adjust the margin so that the label is shown properly\n                      col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#interactive-heatmap-usign-heatmaply",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#interactive-heatmap-usign-heatmaply",
    "title": "Hands-on Exercise 5c",
    "section": "5 Interactive Heatmap usign heatmaply()",
    "text": "5 Interactive Heatmap usign heatmaply()\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili. Refer to Introduction to Heatmaply and user manual for more details.\n\n5.1 Working with heatmaply\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])     # remove these columns\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n5.2 Data trasformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely:\n\nscale\nnormalise\npercentilse\n\n\nScaling methodNormalising methodPercentising method\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\n- In such a case, each value would reflect the distance from the mean in units of standard deviation. - The scale argument in heatmaply() supports column and row scaling.\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\",\n          main = \"Data transformation using 'scale'\",\n          fontsize_col = 7,\n          fontsize_row = 5)\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. - This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n\n5.3 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\nManual approachStatistical approach\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\n\n5.4 Seriation\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\n4 seriation algorithm:\n\nOptimal Leaf Ordering (OLO) starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. The default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)).\nGW (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\nmean gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\nnone gives us the dendrograms without any rotation that is based on the data matrix.\n\n\nOLOGWmeannone\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\n\n5.5 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blue-Red colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = 'Blue-Red')\n\n\n\n\n\n\n\n5.6 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\n\nIn the code chunk below the following arguments are used:\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Spectral,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\n\n\n\nR packages for Ternary Diagram\n\n\nggtern - ggplot extension for plotting static ternary diagrams\n\nPlotly R - R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\n\n\n\nCode\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used.\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n\nCode\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\nBefore we make any changes, let us examine the data table.\n\nFirst 5 rowssummarystructure\n\n\n\n\nCode\nhead(pop_data,5)\n\n\n# A tibble: 5 × 5\n  PA         SZ                     AG      Year Population\n  &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2011        290\n2 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2012        270\n3 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2013        260\n4 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2014        250\n5 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2015        260\n\n\n\n\n\n\nCode\nsummary(pop_data)\n\n\n      PA                 SZ                 AG                 Year     \n Length:108126      Length:108126      Length:108126      Min.   :2000  \n Class :character   Class :character   Class :character   1st Qu.:2004  \n Mode  :character   Mode  :character   Mode  :character   Median :2009  \n                                                          Mean   :2009  \n                                                          3rd Qu.:2014  \n                                                          Max.   :2018  \n   Population     \n Min.   :    0.0  \n 1st Qu.:    0.0  \n Median :  140.0  \n Mean   :  644.1  \n 3rd Qu.:  800.0  \n Max.   :14560.0  \n\n\n\n\n\n\nCode\nstr(pop_data)\n\n\nspc_tbl_ [108,126 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PA        : chr [1:108126] \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ        : chr [1:108126] \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ AG        : chr [1:108126] \"AGE0-4\" \"AGE0-4\" \"AGE0-4\" \"AGE0-4\" ...\n $ Year      : num [1:108126] 2011 2012 2013 2014 2015 ...\n $ Population: num [1:108126] 290 270 260 250 260 250 200 180 290 290 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PA = col_character(),\n  ..   SZ = col_character(),\n  ..   AG = col_character(),\n  ..   Year = col_double(),\n  ..   Population = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nCode\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  pivot_wider(names_from = AG, values_from = Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\nWhat did code chunk above do?\n\nConvert Year to character\nUse pivot_wider() (replace spread()) to widens data\nSum across columns using rowSums()\nFilter data for Year equal to 2018\nFilter data of value greater than 0\n\n\n\nagpop_mutated\n\n# A tibble: 234 × 25\n   PA         SZ      Year  `AGE0-4` `AGE10-14` `AGE15-19` `AGE20-24` `AGE25-29`\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo… 2018       180        320        300        260        300\n 2 Ang Mo Kio Cheng … 2018      1060       1080       1260       1400       1880\n 3 Ang Mo Kio Chong … 2018       900       1030       1220       1380       1760\n 4 Ang Mo Kio Kebun … 2018       720       1010       1120       1230       1460\n 5 Ang Mo Kio Sembaw… 2018       220        380        500        550        500\n 6 Ang Mo Kio Shangr… 2018       550        670        780        950       1080\n 7 Ang Mo Kio Tagore  2018       260        430        500        640        690\n 8 Ang Mo Kio Townsv… 2018       830        930        860       1020       1400\n 9 Ang Mo Kio Yio Ch… 2018       160        220        260        350        340\n10 Ang Mo Kio Yio Ch… 2018       810       1300       1450       1500       1590\n# ℹ 224 more rows\n# ℹ 17 more variables: `AGE30-34` &lt;dbl&gt;, `AGE35-39` &lt;dbl&gt;, `AGE40-44` &lt;dbl&gt;,\n#   `AGE45-49` &lt;dbl&gt;, `AGE05-9` &lt;dbl&gt;, `AGE50-54` &lt;dbl&gt;, `AGE55-59` &lt;dbl&gt;,\n#   `AGE60-64` &lt;dbl&gt;, `AGE65-69` &lt;dbl&gt;, `AGE70-74` &lt;dbl&gt;, `AGE75-79` &lt;dbl&gt;,\n#   `AGE80-84` &lt;dbl&gt;, AGE85over &lt;dbl&gt;, YOUNG &lt;dbl&gt;, ACTIVE &lt;dbl&gt;, OLD &lt;dbl&gt;,\n#   TOTAL &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nInstead of long and thin, now the data table is fat and short!\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\nDefaultModified\n\n\n\n# Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n# Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_tropical()                      # ggtern_themes\n\n\n\n\n\n\n\n\nRefer to this link for more ggtern themes.\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n\nCode\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"lightblue\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#overview",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\n\n\n\nR packages for Ternary Diagram\n\n\nggtern - ggplot extension for plotting static ternary diagrams\n\nPlotly R - R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\n\n\n\nCode\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used.\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n\nCode\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\nBefore we make any changes, let us examine the data table.\n\nFirst 5 rowssummarystructure\n\n\n\n\nCode\nhead(pop_data,5)\n\n\n# A tibble: 5 × 5\n  PA         SZ                     AG      Year Population\n  &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2011        290\n2 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2012        270\n3 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2013        260\n4 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2014        250\n5 Ang Mo Kio Ang Mo Kio Town Centre AGE0-4  2015        260\n\n\n\n\n\n\nCode\nsummary(pop_data)\n\n\n      PA                 SZ                 AG                 Year     \n Length:108126      Length:108126      Length:108126      Min.   :2000  \n Class :character   Class :character   Class :character   1st Qu.:2004  \n Mode  :character   Mode  :character   Mode  :character   Median :2009  \n                                                          Mean   :2009  \n                                                          3rd Qu.:2014  \n                                                          Max.   :2018  \n   Population     \n Min.   :    0.0  \n 1st Qu.:    0.0  \n Median :  140.0  \n Mean   :  644.1  \n 3rd Qu.:  800.0  \n Max.   :14560.0  \n\n\n\n\n\n\nCode\nstr(pop_data)\n\n\nspc_tbl_ [108,126 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PA        : chr [1:108126] \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ        : chr [1:108126] \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ AG        : chr [1:108126] \"AGE0-4\" \"AGE0-4\" \"AGE0-4\" \"AGE0-4\" ...\n $ Year      : num [1:108126] 2011 2012 2013 2014 2015 ...\n $ Population: num [1:108126] 290 270 260 250 260 250 200 180 290 290 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PA = col_character(),\n  ..   SZ = col_character(),\n  ..   AG = col_character(),\n  ..   Year = col_double(),\n  ..   Population = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nCode\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  pivot_wider(names_from = AG, values_from = Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\nWhat did code chunk above do?\n\nConvert Year to character\nUse pivot_wider() (replace spread()) to widens data\nSum across columns using rowSums()\nFilter data for Year equal to 2018\nFilter data of value greater than 0\n\n\n\nagpop_mutated\n\n# A tibble: 234 × 25\n   PA         SZ      Year  `AGE0-4` `AGE10-14` `AGE15-19` `AGE20-24` `AGE25-29`\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo… 2018       180        320        300        260        300\n 2 Ang Mo Kio Cheng … 2018      1060       1080       1260       1400       1880\n 3 Ang Mo Kio Chong … 2018       900       1030       1220       1380       1760\n 4 Ang Mo Kio Kebun … 2018       720       1010       1120       1230       1460\n 5 Ang Mo Kio Sembaw… 2018       220        380        500        550        500\n 6 Ang Mo Kio Shangr… 2018       550        670        780        950       1080\n 7 Ang Mo Kio Tagore  2018       260        430        500        640        690\n 8 Ang Mo Kio Townsv… 2018       830        930        860       1020       1400\n 9 Ang Mo Kio Yio Ch… 2018       160        220        260        350        340\n10 Ang Mo Kio Yio Ch… 2018       810       1300       1450       1500       1590\n# ℹ 224 more rows\n# ℹ 17 more variables: `AGE30-34` &lt;dbl&gt;, `AGE35-39` &lt;dbl&gt;, `AGE40-44` &lt;dbl&gt;,\n#   `AGE45-49` &lt;dbl&gt;, `AGE05-9` &lt;dbl&gt;, `AGE50-54` &lt;dbl&gt;, `AGE55-59` &lt;dbl&gt;,\n#   `AGE60-64` &lt;dbl&gt;, `AGE65-69` &lt;dbl&gt;, `AGE70-74` &lt;dbl&gt;, `AGE75-79` &lt;dbl&gt;,\n#   `AGE80-84` &lt;dbl&gt;, AGE85over &lt;dbl&gt;, YOUNG &lt;dbl&gt;, ACTIVE &lt;dbl&gt;, OLD &lt;dbl&gt;,\n#   TOTAL &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nInstead of long and thin, now the data table is fat and short!\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\nDefaultModified\n\n\n\n# Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n# Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_tropical()                      # ggtern_themes\n\n\n\n\n\n\n\n\nRefer to this link for more ggtern themes.\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n\nCode\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"lightblue\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "Summary\nThis hands-on exercise consist of two main topic, namely:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#Section1",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#Section1",
    "title": "Hands-on Exercise 3",
    "section": "1 Programming Interactive Data Visualisation with R",
    "text": "1 Programming Interactive Data Visualisation with R\n\n1.1 Loading R packages\n\npacman::p_load(ggiraph, plotly,\n               patchwork, DT, tidyverse)\n\n\nR packages for Interactive Data\n\nggiraph ：making ‘ggplot’ graphics interactive.\nplotly ： R library for plotting interactive statistical graphs.\nDT ： provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse ： a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork ： combining multiple ggplot2 graphs into one figure.\n\n\n\n\n1.2 Importing the Data\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n1.3 Overview of the data\n\nexam_data\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\n\n1.4 Interactive Data Visualisation - ggiraph methods\n\nggiraph makes ‘ggplot’ graphics interactive with these arguments.\n\nTooltip : tooltips to be displayed when mouse is over elements.\nData_id: id to be associated with elements (used for hover and click actions).\n\nOnclick: JavaScript function to be executed when elements are clicked.\n\n\n\n1.4.1 Tooltip effect with tooltip aesthetic\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(                          # Create basic graph\n    aes(tooltip = ID),                               # specify tooltip here\n    stackgroups = TRUE,                             \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(ggobj = p,                                 # generate svg object on an html page.\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nHover over a dot to check out the student’s ID\n\n\nDisplaying multiple information on tooltip\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n# create a new field called tooltip with desired data \nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID,         \n                              \"\\n Class = \", exam_data$CLASS,\n                              \"\\n Gender = \", exam_data$GENDER)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip),      # newly created field used as tooltip field \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(ggobj = p,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nHover over a dot. Now, more information is shown!\n\n\nCustomising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ntooltip_css &lt;- \n\"background-color:grey;                     \nfont-style:bold; \ncolor:black;\nfont-size: 1.2em\"                     # customise tooltip css\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(ggobj = p,                             \n       width_svg = 6,                         \n       height_svg = 6*0.618,\n       options = list(opts_tooltip(css = tooltip_css)))  # add the tooltip_css here                                      \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nbackground colour of the tooltip is grey and the font colour is white and bold.\n\n\nDisplaying statistics on tooltip\nUsing stat_summary(), a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ntooltip &lt;- function(y, ymax, accuracy = .01) \n  {mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),) +\n  \n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(tooltip(y, ymax))),        # adding tool tip\n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\") +\n  \n  stat_summary(aes(y = MATHS),                        # adding error bar\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.1, size = 0.2) +\n  theme_minimal()\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n1.4.2 Hover effect with data_id\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  \n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),                        # specify data_id here       \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +\n  \n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(ggobj = p,                             \n       width_svg = 6,                         \n       height_svg = 6*0.618)                                             \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. The default color is orange.\n\n\nStyling hover effect\n\nCustomize highlighting effect\n\nusing opts_hover() for effect on geometries\nusing opts_hover_inv for effect on other geometries\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  \n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),                        # specify data_id here       \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +\n  \n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(ggobj = p,                             \n       width_svg = 6,                         \n       height_svg = 6*0.618,\n       options = list(opts_hover(css = \"fill: blue;\"),          # effect on geometries\n                      opts_hover_inv(css = \"opacity:0.2;\")))    # effect on other geometries\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\n\n1.4.3 Combining tooltip and hover effect\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  \n  geom_dotplot_interactive(           \n    aes(tooltip = CLASS,                          # specify tooltip here  \n        data_id =  CLASS),                        # specify data_id here       \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +\n  \n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(ggobj = p,                             \n       width_svg = 6,                         \n       height_svg = 6*0.618,\n       options = list(opts_hover(css = \"fill: blue;\"),          # effect on geometries\n                      opts_hover_inv(css = \"opacity:0.2;\")))    # effect on other geometries\n\n\n\n\n\n\n\n\n\n\nInteractivity\n\n\n\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show multiple information.\n\n\n\n\n1.4.4 Coordinated Multiple Views\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n# create a new field called tooltip with desired data \nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID,         \n                              \"\\n Class = \", exam_data$CLASS,\n                              \"\\n Gender = \", exam_data$GENDER)) \n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  \n  geom_dotplot_interactive(              \n    aes(tooltip = exam_data$tooltip,\n        data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  \n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS,\n        data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(code = print(p1 / p2), \n       width_svg = 6,\n       height_svg = 6,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\"))) \n\n\n\n\n\n\n\n\n\n\nInteractivity\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\n\n\n\n\n1.4.5 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  \n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  \n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n  theme_minimal()\n\ngirafe(ggobj = p,                             \n       width_svg = 6,                         \n       height_svg = 6*0.618)   \n\n\n\n\n\n\n\n\n\n\nInteractivity\n\n\n\nWeb document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n1.5 Interactive Data Visualisation - plotly methods\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly()\nby using ggplotly()\n\n\n\n1.5.1 Using plot_ly()\n\nbasicadding color\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\n\n\n\n1.5.2 Using ggplotly()\n\nAppropriate ggplot2 functions are used to create a scatter plot.\nggplotly() is used to convert the R graphic object into interactive object.\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)                                   # add this line\n\n\n\n\n\n\n1.5.3 Coordinated Multiple Views with ggplotly()\nThree steps for creating coordinated linked plot:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nsubplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data)                        # Step 1 \n\np1 &lt;- ggplot(data=d,                                 # Step 2\n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) \n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\nsubplot(ggplotly(p1),                              # Step 3\n        ggplotly(p2))\n\n\n\n\n\n\n\n1.6 Interactive Data Visualisation - crosstalk methods\n\n\nCrosstalk is an add-on to the htmlwidgets package.\nIt extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n\n\n1.6.1 Interactive Data Table: DT package\nDT package allow rendering of data objects as HTML tables.\n\nDT::datatable(exam_data[c(\"ID\",\"CLASS\",\"GENDER\",\"RACE\",\"ENGLISH\",\"MATHS\",\"SCIENCE\")], class= \"compact\")\n\n\n\n\n\n\n\n\n1.6.2 Linked brushing: crosstalk method\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \n\np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nThings to learn from the code chunk:\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#Section2",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#Section2",
    "title": "Hands-on Exercise 3",
    "section": "2 Programming Animated Statistical Graphics with R",
    "text": "2 Programming Animated Statistical Graphics with R\n\n2.1 Loading R packages\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\nR packages for Animated plot\n\nplotly : plotting interactive statistical graphs.\ngganimate : creating animated statistical graphs.\ngifski : converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse : a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\n\n\n2.2 Importing the Data\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  \n# change \"Country\" and \"Continent\" (aka col) as factor \n  mutate(across(col, as.factor)) %&gt;%  \n  \n# change \"Year\" as integer   \n  mutate(Year = as.integer(Year))          \n\nAlternatively, use mutate_all()\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  \n# change \"Country\" and \"Continent\" (aka col) as factor \n  mutate_at(col, as.factor) %&gt;%  \n  \n# change \"Year\" as integer   \n  mutate(Year = as.integer(Year))          \n\n\nThings to learn from the code chunk above\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate() of dplyr package is used to create new columns or modify columns that are functions of existing variables.\nacross() apply the same functions to multiple columns\nmutate_at() convert all character data type columns into factor.\n\n\n\n\n2.3 Overview of the data\n\nglobalPop\n\n# A tibble: 6,204 × 6\n   Country      Year Young   Old Population Continent\n   &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;    \n 1 Afghanistan  1996  83.6   4.5     21560. Asia     \n 2 Afghanistan  1998  84.1   4.5     22913. Asia     \n 3 Afghanistan  2000  84.6   4.5     23898. Asia     \n 4 Afghanistan  2002  85.1   4.5     25268. Asia     \n 5 Afghanistan  2004  84.5   4.5     28514. Asia     \n 6 Afghanistan  2006  84.3   4.6     31057  Asia     \n 7 Afghanistan  2008  84.1   4.6     32738. Asia     \n 8 Afghanistan  2010  83.7   4.6     34505. Asia     \n 9 Afghanistan  2012  82.9   4.6     36416. Asia     \n10 Afghanistan  2014  82.1   4.7     38327. Asia     \n# ℹ 6,194 more rows\n\n\n\n\n2.4 Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\nSample Syntax\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nStaticAnimated\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population,     # the size of dot depends on population \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +              # add this line\n  ease_aes('linear')                   # and this line\n\n\n\n\n\n\n\n\nFor animated plot:\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\n\n\n2.5 Animated Data Visualisation: plotly\n\n2.5.1 Using plot_ly()\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  \n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\n\nbp\n\n\n\n\n\n\n2.5.2 Using ggplotly()\n\nAppropriate ggplot2 functions are used to create a static bubble plot.\nThe output is then saved as an R object called gg.\nggplotly() is used to convert the R graphic object into an animated svg object.\n\n\nWith legendWithout legend\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +                       # this doesn't work\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')                       # use this instead\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\nalthough show.legend = FALSE argument was used, the legend still appears on the plot.\nTo overcome this problem, theme(legend.position='none') should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#reference",
    "title": "Hands-on Exercise 3",
    "section": "3 Reference",
    "text": "3 Reference\n\nKam, T.S.(2023) Programming Interactive Data Visualisation with R\nKam, T.S.(2023) Programming Animated Statistical Graphics with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "First, load R packages using the below code:\n\npacman::p_load(tidyverse)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nhead(exam_data)\n\n# A tibble: 6 × 7\n  ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Student321 3I    Male   Malay        21     9      15\n2 Student305 3I    Female Malay        24    22      16\n3 Student289 3H    Male   Chinese      26    16      16\n4 Student227 3F    Male   Chinese      27    77      31\n5 Student318 3I    Male   Malay        27    11      25\n6 Student306 3I    Female Malay        31    16      16\n\n\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "First, load R packages using the below code:\n\npacman::p_load(tidyverse)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nhead(exam_data)\n\n# A tibble: 6 × 7\n  ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Student321 3I    Male   Malay        21     9      15\n2 Student305 3I    Female Malay        24    22      16\n3 Student289 3H    Male   Chinese      26    16      16\n4 Student227 3F    Male   Chinese      27    77      31\n5 Student318 3I    Male   Malay        27    11      25\n6 Student306 3I    Female Malay        31    16      16\n\n\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#introducing-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#introducing-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "2 Introducing ggplot",
    "text": "2 Introducing ggplot\nggplot2 is an R packages for declaratively creating graphics, based on The Grammar of Graphics. It is also a part of Tidyverse family.\n\n2.1 Grammar of Graphics\nGrammar of Graphics is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. It was introduced by Leland Wilkinson (1999) Grammar of Graphics, Springer. It defines the rules of structuring mathematical and aesthetic elements into a meaningful graph.\n\n\n\nA Layered Grammar of Graphics\n\n\nA short description of each building block are as follows:\n\nData: The dataset being plotted.\nAesthetics take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line.\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics, statiscal transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-geometries",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-geometries",
    "title": "Hands-on Exercise 1",
    "section": "3 ggplot2: Geometries",
    "text": "3 ggplot2: Geometries\nGeometric objects are the actual marks we put on a plot. Refer to complete list here. Examples include:\n\ngeom_point for drawing individual points (e.g., a scatter plot)\n\ngeom_line for drawing lines (e.g., for a line charts)\n\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\n\ngeom_bar for drawing bars (e.g., for bar charts)\n\ngeom_histogram for drawing binned values (e.g. a histogram)\n\ngeom_polygon for drawing arbitrary shapes\n\ngeom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function)\n\ngeom_dotplot for plotting a dot plot.\ngeom_boxplot for distribution of continuous variable.\ngeom_density for plotting kernel density estimate, a smoothed version of histogram.\ngeom_violin for compact display of a continuous distribution, blending geom_boxplot and geom_density.\n\n\n3.1 Examples of Geom plot\n\nBar chartDotplotHistogramDensity plotBoxplotViolin plotScatterplot\n\n\nThe code chunk below plots a bar chart by using geom_bar().\n\nggplot(data=exam_data,\n       aes(x= RACE)) +\n  geom_bar() \n\n\n\n\n\n\ngeom_dotplot() is used to plot a dot plot. In a dot plot, each dot represents one observation, the width of a dot corresponds to the bin width.\n\nggplot(data=exam_data,\n       aes(x=ENGLISH))+\n  geom_dotplot(binwidth=2.5,\n               dotsize = 0.5)+\n  scale_y_continuous(NULL, breaks = NULL)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk above performs the following two steps:\n\nscale_y_continuous() is used to turn off the y-axis, and\nbinwidth argument is used to change the binwidth to 2.5\n\n\n\n\n\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()    \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the default bin is 30.\n\n\n\n\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram. It is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of English scores by showing two kernel density lines by using colour or fill arguments of aes().\n\n\nggplot(data=exam_data, \n       aes(x = ENGLISH, \n           colour = GENDER)) +\n  geom_density()  \n\n\n\n\n\n\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\n\nggplot(data=exam_data, \n       aes(y = ENGLISH,       \n           x= GENDER)) +    \n  geom_boxplot() \n\n\n\n\nNotches are used to compare groups; if the notches of two boxes do not overlap, this suggests that the medians are significantly different.\n\nggplot(data=exam_data, \n       aes(y = ENGLISH, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\ngeom_violin is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plot the distribution of English score by race in violin plot.\n\nggplot(data= exam_data,\n       aes(y= ENGLISH,\n           x= RACE)) +\n  geom_violin()\n\n\n\n\n\n\ngeom_point() is especially useful for creating scatterplot.\nThe code chunk below plots a scatterplot showing the Maths and English grades of pupils by using geom_point().\n\nggplot(data= exam_data,\n      aes(x= MATHS,\n          y= ENGLISH)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n3.2 Modifying the plot\n\n3.2.1 By changing geom()\nModifying histogram by changing geom()\n\nggplot(data=exam_data,\n       aes(x= ENGLISH)) +\n  geom_histogram(bins=20,\n                 color=\"#000000\",\n                 fill=\"lightblue\",\n                 size=0.6)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBin, color and fill is now added to the plot\n\n\n\n\n3.2.2 By changing aes()\nModifying histogram by changing aes()\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"Grey30\",\n                 size=0.6,\n                 alpha=0.3)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInstead of showing total count, now they are split into Male and Female\n\n\n\n\n3.2.3 By combining two or more geom objects\n\nggplot(data=exam_data,\n       aes(y= MATHS,\n           x= GENDER)) +\n  geom_boxplot(aes(color=GENDER),\n               width=0.3)+\n  geom_jitter(aes(color=GENDER),\n              size= 0.8)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-facets",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-facets",
    "title": "Hands-on Exercise 1",
    "section": "4 ggplot2: Facets",
    "text": "4 ggplot2: Facets\nFacetting generates small multiples, each displaying a different subset of the data. Facets are an alternative to aesthetics for displaying additional discrete variables. There are two types of facets:\n(i) facet_wrap\n(ii) facet_grid\n\n4.1 Working with facet_wrap()\nfacet_wrap() wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid() because most displays are roughly rectangular. If you have only one variable with many levels, use facet_wrap() instead of facet_grid.\n\nggplot(data=exam_data, \n       aes(x= SCIENCE)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n4.2 Working with facet_grid()\nfacet_grid() forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\n\nggplot(data=exam_data,\n       aes(x= SCIENCE,\n           y= MATHS))+\n  geom_point() +\n  facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-statistics",
    "title": "Hands-on Exercise 1",
    "section": "5 ggplot2: Statistics",
    "text": "5 ggplot2: Statistics\n\n5.1 Working with stat_summary()\nThe code chunk below adds mean values by using stat_summary() function and overriding the default geom.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"blue\",        \n               size=5)        \n\n\n\n\n\n\n5.2 Working with geom() method\nThe code chunk below adding mean values by using geom_() function and overriding the default stat, refer to the function in stat_summary().\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"green\",          \n             size=5)        \n\n\n\n\n\n\n5.3 Adding a best fit curve on a scatterplot\nThe scatterplot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-coordinates",
    "title": "Hands-on Exercise 1",
    "section": "6 ggplot2: Coordinates",
    "text": "6 ggplot2: Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use, they are:\n\ncoord_cartesian(): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\ncoord_flip(): a cartesian system with the x and y flipped.\ncoord_fixed(): a cartesian system with a “fixed” aspect ratio (e.g. 1.78 for a “widescreen” plot).\ncoord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\n\n6.1 Working with coordinates\nBy default, the bar chart of ggplot2 is in vertical form. The code chunk below flips the vertical bar chart into horizontal bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n6.2 Changing the y-axis and x-axis range\nThe x-axis and y-axis could sometimes be misleading. The code chunk below fixed both the y-axis and x-axis range from 0 to 100.\n\nggplot(data=exam_data,\n       aes(x= MATHS,\n           y= SCIENCE)) +\n  geom_point() +\n  geom_smooth(method= lm,\n              size= 0.5)+\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n\n\n\nAnother method to work with x-axis and y-axis range is to use xlim() and ylim(). Refer to scales.\n\nggplot(data=exam_data,\n       aes(x= MATHS,\n           y= SCIENCE)) +\n  geom_point() +\n  geom_smooth(method= lm,\n              size= 0.5)+\n  xlim(0,100) +\n  ylim(0,100)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot2-themes",
    "title": "Hands-on Exercise 1",
    "section": "7 ggplot2: Themes",
    "text": "7 ggplot2: Themes\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nsize of fonts\ngridlines\ncolour of labels\n\nBuilt-in themes include: theme_gray() (default) theme_bw() theme_classic()\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title). To modify component of a theme, refer to this link.\nThe default theme is theme_gray().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\nA horizontal bar chart plotted using theme_minimal().\n\nggplot(data=exam_data,\n       aes(x= RACE)) +\n  geom_bar()+\n  coord_flip()+\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#modifying-the-plot-1",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#modifying-the-plot-1",
    "title": "Hands-on Exercise 1",
    "section": "8 Modifying the plot",
    "text": "8 Modifying the plot\nModifying bar chart by\n(i) changing geom()\n(ii) adding labels and title\n(iii) changing the theme\n\nggplot(data=exam_data,\n       aes(x= RACE)) +\n  geom_bar(alpha= 0.5) +\n  labs(x = \"Race\",\n       y = \"Number of Students\",\n       title = \"Race Distribution of Exam Data\")+\n  theme_classic()+\n  theme(plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOpacity of the bar chart has changed.\nX-axis label, y-axis label and title was added.\nA classic-looking theme, with x and y axis lines and no gridlines."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\n\nexam_data &lt;- read_csv(\"data/exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\n\nexam_data &lt;- read_csv(\"data/exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyong-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyong-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2",
    "section": "2 Beyong ggplot2 Annotation: ggrepel",
    "text": "2 Beyong ggplot2 Annotation: ggrepel\nggrepel provides geoms for ggplot2 to repel overlapping text labels:\n\ngeom_text_repel()\ngeom_label_repel()\n\ngeom_text_repel adds text directly to the plot. geom_label_repel draws a rectangle underneath the text, making it easier to read. The text labels repel away from each other and away from the data points.\n\nggplot(data= exam_data,\n       aes(x= MATHS,\n           y= ENGLISH,\n           colour= GENDER)) +\n  geom_point() +\n  geom_label_repel(aes(label=ID),\n                   label.padding = 0.2,\n                   label.size = 0.2) +\n  xlim(0,100) +\n  ylim (0,100)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyong-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyong-ggplot2-themes",
    "title": "Hands-on Exercise 2",
    "section": "3 Beyong ggplot2 Themes",
    "text": "3 Beyong ggplot2 Themes\ngglot2 comes with 10 built-in themes. Theme could be modified by using theme() and element_.\nThe code chunk below plots basic plot with modification to color and fill of the bar before any themes were applied.\n\nggplot(data= exam_data,\n       aes(x= SCIENCE))+\n  geom_histogram(binwidth = 5,\n                 boundary = 5,\n                 color= \"black\",\n                 fill= \"white\") \n\n\n\n\n\n3.1 Working with ggtheme package\nggthemes provides extra theme, geoms, and scales for ggplot2. Refer to these (i) link (ii) link for examples.\n\nggplot(data= exam_data,\n       aes(x= SCIENCE))+\n  geom_histogram(binwidth = 5,\n                 boundary = 5,\n                 color= \"black\",\n                 fill= \"white\") +\n  theme_wsj(base_size = 10,\n             color = \"brown\",\n             base_family = \"sans\",\n             title_family = \"mono\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme(plot.title= element_text(size= 15))\n\n\n\n\n\n\n3.2 Working with hrbrthemes package\nhrbrthemes is a very focused package that provides typography-centric themes and theme components for ggplot2. Consult this vignette to learn more.\n\nggplot(data= exam_data,\n       aes(x= SCIENCE))+\n  geom_histogram(binwidth = 5,\n                 boundary = 5,\n                 color = \"black\",\n                 fill = \"bisque3\",\n                 alpha = 0.6,\n                 linewidth =0.3) +\n  ggtitle(\"Distribution of Science scores\") +\n  theme_ipsum(axis_title_size = 12,\n              base_size = 10,\n              grid_col = \"grey80\",\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyong-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyong-single-graph",
    "title": "Hands-on Exercise 2",
    "section": "4 Beyong Single Graph",
    "text": "4 Beyong Single Graph\nPatchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\nTo learn more about, refer to Plot Assembly.\n\n4.1 Combining 2 graphs using ’ + ’\n\np1 &lt;- ggplot(data= exam_data,\n       aes(x= MATHS))+\n  geom_histogram(binwidth = 5,\n                 boundary = 5,\n                 color = \"black\",\n                 fill = \"bisque\",\n                 alpha = 0.6,\n                 linewidth =0.3) +\n  labs(x= \"Maths score\",\n       y= \"No. of students\",\n       title=\"Distribution of Maths scores\") +\n    theme(plot.title= element_text(size= 12, hjust= 0.5),\n          axis.title= element_text(size= 10),\n          panel.background = element_rect(fill= \"grey90\"))\n\np2 &lt;- ggplot(data= exam_data,\n       aes(x= SCIENCE))+\n  geom_histogram(binwidth = 5,\n                 boundary = 5,\n                 color = \"black\",\n                 fill = \"bisque4\",\n                 alpha = 0.6,\n                 linewidth =0.3) +\n  labs(x= \"Science score\",\n       y= \"No. of students\",\n       title=\"Distribution of Science scores\") +\n    theme(plot.title= element_text(size= 12, hjust= 0.5),\n          axis.title= element_text(size= 10),\n          panel.background = element_rect(fill= \"grey90\"),\n          axis.title.y = element_blank())\n          \n\np1 + p2\n\n\n\n\n\n\n4.2 Combining 3 graphs using ’ | ’ ’ / ’ ’ ( ) ’\n\np3 &lt;- ggplot(data = exam_data,\n             aes(MATHS, SCIENCE)) +\n  geom_point(size = 2,\n             color = \"khaki4\",\n             alpha = 0.8) +\n  geom_smooth(method = lm,\n              size = 0.8,\n              color = \"grey20\") +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  labs(title = \"Correlation Plot\",\n       subtitle = \"Science vs Maths scores for Primary 3\") +\n  theme(plot.title=element_text(size= 12),\n        plot.subtitle=element_text(size= 10),\n        axis.title = element_text(size= 10))\n\n(p1 / p2) | p3\n\n\n\n\n\n\n4.3 Adding annotation\n\n((p1 / p2) | p3) + \n  plot_annotation(title= \"Distribution and correlation of Maths and Science scores for Primary 3 students\",\n                  tag_levels = 'i')\n\n\n\n\n\n\n4.4 Insert Figures with Inset_element\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.5, \n                   right = 0.4, \n                   top = 1)\n\n\n\n\n\n\n4.5 Creating a composite figure by using patchwork and ggtheme\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_stata()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "Summary\nThis hands-on exercise consists of 4 main sections, namely:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section1",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section1",
    "title": "Hands-on Exercise 4",
    "section": "1 Visualising Distribution",
    "text": "1 Visualising Distribution\nIn previous chapter, we learn to create following statistical graphics using ggplot2: - histogram, probability density curve (pdf), boxplot, notch plot and violin plot\nIn this chapter, two relatively new statistical graphic methods are introduced, namely: (i) ridgeline plot (ii) raincloud plot by using ggplot2 and its extensions.\n\n1.1 Loading R packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse - a family of R packages for data science process,\nggridges - a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist - visualising distribution and uncertainty.\n\n\n\nCode\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\n1.2 Importing data\nFor the purpose of this exercise, Exam_data.csv will be used.\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n1.3 Ridgeline Plot\nUsing Ridgeline plot (aka Joyplot), distribution of a numeric value for several groups can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nRidgeline plots works well when: - the number of group to represent is medium to high as it allows us to use space more efficiently - there is more than 5 groups (distribution plots is probably better for less than 5 groups) - there is a clear pattern in the result, like if there is an obvious ranking in groups to avoid overlapping\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: - geom_ridgeline() - takes height values directly to draw the ridgelines - geom_density_ridges() - first estimates data densities and then draws those using ridgelines\nFollowing are examples of plot using geom_density_ridges():\n\nBasic plot\nVarying fill colors using geom_density_ridges_gradient() (or geom_ridgeline_gradient())\nMapping the probabilities directly onto colour using stat(ecdf)\nAdding quantile lines using stat(quantile)\nHighlighting specific quantile - 2.5% and 97.5%\n\n\niiiiiiivv\n\n\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n1.4 Raincloud Plot\nRaincloud Plot produces a half-density to a distribution plot which enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist).\nIn this section, a raincloud plot visualising the distribution of English score by race will be created by using functions provided by ggdist and ggplot2 packages.\n\nPlotting a Half Eye graph by using stat_halfeye()\nAdding the boxplot with geom_boxplot()\nAdding the Dot Plots with stat_dots()\nFlipping it horizontally using coord_flip()\n\nFollowing are examples of raincloud plot and the modification:\n\niiiiiiiv\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               fill = \"lightblue\") +\n  theme_minimal()\n\n\n\n\n\n\nThings to learn from the code chunk above\nslab interval is removed by setting .width = 0 and point_colour = NA.\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.15,\n               .width = 0,\n               point_colour = NA,\n               fill = \"lightblue\") +\n  geom_boxplot(width = .1,\n               outlier.shape = NA) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.15,\n               .width = 0,\n               point_colour = NA,\n               fill = \"lightblue\") +\n  geom_boxplot(width = .1,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.1, \n            binwidth = .5,\n            dotsize = 2) +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.15,\n               .width = 0,\n               point_colour = NA,\n               fill = \"lightblue\") +\n  geom_boxplot(width = .1,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.1, \n            binwidth = .5,\n            dotsize = 1.5,\n            fill = \"grey70\",\n            color = \"grey70\") +          # dotsize is adjusted\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section2",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section2",
    "title": "Hands-on Exercise 4",
    "section": "2 Visual Statistical Analysis",
    "text": "2 Visual Statistical Analysis\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:\n\n\n\n2.1 ggstatsplot\n\n2.1.1 Loading R packages\nIn this exercise, ggstatsplot and tidyverse will be used.\n\n\nCode\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n2.1.2 Importing data\nFor the purpose of this exercise, Exam_data.csv will be used. The following code is used to import Exam_data.csv\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n2.1.3 Statistical Tests\nFollowing are the graphics with details from various statistical tests:\n\nOne-sample test: gghistostats()\nTwo-sample mean test: ggbetweenstats()\nTwo-sample mean test: ggbetweenstats()\nSignificant Test of Correlation: ggscatterstats()\nSignificant Test of Association (Depedence) : ggbarstats()\n\n\niiiiiiivv\n\n\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nCode\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\n\nCode\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100)))\n\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\n\nCode\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\n\n\n2.1.4 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10.\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013\n\n\n\n\n2.2 Models\nIn this section, we visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n2.2.1 Loading R packages\n\n\nCode\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n\n2.2.2 Importing data\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\n\nCode\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n2.2.3 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\n\nCode\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n2.2.4 Model Diagnostic\n\nchecking for multicolinearity using check_collinearity()\nchecking normality assumption using check_normality()\nCheck model for homogeneity of variances using check_heteroscedasticity()\nComplete check using check_model()\n\n\niiiiiiiv\n\n\n\n\nCode\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\n\nCode\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\nCode\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\n\n\nCode\ncheck_n &lt;- check_normality(model1)\n\n\n\n\nCode\nplot(check_n)\n\n\n\n\n\n\n\n\n\nCode\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\n\n\nCode\nplot(check_h)\n\n\n\n\n\n\n\n\n\nCode\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n2.2.5 Visualising Regression Parameters\nThere are two methods to visualise regression parameters: (i) See method using plot() and parameters() (ii) using ggcoefstats()\nExamples of the plots are as follows:\n\niii\n\n\n\n\nCode\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\nCode\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section3",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section3",
    "title": "Hands-on Exercise 4",
    "section": "3 Visualising Uncertainty",
    "text": "3 Visualising Uncertainty\n\n3.1 Installing and loading the packages\n\n\nCode\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\n\n\nCode\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n\n3.2 Importing data\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n3.3 ggplot2 methods\n\n\nCode\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\nCode\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\nPlotting standard error bars of point estimates\nPlotting confidence interval of point estimates\nVisualizing the uncertainty of point estimates with interactive error bars\n\n\niiiiii\n\n\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\nCode\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 ggdist methods\n\ndisplaying distribution of maths scores by race using stat_pointinterval()\nShowing median\nshowing 95% and 99% confidence intervals\ndisplaying distribution of maths scores by race using stat_gradientinterval()\n\n\niiiiiiiv\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"pink\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\n\n3.5 Hypothetical Outcome Plots (HOPs)\n\n\nCode\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\n\n\nCode\nlibrary(ungeviz)\n\n\n\n\nCode\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)          # `.draw` is a generated column indicating the sample draw"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section4",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#Section4",
    "title": "Hands-on Exercise 4",
    "section": "4 Funnel Plots for Fair Comparisons",
    "text": "4 Funnel Plots for Fair Comparisons\n\n4.1 Installing and Launching R Packages\n\n\nCode\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\n4.2 Importing Data\n\n\nCode\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\n\n\n\n4.3 FunnelPlotR methods\n\nbasicmakeover 1makeover 2\n\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n4.4 ggplot2 methods\n\n4.4.1 Computing the basic derived fields\n\n\nCode\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\n\n\n\nCode\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\n4.4.2 Calculate lower and upper limits for 95% and 99.9% CI\n\n\nCode\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\nStaticInteractive\n\n\n\n\nCode\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\nCode\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#reference",
    "title": "Hands-on Exercise 4",
    "section": "5 Reference",
    "text": "5 Reference\n\nKam, T.S.(2023) Visualising Distribution\nKam, T.S.(2023) Visual Statistical Analysis\nKam, T.S.(2023) Visualising Uncertainty\nKam, T.S.(2023) Funnel Plots for Fair Comparisons"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0, revealing different relationships:\n\n1 shows a perfect linear relationship between the two variables\n-1.0 shows a perfect inverse relationship between the two variables\n0.0 shows no linear relationship between the two variables\n\nThe correlation coefficeints of the pair comparisons are displayed in correlation matrix or scatterplot matrix. There are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\n\n\n\npacman::p_load(tidyverse, ggstatsplot,corrplot)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nRows: 6497 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): type\ndbl (12): fixed acidity, volatile acidity, citric acid, residual sugar, chlo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet us first started by examining the data table.\n\nFirst 5 rowsStructure\n\n\n\nhead(wine,5)\n\n# A tibble: 5 × 13\n  `fixed acidity` `volatile acidity` `citric acid` `residual sugar` chlorides\n            &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1             7.4               0.7           0                 1.9     0.076\n2             7.8               0.88          0                 2.6     0.098\n3             7.8               0.76          0.04              2.3     0.092\n4            11.2               0.28          0.56              1.9     0.075\n5             7.4               0.7           0                 1.9     0.076\n# ℹ 8 more variables: `free sulfur dioxide` &lt;dbl&gt;,\n#   `total sulfur dioxide` &lt;dbl&gt;, density &lt;dbl&gt;, pH &lt;dbl&gt;, sulphates &lt;dbl&gt;,\n#   alcohol &lt;dbl&gt;, quality &lt;dbl&gt;, type &lt;chr&gt;\n\n\n\n\n\nstr(wine)\n\nspc_tbl_ [6,497 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ fixed acidity       : num [1:6497] 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...\n $ volatile acidity    : num [1:6497] 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...\n $ citric acid         : num [1:6497] 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...\n $ residual sugar      : num [1:6497] 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...\n $ chlorides           : num [1:6497] 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...\n $ free sulfur dioxide : num [1:6497] 11 25 15 17 11 13 15 15 9 17 ...\n $ total sulfur dioxide: num [1:6497] 34 67 54 60 34 40 59 21 18 102 ...\n $ density             : num [1:6497] 0.998 0.997 0.997 0.998 0.998 ...\n $ pH                  : num [1:6497] 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...\n $ sulphates           : num [1:6497] 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...\n $ alcohol             : num [1:6497] 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...\n $ quality             : num [1:6497] 5 5 5 6 5 5 5 7 7 5 ...\n $ type                : chr [1:6497] \"red\" \"red\" \"red\" \"red\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `fixed acidity` = col_double(),\n  ..   `volatile acidity` = col_double(),\n  ..   `citric acid` = col_double(),\n  ..   `residual sugar` = col_double(),\n  ..   chlorides = col_double(),\n  ..   `free sulfur dioxide` = col_double(),\n  ..   `total sulfur dioxide` = col_double(),\n  ..   density = col_double(),\n  ..   pH = col_double(),\n  ..   sulphates = col_double(),\n  ..   alcohol = col_double(),\n  ..   quality = col_double(),\n  ..   type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\n\n\n\nIn this section, we learn to create scatterplot matrix by using the pairs function of R Graphics.\n\nbasicCornercorrelation coefficients\n\n\nColumns 1 to 11 of wine dataframe is used to build the scatterplot matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\n  usr &lt;- par(\"usr\")\n  on.exit(par(usr))\n  par(usr = c(0, 1, 0, 1))\n  r &lt;- abs(cor(x, y, use=\"complete.obs\"))\n  txt &lt;- format(c(r, 0.123456789), digits=digits)[1]\n  txt &lt;- paste(prefix, txt, sep=\"\")\n  if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\n  text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\nIn this section, we learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\nThese are some other R packages that provide function to plot corrgram:\n\ncorrgram\nellipse\ncorrplot\n\n\nbasicmodified\n\n\nThis is how a basic correlation matrix look like when using ggcorrmat(), for further modification, check out next tab.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"grey80\", \n                         hc.order = TRUE,\n                         tl.cex = 9,\n                         lab_size = 3,\n                         lab_col = \"grey30\",\n                         pch.cex = 8,\n                         pch.col = \"grey40\"),\n  pch = \"square cross\",\n  colors = c(\"red\", \"white\", \"#0072B2\"),\n\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n) +\ntheme(legend.text = element_text(size = 8),\n      legend.title = element_text(size =12),\n      plot.title=element_text(size= 14))\n\n\n\n\n\n\n\nMore information related to ggcorrmat() can be found here and here.\n\n\n\nInstea of ggcorrmat(), we use grouped_ggcorrmat() of ggstatsplot to create multiple plots.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 6,\n                         lab_size = 2),\n  colors = c(\"red\", \"white\", \"#0072B2\"),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)+\ntheme(legend.position = \"bottom\")\n\n\n\n\n\nThings to learn from the code chunk above\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\n\n\nRefer to An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\n\n\ncorrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\nDefaultmethodTypeMore\n\n\n\ncorrplot(wine.cor)\n\n\n\n\n\n\nCurrently, corrplot() offer 7 visualization methods, named “circle”, “square”, “ellipse”, “number”, “pie”, “shade” and “color”\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\") \n\n\n\n\n\n\nplot “full” matrix or just “upper” or “lower” triangular part of it.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.cex = 0.8)\n\n\n\n\n\nArguments\ndiag - to turn off the diagonal cells\ntl.col - to change the axis text label colour to black colour\ntl.cex - to adjust the title label size\n\n\n\n\n\n\n\nIn this section, we learn to use mixed methods to visualize a correlation matrix by using corrplot.mixed(). We use lower and upper to specify the method from any of the 7 visualization methods, named “circle”, “square”, “ellipse”, “number”, “pie”, “shade” and “color”.\n\ncorrplot.mixed(wine.cor, \n               lower = \"square\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\",\n               tl.cex = 0.8,\n               number.cex = 0.7)\n\n\n\n\n\n\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05,\n         tl.cex = 0.8,\n         number.cex = 0.7)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNumbers shows in corrgram is correlation coefficient. Not p-value!\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). Currently, corrplot package support four sorting methods, they are:\n\nAOE - the angular order of the eigenvectors. See Michael Friendly (2002) for details.\nFPC - first principal component order.\nhclust - hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\nalphabet for alphabetical order.\n\nThe order above can be applied to corrplot() and corrplot.mixed().\n\ncorrplot.mixed(wine.cor, \n               lower = \"square\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\",\n               tl.cex = 0.8,\n               number.cex = 0.7,\n               order = \"AOE\")\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.cex = 0.8,\n         order = \"AOE\")\n\n\n\n\nUsing hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"average\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#overview",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0, revealing different relationships:\n\n1 shows a perfect linear relationship between the two variables\n-1.0 shows a perfect inverse relationship between the two variables\n0.0 shows no linear relationship between the two variables\n\nThe correlation coefficeints of the pair comparisons are displayed in correlation matrix or scatterplot matrix. There are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\n\n\n\npacman::p_load(tidyverse, ggstatsplot,corrplot)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nRows: 6497 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): type\ndbl (12): fixed acidity, volatile acidity, citric acid, residual sugar, chlo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet us first started by examining the data table.\n\nFirst 5 rowsStructure\n\n\n\nhead(wine,5)\n\n# A tibble: 5 × 13\n  `fixed acidity` `volatile acidity` `citric acid` `residual sugar` chlorides\n            &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1             7.4               0.7           0                 1.9     0.076\n2             7.8               0.88          0                 2.6     0.098\n3             7.8               0.76          0.04              2.3     0.092\n4            11.2               0.28          0.56              1.9     0.075\n5             7.4               0.7           0                 1.9     0.076\n# ℹ 8 more variables: `free sulfur dioxide` &lt;dbl&gt;,\n#   `total sulfur dioxide` &lt;dbl&gt;, density &lt;dbl&gt;, pH &lt;dbl&gt;, sulphates &lt;dbl&gt;,\n#   alcohol &lt;dbl&gt;, quality &lt;dbl&gt;, type &lt;chr&gt;\n\n\n\n\n\nstr(wine)\n\nspc_tbl_ [6,497 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ fixed acidity       : num [1:6497] 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...\n $ volatile acidity    : num [1:6497] 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...\n $ citric acid         : num [1:6497] 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...\n $ residual sugar      : num [1:6497] 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...\n $ chlorides           : num [1:6497] 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...\n $ free sulfur dioxide : num [1:6497] 11 25 15 17 11 13 15 15 9 17 ...\n $ total sulfur dioxide: num [1:6497] 34 67 54 60 34 40 59 21 18 102 ...\n $ density             : num [1:6497] 0.998 0.997 0.997 0.998 0.998 ...\n $ pH                  : num [1:6497] 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...\n $ sulphates           : num [1:6497] 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...\n $ alcohol             : num [1:6497] 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...\n $ quality             : num [1:6497] 5 5 5 6 5 5 5 7 7 5 ...\n $ type                : chr [1:6497] \"red\" \"red\" \"red\" \"red\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `fixed acidity` = col_double(),\n  ..   `volatile acidity` = col_double(),\n  ..   `citric acid` = col_double(),\n  ..   `residual sugar` = col_double(),\n  ..   chlorides = col_double(),\n  ..   `free sulfur dioxide` = col_double(),\n  ..   `total sulfur dioxide` = col_double(),\n  ..   density = col_double(),\n  ..   pH = col_double(),\n  ..   sulphates = col_double(),\n  ..   alcohol = col_double(),\n  ..   quality = col_double(),\n  ..   type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\n\n\n\nIn this section, we learn to create scatterplot matrix by using the pairs function of R Graphics.\n\nbasicCornercorrelation coefficients\n\n\nColumns 1 to 11 of wine dataframe is used to build the scatterplot matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\n  usr &lt;- par(\"usr\")\n  on.exit(par(usr))\n  par(usr = c(0, 1, 0, 1))\n  r &lt;- abs(cor(x, y, use=\"complete.obs\"))\n  txt &lt;- format(c(r, 0.123456789), digits=digits)[1]\n  txt &lt;- paste(prefix, txt, sep=\"\")\n  if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\n  text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\nIn this section, we learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\nThese are some other R packages that provide function to plot corrgram:\n\ncorrgram\nellipse\ncorrplot\n\n\nbasicmodified\n\n\nThis is how a basic correlation matrix look like when using ggcorrmat(), for further modification, check out next tab.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"grey80\", \n                         hc.order = TRUE,\n                         tl.cex = 9,\n                         lab_size = 3,\n                         lab_col = \"grey30\",\n                         pch.cex = 8,\n                         pch.col = \"grey40\"),\n  pch = \"square cross\",\n  colors = c(\"red\", \"white\", \"#0072B2\"),\n\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n) +\ntheme(legend.text = element_text(size = 8),\n      legend.title = element_text(size =12),\n      plot.title=element_text(size= 14))\n\n\n\n\n\n\n\nMore information related to ggcorrmat() can be found here and here.\n\n\n\nInstea of ggcorrmat(), we use grouped_ggcorrmat() of ggstatsplot to create multiple plots.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 6,\n                         lab_size = 2),\n  colors = c(\"red\", \"white\", \"#0072B2\"),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)+\ntheme(legend.position = \"bottom\")\n\n\n\n\n\nThings to learn from the code chunk above\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\n\n\nRefer to An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\n\n\ncorrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\nDefaultmethodTypeMore\n\n\n\ncorrplot(wine.cor)\n\n\n\n\n\n\nCurrently, corrplot() offer 7 visualization methods, named “circle”, “square”, “ellipse”, “number”, “pie”, “shade” and “color”\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\") \n\n\n\n\n\n\nplot “full” matrix or just “upper” or “lower” triangular part of it.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.cex = 0.8)\n\n\n\n\n\nArguments\ndiag - to turn off the diagonal cells\ntl.col - to change the axis text label colour to black colour\ntl.cex - to adjust the title label size\n\n\n\n\n\n\n\nIn this section, we learn to use mixed methods to visualize a correlation matrix by using corrplot.mixed(). We use lower and upper to specify the method from any of the 7 visualization methods, named “circle”, “square”, “ellipse”, “number”, “pie”, “shade” and “color”.\n\ncorrplot.mixed(wine.cor, \n               lower = \"square\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\",\n               tl.cex = 0.8,\n               number.cex = 0.7)\n\n\n\n\n\n\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05,\n         tl.cex = 0.8,\n         number.cex = 0.7)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNumbers shows in corrgram is correlation coefficient. Not p-value!\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). Currently, corrplot package support four sorting methods, they are:\n\nAOE - the angular order of the eigenvectors. See Michael Friendly (2002) for details.\nFPC - first principal component order.\nhclust - hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\nalphabet for alphabetical order.\n\nThe order above can be applied to corrplot() and corrplot.mixed().\n\ncorrplot.mixed(wine.cor, \n               lower = \"square\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\",\n               tl.cex = 0.8,\n               number.cex = 0.7,\n               order = \"AOE\")\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.cex = 0.8,\n         order = \"AOE\")\n\n\n\n\nUsing hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"average\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them.\nParallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, we will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#overview",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them.\nParallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, we will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5d",
    "section": "2 Installing and Launching R Packages",
    "text": "2 Installing and Launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#data-preparation",
    "title": "Hands-on Exercise 5d",
    "section": "3 Data Preparation",
    "text": "3 Data Preparation\nIn this hands-on exercise, the World Happinees 2018 data will be used. The data set can be downloaded at this link. The original data set has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 5d",
    "section": "4 Plotting Static Parallel Coordinates Plot",
    "text": "4 Plotting Static Parallel Coordinates Plot\nIn this section, you learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. B\n\n\n4.1 Plotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\n\n4.2 Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\nThings to learn from the code chunk above\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\n\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\n4.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\nThings to learn from the code chunk above\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\nTo adjust the x-axis text location, we use hjust argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 5d",
    "section": "5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js.\n\nbasicRotate axis labelChanging the colour scheme\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nrotateTitle argument is used to avoid overlapping axis labels.\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels. One of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used. change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             width = 320,\n             height = 250,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             width = 320,\n             height = 250,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, different intensity colour scheme will be used to highlight the variable chosen.\n\n\n\n5.1 Parallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             width = 320,\n             height = 250,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: Now You See It!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "title": "In-class Exercise 1: Now You See It!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-pisa-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-pisa-data",
    "title": "In-class Exercise 1: Now You See It!",
    "section": "2 Importing PISA data",
    "text": "2 Importing PISA data\nThe code chunk below uses read_sas() of haven to import PISA data into R environment.\nStep 1: Use read_sas() to read SAS file\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nStep 2: Filter required data using filter()\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\nStep 3: Save filtered data in R Data Format\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")\n\nStep 4: Read saved R Data format file to ensure that it is working\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "OECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) However, there are public concern about the inequality, particularly in the context between elite schools and neighborhood school, between families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families.\n\n\nThis analysis using the 2022 PISA data aims to explore these aspects in Singapore’s context. The objective of this study is to adopt appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "OECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) However, there are public concern about the inequality, particularly in the context between elite schools and neighborhood school, between families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families.\n\n\nThis analysis using the 2022 PISA data aims to explore these aspects in Singapore’s context. The objective of this study is to adopt appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-preparation",
    "title": "Take-home Exercise 1",
    "section": "2 Data preparation",
    "text": "2 Data preparation\n\n2.1 Loading R packages\nFirst, following code chunk with pacman::p_load() function is used to load required R packages into our working environment:\n\npacman::p_load(tidyverse, haven, patchwork, ggdist, ggrain, ggridges)\n\n\n\n2.2 Importing PISA data\nThe code chunk below uses read_sas() of haven to import PISA data into working environment\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe data table consists of 613,744 observations and 1,279 variables.\n\n\n2.3 Data extracting\nReferring to PISA code book, it is noted that the data table include results from 81 participating countries. As this study focus on Singapore’s context, relevant observations is filtered using filter() on the country code variable “CNT”.\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\nThe filtered data is then saved in R Data Format and re-imported into the working environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\nstu_qqq_SG\n\n# A tibble: 6,606 × 1,279\n   CNT   CNTRYID CNTSCHID CNTSTUID CYC   NatCen STRATUM SUBNATIO REGION  OECD\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 SGP       702 70200052 70200001 08MS  070200 SGP01   7020000   70200     0\n 2 SGP       702 70200134 70200002 08MS  070200 SGP01   7020000   70200     0\n 3 SGP       702 70200112 70200003 08MS  070200 SGP01   7020000   70200     0\n 4 SGP       702 70200004 70200004 08MS  070200 SGP01   7020000   70200     0\n 5 SGP       702 70200152 70200005 08MS  070200 SGP01   7020000   70200     0\n 6 SGP       702 70200043 70200006 08MS  070200 SGP01   7020000   70200     0\n 7 SGP       702 70200049 70200007 08MS  070200 SGP01   7020000   70200     0\n 8 SGP       702 70200107 70200008 08MS  070200 SGP01   7020000   70200     0\n 9 SGP       702 70200012 70200009 08MS  070200 SGP01   7020000   70200     0\n10 SGP       702 70200061 70200010 08MS  070200 SGP01   7020000   70200     0\n# ℹ 6,596 more rows\n# ℹ 1,269 more variables: ADMINMODE &lt;dbl&gt;, LANGTEST_QQQ &lt;dbl&gt;,\n#   LANGTEST_COG &lt;dbl&gt;, LANGTEST_PAQ &lt;dbl&gt;, Option_CT &lt;dbl&gt;, Option_FL &lt;dbl&gt;,\n#   Option_ICTQ &lt;dbl&gt;, Option_WBQ &lt;dbl&gt;, Option_PQ &lt;dbl&gt;, Option_TQ &lt;dbl&gt;,\n#   Option_UH &lt;dbl&gt;, BOOKID &lt;dbl&gt;, ST001D01T &lt;dbl&gt;, ST003D02T &lt;dbl&gt;,\n#   ST003D03T &lt;dbl&gt;, ST004D01T &lt;dbl&gt;, ST250Q01JA &lt;dbl&gt;, ST250Q02JA &lt;dbl&gt;,\n#   ST250Q03JA &lt;dbl&gt;, ST250Q04JA &lt;dbl&gt;, ST250Q05JA &lt;dbl&gt;, ST250D06JA &lt;chr&gt;, …\n\n\nAfter completing the previous steps, we are left with a substantial data table consisting 1,279 variables. At this point, it is essential to narrow down these variables to focus on the most relevant ones. By examining the code book, the following variables were chosen to align with the objective of this study:\n\n\n\n\n\n\n\n\nVariable name\nLabel\nDescription\n\n\n\n\nCNTSTUID\nIntl. Student ID\nStudent ID of participating students\n\n\nSTRATUM\nStratum ID 5-character\n(cnt + original stratum ID)\nType of School\nSGP01 : Public / Secondary\nSGP03 : Private / Secondary\n\n\nST004D01T\nStudent (Standardized) Gender\nGender\n1 : Female\n2 : Male\n\n\nESCS\nIndex of economic, social and cultural status\nMeasurement of student’s socio-economic status\n\n\nPV1MATH\nPlausible Value 1 in Mathematics\nStudent’s performance for Mathematics\n\n\nPV1READ\nPlausible Value 1 in Reading\nStudent’s performance for Reading\n\n\nPV1SCIE\nPlausible Value 1 in Science\nStudent’s performance for Science\n\n\n\nWe use select() and rename()from dplyr to select the column and rename the variable for clarity.\n\nstu_qqq_SG_selected &lt;- stu_qqq_SG %&gt;%\n  select('CNTSTUID',\n         'STRATUM',\n         'ST004D01T',\n         'ESCS',\n         'PV1MATH',\n         'PV1READ',\n         'PV1SCIE') %&gt;%\n  rename(StudentID = CNTSTUID,\n         TypeofSchool = STRATUM,\n         Gender = ST004D01T,\n         MATH = PV1MATH,\n         READ = PV1READ,\n         SCIENCE = PV1SCIE)\n\n\n\n2.4 Summary Statistics of data table\n\nData tableSummaryDuplicateMissing\n\n\n\nstu_qqq_SG_selected\n\n# A tibble: 6,606 × 7\n   StudentID TypeofSchool Gender    ESCS  MATH  READ SCIENCE\n       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  70200001 SGP01             1  0.184   639.  676.    711.\n 2  70200002 SGP01             2  0.826   697.  626.    671.\n 3  70200003 SGP01             2 -1.04    694.  620.    666.\n 4  70200004 SGP01             2 -0.961   427.  381.    340.\n 5  70200005 SGP01             1  0.0856  436.  448.    456.\n 6  70200006 SGP01             1  0.127   570.  469.    475.\n 7  70200007 SGP01             2 -0.0154  772.  745.    694.\n 8  70200008 SGP01             2  1.16    568.  527.    583.\n 9  70200009 SGP01             1  1.47    740.  722.    725.\n10  70200010 SGP01             2  0.520   586.  540.    590.\n# ℹ 6,596 more rows\n\n\n\n\n\nsummary(stu_qqq_SG_selected)\n\n   StudentID        TypeofSchool           Gender           ESCS        \n Min.   :70200001   Length:6606        Min.   :1.000   Min.   :-3.5488  \n 1st Qu.:70201836   Class :character   1st Qu.:1.000   1st Qu.:-0.2327  \n Median :70203674   Mode  :character   Median :2.000   Median : 0.4817  \n Mean   :70203673                      Mean   :1.508   Mean   : 0.2904  \n 3rd Qu.:70205513                      3rd Qu.:2.000   3rd Qu.: 0.9036  \n Max.   :70207345                      Max.   :2.000   Max.   : 3.2780  \n                                                       NA's   :47       \n      MATH            READ          SCIENCE     \n Min.   :218.6   Min.   :135.9   Min.   :187.5  \n 1st Qu.:503.1   1st Qu.:476.9   1st Qu.:495.7  \n Median :582.5   Median :552.9   Median :568.7  \n Mean   :574.2   Mean   :544.4   Mean   :560.8  \n 3rd Qu.:648.2   3rd Qu.:619.6   3rd Qu.:631.1  \n Max.   :943.0   Max.   :859.5   Max.   :873.3  \n                                                \n\n\n\n\nCheck for duplicated value\n\nstu_qqq_SG_selected[duplicated(stu_qqq_SG_selected),]\n\n# A tibble: 0 × 7\n# ℹ 7 variables: StudentID &lt;dbl&gt;, TypeofSchool &lt;chr&gt;, Gender &lt;dbl&gt;, ESCS &lt;dbl&gt;,\n#   MATH &lt;dbl&gt;, READ &lt;dbl&gt;, SCIENCE &lt;dbl&gt;\n\n\n\n\nChecking for missing data\n\nstu_qqq_SG_selected %&gt;% summarise_all(funs(sum(is.na(.))))\n\n# A tibble: 1 × 7\n  StudentID TypeofSchool Gender  ESCS  MATH  READ SCIENCE\n      &lt;int&gt;        &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n1         0            0      0    47     0     0       0\n\n\nThe output shows 47 missing values for ESCS, the observations were dropped using drop_na():\n\nstu_qqq_SG_selected2 &lt;- stu_qqq_SG_selected %&gt;% drop_na()\n\n\n\n\n\n⚠️ Data quality issues\n\n\nInappropriate column types\nFollowing columns were converted to appropriate type:\n\n“StudentID” : convert from &lt;dbl&gt; to &lt;chr&gt;\n“TypeofSchool” : convert from &lt;chr&gt; to &lt;fct&gt;\n“Gender” : convert from &lt;dbl&gt; to &lt;fct&gt;\n\nNon-Descriptive Values\nValues from “Gender” and “TypeofSchool” were recoded to descriptive values using fct_recode to improve clarity.\nDisaggregated ESCS Index\nBinning of the ESCS index into categories of most disadvantaged to the most advantaged groups can facilitate more granular analysis and provide meaningful insights. 4 groups of students of equal size were created using cut_number(), each group comprises 25% of the population of 15-year-old students in Singapore.\n\n\n\nCode\nstu_qqq_SG_converted &lt;- stu_qqq_SG_selected2 %&gt;%\n  \n  # change column type\n  mutate(StudentID = as.character(StudentID),         \n         TypeofSchool = as.factor(TypeofSchool),\n         Gender = as.factor(Gender)) %&gt;%\n  # recode non-descriptive values  \n  mutate(Gender = fct_recode (Gender,\n                              \"Female\" = \"1\",\n                              \"Male\" = \"2\"),\n           TypeofSchool = fct_recode (TypeofSchool,\n                              \"Public\" = \"SGP01\",\n                              \"Private\" = \"SGP03\"),\n  # binning of disaggregated data       \n         binned_ESCS = cut_number(stu_qqq_SG_selected2$ESCS, \n                                    n = 4, \n                                    labels = c(\"Disadvantaged\",\n                                               \"Slightly Disadvantaged\",\n                                               \"Slightly Advantaged\",\n                                               \"Advantaged\")))\n\n\nFollowing final data table is used for Exploratory data analysis:\n\nstu_qqq_SG_converted\n\n# A tibble: 6,559 × 8\n   StudentID TypeofSchool Gender    ESCS  MATH  READ SCIENCE binned_ESCS        \n   &lt;chr&gt;     &lt;fct&gt;        &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;              \n 1 70200001  Public       Female  0.184   639.  676.    711. Slightly Disadvant…\n 2 70200002  Public       Male    0.826   697.  626.    671. Slightly Advantaged\n 3 70200003  Public       Male   -1.04    694.  620.    666. Disadvantaged      \n 4 70200004  Public       Male   -0.961   427.  381.    340. Disadvantaged      \n 5 70200005  Public       Female  0.0856  436.  448.    456. Slightly Disadvant…\n 6 70200006  Public       Female  0.127   570.  469.    475. Slightly Disadvant…\n 7 70200007  Public       Male   -0.0154  772.  745.    694. Slightly Disadvant…\n 8 70200008  Public       Male    1.16    568.  527.    583. Advantaged         \n 9 70200009  Public       Female  1.47    740.  722.    725. Advantaged         \n10 70200010  Public       Male    0.520   586.  540.    590. Slightly Advantaged\n# ℹ 6,549 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "title": "Take-home Exercise 1",
    "section": "3 Exploratory Data Analysis",
    "text": "3 Exploratory Data Analysis\n\n3.1 Distribution of Performance in Mathematics, Reading and Science\n\n\nCode\n# Distribution of Performance in Mathematics\nP1 &lt;- ggplot(data = stu_qqq_SG_converted,\n       aes(x = MATH)) +\n  geom_density(color = \"#459395\", size = 0.6, fill= \"#459395\", alpha = 0.4) +\n  coord_cartesian(xlim = c(0,1000)) +\n  geom_vline(aes(xintercept = mean(MATH)),\n             color = \"red\", alpha = 0.8, linewidth = 0.7, linetype = \"dashed\") +\n  annotate(\"text\", x = 400, y = 0.0035,\n           label = paste(\"Mean=\", \n                         round(mean(stu_qqq_SG_converted$MATH, na.rm=T), 2)),\n           color = \"red\", size = 3) +\n  geom_vline(aes(xintercept = median(MATH)),\n             color= \"grey50\", linewidth = 0.7, linetype = \"solid\") +\n  annotate(\"text\", x = 800, y = 0.0035,\n           label = paste(\"Median=\", \n                         round(median(stu_qqq_SG_converted$MATH, na.rm=T), 2)),\n           color = \"grey20\", size = 3) +  \n  geom_boxplot(width = 0.0005, fill = \"white\", alpha = 0.5,\n               position = position_nudge(y = -0.0005)) +\n  theme_minimal()+\n  labs(title=\"Distribution of Performance in Mathematics\") +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12),\n        axis.text = element_text(size= 8)) \n\n# Distribution of Performance in Reading\nP2 &lt;- ggplot(data = stu_qqq_SG_converted,\n       aes(x = READ)) +\n  geom_density(color = \"#EB7C69\", size = 0.6, fill= \"#EB7C69\", alpha = 0.4) +\n  coord_cartesian(xlim = c(0,1000)) +\n  geom_vline(aes(xintercept = mean(READ)),\n             color = \"red\", alpha = 0.8, linewidth = 0.7, linetype = \"dashed\") +\n  annotate(\"text\", x = 400, y = 0.0035,\n           label = paste(\"Mean=\", \n                         round(mean(stu_qqq_SG_converted$READ, na.rm=T), 2)),\n           color = \"red\", size = 3) +\n  geom_vline(aes(xintercept = median(READ)),\n             color= \"grey50\", linewidth = 0.7, linetype = \"solid\") +\n  annotate(\"text\", x = 800, y = 0.0035,\n           label = paste(\"Median=\", \n                         round(median(stu_qqq_SG_converted$READ, na.rm=T), 2)),\n           color = \"grey20\", size = 3) +  \n  geom_boxplot(width = 0.0005, fill = \"white\", alpha = 0.5,\n               position = position_nudge(y = -0.0005)) +\n  theme_minimal()+\n  labs(title=\"Distribution of Performance in Reading\",\n       y = \"density\") +\n  theme(axis.title.x = element_blank(),\n        plot.title=element_text(size= 12),\n        axis.text = element_text(size= 8)) \n  \n# Distribution of Performance in Science\nP3 &lt;- ggplot(data = stu_qqq_SG_converted,\n       aes(x = SCIENCE)) +\n  geom_density(color = \"#FDA638\", size = 0.6, fill= \"#FDA638\", alpha = 0.4) +\n  coord_cartesian(xlim = c(0,1000)) +\n  geom_vline(aes(xintercept = mean(SCIENCE)),\n             color = \"red\", alpha = 0.8, linewidth = 0.7, linetype = \"dashed\") +\n  annotate(\"text\", x = 400, y = 0.0035,\n           label = paste(\"Mean=\", \n                         round(mean(stu_qqq_SG_converted$SCIENCE, na.rm=T), 2)),\n           color = \"red\", size = 3) +\n  geom_vline(aes(xintercept = median(SCIENCE)),\n             color= \"grey50\", linewidth = 0.7, linetype = \"solid\") +\n  annotate(\"text\", x = 800, y = 0.0035,\n           label = paste(\"Median=\", \n                         round(median(stu_qqq_SG_converted$SCIENCE, na.rm=T), 2)),\n           color = \"grey20\", size = 3) +  \n  geom_boxplot(width = 0.0005, fill = \"white\", alpha = 0.5,\n               position = position_nudge(y = -0.0005)) +\n  theme_minimal()+\n  labs(title=\"Distribution of Performance in Science\") +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12),\n        axis.text = element_text(size= 8)) \n\n\nP1 / P2 / P3\n\n\n\n\n\nThe density plot for performance in Mathematics, reading and science shows a predominantly normal distribution with a slight left skew, as indicated by mean scores being lower than medians in all subjects. The box plot below reveals tight interquartile range in all subjects, which indicates that 50% of the students performed within a relatively narrow range, emphasizing a strong central tendency.\nAcross three subjects, mathematics showed the highest performance on average, followed by Science and Reading. A similar pattern is observed in the median scores. In mathematics, the right tail extends farther than the other two subjects, indicating the occurrence of maximum PV value in this subject. In reading, the box plot shows a relatively higher number of outliers at the lower range, which indicates this subject has the weakest performance among all subjects.\nThese distributions could be further explored in the context of school, gender, and socioeconomic background to determine if these factors influence the performance distribution.\n\n\n3.2 Relationship between Performance in Mathematics, Reading and Science with Gender\n\n\nCode\nP4 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= Gender, y= MATH)) +\n  geom_violin(color = \"#459395\", size = 0.6, fill= \"#459395\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +\n  scale_color_manual(values=c(\"#999999\", \"#E69F00\")) +\n  theme_minimal() +\n  labs(title=\"Mathematics\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n  \nP5 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= Gender, y= READ)) +\n  geom_violin(color = \"#EB7C69\", size = 0.6, fill= \"#EB7C69\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Reading\") + \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n  \nP6 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= Gender, y= SCIENCE)) +\n  geom_violin(color = \"#FDA638\", size = 0.6, fill= \"#FDA638\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Science\") + \n  theme(axis.title.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n\n(P4 + P5 + P6) +\n    plot_annotation(title= \"Gender-Based Performance Comparison \",\n                    theme = theme(plot.title=element_text(size= 15, hjust= 0.5)))\n\n\n\n\n\nThis EDA visualization utilizes violin plots with integrated box plots to compare academic performance by gender across Mathematics, Reading, and Science among Singaporean students. The violin plots suggest a roughly normal distribution for scores in all subjects, with slight variations. The ‘body’ of each violin is bulged in the middle, indicating a concentration of scores around the median. From the box plot, it is observed that in all subjects, the median and mean are close to each other, with the mean slightly lower, suggesting a modest left skew in the data.\nIn mathematics, the plots indicate males performed slightly better than females, with a slightly higher median and mean. The outliers at the higher end of the scores suggest that some students achieve excellent performance. In Reading, females markedly outperform males, evidenced by higher median and mean values, and a wider distribution at the higher end of scores. Science shows a near-identical performance between genders, with both median and mean scores closely aligned, suggesting minimal gender variation in this subject.\nIn general, performance across different subjects for male and females exhibits a slight variation.\n\n\n3.3 Relationship between Performance in Mathematics, Reading and Science with School Type\n\n\nCode\nP7 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= TypeofSchool, y= MATH)) +\n  geom_violin(color = \"#459395\", size = 0.6, fill= \"#459395\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +\n  scale_color_manual(values=c(\"#999999\", \"#E69F00\")) +\n  theme_minimal() +\n  labs(title=\"Mathematics\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n  \nP8 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= TypeofSchool, y= READ)) +\n  geom_violin(color = \"#EB7C69\", size = 0.6, fill= \"#EB7C69\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Reading\") + \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n  \nP9 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= TypeofSchool, y= SCIENCE)) +\n  geom_violin(color = \"#FDA638\", size = 0.6, fill= \"#FDA638\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Science\") + \n  theme(axis.title.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n\n(P7 + P8 + P9) +\n    plot_annotation(title= \"School-Based Performance Comparison \",\n                    theme = theme(plot.title=element_text(size= 15, hjust= 0.5)))\n\n\n\n\n\nThe EDA visualization comparing student performance across Mathematics, Reading, and Science reveals distinct patterns. Public schools exhibit a broader distribution of scores, suggesting a diverse range of performance levels. Conversely, private schools show a narrower spread, as evidenced by the shorter violin plots, yet with a wider width at the median, indicating a dense clustering of scores around the central tendency. This is further supported by the narrower interquartile range observed in private schools, pointing to a more consistent performance among their students.\nAdditionally, the mean and median scores in private schools are marginally higher than those in public schools, suggesting a slight academic advantage. Moreover, public schools display a greater number of outliers, especially in the lower score range, highlighting a segment of students with performance significantly below the average.\nThese insights suggest that while student performance in private schools is more consistent with a tendency towards higher achievement, public schools demonstrate a wider variability in student outcomes.\n\n\n3.4 Relationship between Performance in Mathematics, Reading and Science with Socioeconomic Status\n\n\nCode\nP10 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= binned_ESCS, y= MATH)) +\n  geom_violin(color = \"#459395\", size = 0.6, fill= \"#459395\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +\n  scale_color_manual(values=c(\"#999999\", \"#E69F00\")) +\n  theme_minimal() +\n  labs(title=\"Mathematics\") +  \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 8),\n        axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_x_discrete(breaks = unique(stu_qqq_SG_converted$binned_ESCS), \n                            labels = str_wrap(unique(stu_qqq_SG_converted$binned_ESCS),\n                                              width = 10))\n  \nP11 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= binned_ESCS, y= READ)) +\n  geom_violin(color = \"#EB7C69\", size = 0.6, fill= \"#EB7C69\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Reading\") + \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 8),\n        axis.text.x = element_text(angle = 45, hjust = 1)) +  \n  scale_x_discrete(breaks = unique(stu_qqq_SG_converted$binned_ESCS), \n                            labels = str_wrap(unique(stu_qqq_SG_converted$binned_ESCS),\n                                              width = 10))\n  \nP12 &lt;- ggplot(data= stu_qqq_SG_converted,\n       aes(x= binned_ESCS, y= SCIENCE)) +\n  geom_violin(color = \"#FDA638\", size = 0.6, fill= \"#FDA638\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Science\") + \n  theme(axis.title.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 8),\n        axis.text.x = element_text(angle = 45, hjust = 1)) +  \n  scale_x_discrete(breaks = unique(stu_qqq_SG_converted$binned_ESCS), \n                            labels = str_wrap(unique(stu_qqq_SG_converted$binned_ESCS),\n                                              width = 10))\n\n(P10 + P11 + P12) +\n    plot_annotation(title= \"Socioeconomic-Based Performance Comparison \",\n                    theme = theme(plot.title=element_text(size= 15, hjust= 0.5)))\n\n\n\n\n\nThe visualization assesses student performance in Mathematics, Reading, and Science against socioeconomic status, categorized as Disadvantaged, Slightly Disadvantaged, Slightly Advantaged, and Advantaged. Across all subjects, a clear pattern is observable, where students from more advantaged backgrounds tend to achieve higher scores. This is evidenced by the upward shift in both the median (the horizontal line within the box) and the mean (the red dot) as we move from Disadvantaged to Advantaged.\nFor all three subjects, the spread of scores (indicated by the width of the violin plots) is broader for Disadvantaged and Slightly Disadvantaged groups, suggesting more variability in performance. It is also observed that the violin plots widen around the 50th percentile range, alongside a narrower interquartile range, indicating a denser concentration of scores near the median as we transition from disadvantaged to advantaged socioeconomic groups.\nThis visualization suggests that socioeconomic status has a consistent correlation with academic performance across different subjects. Further analysis could delve into the specific aspects of socioeconomic advantage that contribute to these patterns.\n\n\n3.5 Relationship between performances in Mathematics and school type across different socioeconomic status\n\n\nCode\nggplot(data= stu_qqq_SG_converted,\n       aes(x=  TypeofSchool, y= MATH)) +\n  geom_boxplot(width= 0.5, outlier.colour = \"grey30\", outlier.size = 2, \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  facet_grid(~ binned_ESCS) +\n  labs(title= str_wrap(\"Comparative Analysis of Mathematics Performance by \n                       School Type Across Socioeconomic Tiers\"),\n       x = \"Type of School\") +\n  theme(plot.title=element_text(size= 12, hjust= .5),\n        axis.text = element_text(size= 10)) +\n  theme_bw()\n\n\n\n\n\nExtending the findings from the previous section, where we noted that private schools typically demonstrate higher and more consistent student performance, and students from advantaged backgrounds achieve higher scores, we now examine the impact of school type on performance across different socioeconomic levels.\nThe visualization offers a comparative analysis of Mathematics performance between public and private schools across different levels of socioeconomic status. For students classified as disadvantaged, private schools show a higher mean score, indicating a performance advantage in this group. As we move to the slightly disadvantaged socioeconomic status bracket, the mean scores for private schools remain higher, though the difference is less obvious.\nInterestingly, a shift occurs among the slightly advantaged group, where the mean scores for public schools marginally surpass those of private schools. This trend amplifies within the advantaged category, where students in public schools outperform those in private schools, as indicated by higher mean scores. These insights suggest that school type affects educational outcomes differently across socioeconomic groups."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "According to an office report as shown in the infographic below, daily mean temperature are projected to increase by 1.4 to 4.6. The objective of this exercise is to apply newly acquired visual interactivity and visualising uncertainty methods to validate the claim made in the report.\n\nFollowing steps will be used to achieve the objective:\n\nSelect a weather station and download historical daily temperature data from Meteorological Service Singapore website,\nSelect daily temperature records of a month of the year 1983, 1993, 2003, 2013 and 2023 and create an analytics-driven data visualisation,\nApply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#overview",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#overview",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "According to an office report as shown in the infographic below, daily mean temperature are projected to increase by 1.4 to 4.6. The objective of this exercise is to apply newly acquired visual interactivity and visualising uncertainty methods to validate the claim made in the report.\n\nFollowing steps will be used to achieve the objective:\n\nSelect a weather station and download historical daily temperature data from Meteorological Service Singapore website,\nSelect daily temperature records of a month of the year 1983, 1993, 2003, 2013 and 2023 and create an analytics-driven data visualisation,\nApply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-preparation",
    "title": "Take-home Exercise 3",
    "section": "2 Data preparation",
    "text": "2 Data preparation\n\n2.1 Loading R packages\nFirst, the following code chunk is used to load required R packages into our working environment:\n\npacman::p_load(tidyverse, ggridges, ggthemes, tidyverse, patchwork, ggiraph, colorspace, ggstatsplot)\n\n\n\n2.2 Importing the data\nThe code chunk below uses read_csv() of readr to import weather data into our working environment. For the purpose of this exercise, we choose October weather data from Changi station.\n\ncombined &lt;- read_csv(c(\"data/DAILYDATA_S24_198310.csv\", \n                       \"data/DAILYDATA_S24_199310.csv\", \n                       \"data/DAILYDATA_S24_200310.csv\",\n                       \"data/DAILYDATA_S24_201310.csv\",\n                       \"data/DAILYDATA_S24_202310.csv\"), \n                     col_select =c(2, 3, 4, 9, 10, 11)) \n\nRows: 155 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): Year, Month, Day, Mean Temperature (°C), Maximum Temperature (°C), ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-wraggling",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-wraggling",
    "title": "Take-home Exercise 3",
    "section": "3 Data Wraggling",
    "text": "3 Data Wraggling\nour analysis commenced with the inspection of the imported data table.\n\n\nCode\ncombined\n\n\n# A tibble: 155 × 6\n    Year Month   Day `Mean Temperature (°C)` `Maximum Temperature (°C)`\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;                      &lt;dbl&gt;\n 1  1983    10     1                    26                         27.8\n 2  1983    10     2                    28.3                       32.1\n 3  1983    10     3                    28.1                       31  \n 4  1983    10     4                    29                         32.2\n 5  1983    10     5                    29                         32.4\n 6  1983    10     6                    28                         31.5\n 7  1983    10     7                    26.3                       31.3\n 8  1983    10     8                    27.7                       32.4\n 9  1983    10     9                    28.5                       32.9\n10  1983    10    10                    28.6                       33.3\n# ℹ 145 more rows\n# ℹ 1 more variable: `Minimum Temperature (°C)` &lt;dbl&gt;\n\n\nAfter examining the data table, the data preparation issues and the corresponding actions are documented in the table below:\n\n\n\n\n\n\n\nData preparation issue\nAction\n\n\n\n\nInappropriate column types\nConvert “Year”, “Month” and “Day” to factor using as.factor()\n\n\nColumn names with spaces\nRemove spaces from column names using rename()\n\n\nDerivation of new column\nDerive the differences between Maxtemp and Mintemp\n\n\n\n\n\nCode\ncombined_data &lt;- combined %&gt;%\n  mutate(Year = as.factor(Year),         \n           Month = as.factor(Month),\n           Day = as.factor(Day)) %&gt;%\n  rename(\"Meantemp\" = \"Mean Temperature (°C)\",\n       \"Maxtemp\" = \"Maximum Temperature (°C)\",\n       \"Mintemp\" = \"Minimum Temperature (°C)\") %&gt;%\n  mutate(Difference = Maxtemp - Mintemp)\n\n\nBefore we proceed to visualization task, we use head() function to preview the processed data set.\n\n\nCode\nhead(combined_data)\n\n\n# A tibble: 6 × 7\n  Year  Month Day   Meantemp Maxtemp Mintemp Difference\n  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 1983  10    1         26      27.8    25          2.8\n2 1983  10    2         28.3    32.1    24.6        7.5\n3 1983  10    3         28.1    31      26.3        4.7\n4 1983  10    4         29      32.2    26          6.2\n5 1983  10    5         29      32.4    27.2        5.2\n6 1983  10    6         28      31.5    24.5        7  \n\n\nWe noted that the data is recorded in daily interval. The following code is used to derive the average mean, maximum and minimum temperature of the month, which will be used in subsequent analysis.\n\n\nCode\nave_temp_by_year &lt;- combined_data %&gt;%\n  group_by(Year) %&gt;%\n  summarize(ave_meantemp = round(mean(Meantemp),2),\n            ave_mintemp = round(mean(Mintemp),2),\n            ave_maxtemp = round(mean(Maxtemp),2),\n            se = round(sd(Meantemp),2))\nave_temp_by_year \n\n\n# A tibble: 5 × 5\n  Year  ave_meantemp ave_mintemp ave_maxtemp    se\n  &lt;fct&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 1983          27.6        24.7        31.5  0.98\n2 1993          27.5        24.7        31.6  0.94\n3 2003          27.6        24.6        31.1  1.15\n4 2013          27.7        24.6        31.4  0.78\n5 2023          29.0        26.2        33.3  0.91"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-visualization",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-visualization",
    "title": "Take-home Exercise 3",
    "section": "4 Data visualization",
    "text": "4 Data visualization\nThe graph illustrates the annual variation in temperature, focusing on the mean temperature, maximum temperature, and minimum temperature over the years. Each point on the graph represents the average mean temperature for a specific year, accompanied by a blue shaded region indicating the 95% confidence interval. The grey ribbon surrounding the mean temperature point shows the range between the average minimum and maximum temperatures.\nAs user hovers over each data point, a tooltip displays detailed information for that particular year, providing insights into the average mean temperature, average maximum temperature, average minimum temperature, and the associated 95% confidence interval.\n\n\nCode\n# customise tooltip content\nave_temp_by_year$tooltip &lt;- c(paste0(\"Year: \", ave_temp_by_year$Year,\n        \"\\nAve mean temp: \", ave_temp_by_year$ave_meantemp, \"°C\",\n        \"\\nAve min temp: \", ave_temp_by_year$ave_mintemp, \"°C\",\n        \"\\nAve max temp: \", ave_temp_by_year$ave_maxtemp, \"°C\",\n        \"\\n95% CI: [\", ave_temp_by_year$ave_meantemp - 1.96 * ave_temp_by_year$se, \", \", ave_temp_by_year$ave_meantemp + 1.96 * ave_temp_by_year$se, \"]\"))\n\n# customise tooltip css\ntooltip_css &lt;- \n\"background-color: white;\ncolor: black;\nfont-family: 'Poppins', sans-serif;\nborder: 1px dashed #3F3E3C;\nfont-size: 14px;\npadding: 5px;\";\n\np &lt;- ggplot(ave_temp_by_year, aes(x = Year)) +\n\n# Plot min to max temp\n  geom_ribbon(\n    aes(x = Year,\n        ymin = ave_mintemp,\n        ymax = ave_maxtemp,\n        group = 1),\n    alpha = 0.4,\n    fill=\"grey80\") +\n  \n# Plot 95% confidence interval of mean\n  geom_ribbon(\n    aes(x = Year,\n        ymin = ave_meantemp-1.96*se,\n        ymax = ave_meantemp+1.96*se,\n        group = 1),\n    alpha = 0.4,\n    fill=\"lightblue\") +\n  \n# Plot point for mean\n  geom_point_interactive(aes(y = ave_meantemp,tooltip = ave_temp_by_year$tooltip), \n             color = \"black\") +\n  \n# plot mean line \n  geom_line(aes(x = Year,\n                y = ave_meantemp,\n                group = 1),\n            color = \"black\",\n            size = 0.8,\n            alpha = 0.6) +\n  \n  labs(title = \"Analyzing Temperature Patterns Over the Decades (1983-2023)\",\n       y = \"Temperature (°C)\")  +\n  coord_cartesian(ylim = c(20, 40))+\n  theme_minimal() \n\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    \n    opts_tooltip(    \n      css = tooltip_css)) \n)  \n\n\n\n\n\n\nThe figure above shows a consistent daily mean temperature from 1983 to 2013, followed by an increase from 2013 to 2023. To validate these observed temperature trends, we move on to confirmatory data analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#confirmatory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#confirmatory-data-analysis",
    "title": "Take-home Exercise 3",
    "section": "5 Confirmatory data analysis",
    "text": "5 Confirmatory data analysis\nOur visualization from the previous section shows a consistent daily mean temperature from 1983 to 2013, followed by an increase from 2013 to 2023. To validate these observed temperature trends, we move on to confirmatory data analysis.\nA normality assumption test is first conducted for mean temperature accross different years. Using confidence level of 95%, we test the following hypothesis:\nH0: The observed distribution resembles normal distribution.\nH1: The observed distribution failed to resemble normal distribution.\n\n5.1 Distribution\nFigure below are ridgelines plots showing the distribution of temperature across different years.\n\n\nCode\nd_mean &lt;- ggplot(combined_data, \n       aes(x = Meantemp, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 1.5,\n    rel_min_height = 0.01,\n    fill = lighten(\"tan\", .3),\n    color = \"snow\",\n    alpha = 0.6) +\n    scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Mean\")  +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n\n\nd_min &lt;- ggplot(combined_data, \n       aes(x = Mintemp, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 1.5,\n    rel_min_height = 0.01,\n    fill = lighten(\"tan\", .3),\n    color = \"snow\",\n    alpha = 0.6) +\n    scale_x_continuous(\n    name = \"Temperature\",\n    expand = c(0, 0)) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Min\")  +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10),\n        axis.title.x = element_text(size= 11)) \n\nd_max &lt;-ggplot(combined_data, \n       aes(x = Maxtemp, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 1.5,\n    rel_min_height = 0.01,\n    fill = lighten(\"tan\", .3),\n    color = \"snow\",\n    alpha = 0.6) +\n    scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Max\")  +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10)) \n\nd_mean + d_min + d_max\n\n\n\nMeanMinMax\n\n\n\n\nCode\nggplot(combined_data, \n       aes(x = Meantemp, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 1.5,\n    rel_min_height = 0.01,\n    fill = lighten(\"tan\", .3),\n    color = \"snow\",\n    alpha = 0.6) +\n    scale_x_continuous(expand = c(0, 0)) +\n  theme_ridges() +\n  labs(title = \"Distribution of October daily mean temperature across different years\",\n       x = \"Mean Temperature (°C)\")  +\n  theme(axis.title.x=element_text(size= 10.5, hjust= 0.5),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 13, hjust= 0.5),\n        axis.text = element_text(size= 9)) \n\n\n\n\n\n\n\n\n\nCode\nggplot(combined_data, \n       aes(x = Mintemp, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 1.5,\n    rel_min_height = 0.01,\n    fill = lighten(\"tan\", .5),\n    color = \"snow\",\n    alpha = 0.6) +\n    scale_x_continuous(expand = c(0, 0)) +\n  theme_ridges() +\n  labs(title = \"Distribution of October daily minimum temperature across different years\",\n       x = \"Min Temperature (°C)\")  +\n  theme(axis.title.x=element_text(size= 10.5, hjust= 0.5),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 13, hjust= 0.5),\n        axis.text = element_text(size= 9)) \n\n\n\n\n\n\n\n\n\nCode\nggplot(combined_data, \n       aes(x = Maxtemp, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 1.5,\n    rel_min_height = 0.01,\n    fill = lighten(\"tan\", .7),\n    color = \"snow\",\n    alpha = 0.6) +\n    scale_x_continuous(expand = c(0, 0)) +\n  theme_ridges() +\n  labs(title = \"Distribution of October daily maximum temperature across different years\",\n       x = \"Max Temperature (°C)\")  +\n  theme(axis.title.x=element_text(size= 10.5, hjust= 0.5),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 13, hjust= 0.5),\n        axis.text = element_text(size= 9)) \n\n\n\n\n\n\n\n\nThe ridgeline plot visually indicates a normal distribution; however, to validate this observation, we performed a Shapiro-Wilk test.\n\n\n5.2 Shapiro-Wilk Test\n\n\nCode\n# Assuming your dataset is named 'data'\nshapiro_results &lt;- combined_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Mean_p_value = shapiro.test(Meantemp)$p.value,\n            Min_p_value = shapiro.test(Mintemp)$p.value,\n            Max_p_value = shapiro.test(Maxtemp)$p.value,)\n\nknitr::kable(shapiro_results, format = 'html')\n\n\n\n\n\nYear\nMean_p_value\nMin_p_value\nMax_p_value\n\n\n\n\n1983\n0.2334118\n0.2551016\n0.0128211\n\n\n1993\n0.2532922\n0.3819273\n0.0001435\n\n\n2003\n0.6588778\n0.1515869\n0.0006685\n\n\n2013\n0.2972025\n0.1924884\n0.2279875\n\n\n2023\n0.0957024\n0.0670380\n0.0001276\n\n\n\n\n\n\n\nThe table above shows at least one result from Shapiro-Wilk test yields p-value of lesser than significant level 0.05, we conclude that the sample failed to confirm normality. Therefore, we use non-parametric test in the subsequent analysis.\n\n\n5.3 ANOVA\nA non-parametric Kruskal-Wallis Test is conducted to determine if there is any significant difference in average daily mean, mininum, maximum temperature across different years. The following hypothesis are used in the respective test:\nMean\nH0 : Average of daily mean temperature across different years are equal\nH1 : Average of daily mean temperature across different years are not equal\nMinimum\nH0 : Average of daily minimum temperature across different years are equal\nH1 : Average of daily minimum temperature across different years are not equal\nMaximum\nH0 : Average of daily maximum temperature across different years are equal\nH1 : Average of daily maximum temperature across different years are not equal\n\nMeanMinMax\n\n\n\n\nCode\nggbetweenstats(\n  data = combined_data,\n  x = Year, \n  y = Meantemp,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  \n)\n\n\n\n\n\n\n\n\n\nCode\nggbetweenstats(\n  data = combined_data,\n  x = Year, \n  y = Mintemp,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode\nggbetweenstats(\n  data = combined_data,\n  x = Year, \n  y = Maxtemp,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nAs the p value is consistently &lt; significant level of 0.05, we conclude that there is significant difference in average daily mean, minimum and maximum temperature across different years.\nUpon close examination through pairwise comparison, we noted that there is no significant difference between 1983 to 2013. However, for year 2023, there is a significant increase in daily temperature as compared to 2013.\nIt is important to note that while the difference between 2013 and 2023 is statistically significant, the actual temperature difference falls within 1.5°C to 1.7°C. This gradual increase aligns with the broader claim of projected temperature rise."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#conclusion",
    "title": "Take-home Exercise 3",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nNevertheless, due to the limitation of this analysis, we are unable to provide a conclusive answer regarding the future projection increment of temperatures by 1.4°C to 4.6°C. The absence of projected data hinders our ability to forecast with certainty.\nIn summary, our exploration of historical temperature trends provides valuable insights, but the missing piece of future projections leaves us with an incomplete picture. Further research with additional data is essential for a comprehensive understanding of future temperature trends."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "In this hands-on exercise, we learn to create the followings data visualisation by using R packages:\n\ncalender heatmap by using ggplot2 functions\n\ncycle plot by using ggplot2 function\nslopegraph\nhorizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "In this hands-on exercise, we learn to create the followings data visualisation by using R packages:\n\ncalender heatmap by using ggplot2 functions\n\ncycle plot by using ggplot2 function\nslopegraph\nhorizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6",
    "section": "2 Installing and Launching R Packages",
    "text": "2 Installing and Launching R Packages\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, \n               readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 6",
    "section": "3 Plotting Calendar Heatmap",
    "text": "3 Plotting Calendar Heatmap\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n3.1 Importing the data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\nFirst, the code chunk below is used to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n3.2 Examining the data structure\nkable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\n\ntz field stores time zone of the source IP address.\n\n\n\n3.3 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n3.4 Plotting the Calendar Heatmaps\nBefore plotting the heatmaps, the following code is used to count the total number of attacks on a particular hour and weekday:\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nLet us examine the data frame.\n\nkable(head(grouped))\n\n\n\n\nwkday\nhour\nn\n\n\n\n\nSaturday\n0\n1081\n\n\nSaturday\n1\n1053\n\n\nSaturday\n2\n1088\n\n\nSaturday\n3\n1130\n\n\nSaturday\n4\n1183\n\n\nSaturday\n5\n1226\n\n\n\n\n\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          linewidth = 0.1,\n          linetype = \"dashed\") + \ntheme_tufte(base_family = \"Helvetica\") +\ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"beige\",\n                    high = \"chocolate4\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\nThings to learn from the code chunk\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n3.5 Plotting Multiple Calendar Heatmaps\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, refer to the followings steps:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\nkable(head(attacks_by_country))\n\n\n\n\nsource_country\nn\npercent\n\n\n\n\nCN\n85243\n42.62171%\n\n\nUS\n48684\n24.34212%\n\n\nKR\n12648\n6.32403%\n\n\nNL\n8572\n4.28602%\n\n\nVN\n6340\n3.17002%\n\n\nTW\n3469\n1.73451%\n\n\n\n\n\nStep 2: Preparing the tidy data frame\nIn this step, attack records of the top 4 countries from attacks data frame are extracted and saved in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\nkable(head(top4_attacks))\n\n\n\n\nsource_country\nwkday\nhour\nn\n\n\n\n\nCN\nSaturday\n0\n438\n\n\nCN\nSaturday\n1\n401\n\n\nCN\nSaturday\n2\n358\n\n\nCN\nSaturday\n3\n487\n\n\nCN\nSaturday\n4\n457\n\n\nCN\nSaturday\n5\n429\n\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"beige\", \n                    high = \"chocolate4\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, \n       y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 6",
    "section": "4 Plotting Cycle Plot",
    "text": "4 Plotting Cycle Plot\nIn this section, we will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n4.1 Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\nAfter importing, we examine the first 5 row of the data by using head().\n\nhead(air,5)\n\n# A tibble: 5 × 36\n  `Month-Year`        `Republic of South Africa` Canada   USA Bangladesh Brunei\n  &lt;dttm&gt;                                   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 2000-01-01 00:00:00                       3291   5545 25906       2883   3749\n2 2000-02-01 00:00:00                       2357   6120 28262       2469   3236\n3 2000-03-01 00:00:00                       4036   6255 30439       2904   3342\n4 2000-04-01 00:00:00                       4241   4521 25378       2843   5117\n5 2000-05-01 00:00:00                       2841   3914 26163       2793   4152\n# ℹ 30 more variables: China &lt;dbl&gt;, `Hong Kong SAR (China)` &lt;dbl&gt;, India &lt;dbl&gt;,\n#   Indonesia &lt;dbl&gt;, Japan &lt;dbl&gt;, `South Korea` &lt;dbl&gt;, Kuwait &lt;dbl&gt;,\n#   Malaysia &lt;dbl&gt;, Myanmar &lt;dbl&gt;, Pakistan &lt;dbl&gt;, Philippines &lt;dbl&gt;,\n#   `Saudi Arabia` &lt;dbl&gt;, `Sri Lanka` &lt;dbl&gt;, Taiwan &lt;dbl&gt;, Thailand &lt;dbl&gt;,\n#   `United Arab Emirates` &lt;dbl&gt;, Vietnam &lt;dbl&gt;, `Belgium & Luxembourg` &lt;dbl&gt;,\n#   CIS &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Ireland &lt;dbl&gt;,\n#   Italy &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Spain &lt;dbl&gt;, Switzerland &lt;dbl&gt;, …\n\n\n\n\n4.2 Data Preparation\nStep 1: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nStep 2: Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam) from 2010 onwards.\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nStep 3: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n4.3 Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.background = element_rect(fill=\"white\", color= \"grey50\"),\n        panel.grid = element_line(color= \"grey80\", linetype = \"dashed\", linewidth = 0.1),\n        strip.background = element_rect(fill = \"grey50\"),\n        strip.text = element_text(colour = \"white\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-slopegraph",
    "title": "Hands-on Exercise 6",
    "section": "5 Plotting Slopegraph",
    "text": "5 Plotting Slopegraph\nIn this section we will learn how to plot a slopegraph by using CGPfunctions.Refer to Using newggslopegraph to learn more about the function and read more about newggslopegraph() and arguments.\n\n5.1 Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\nkable(head(rice))\n\n\n\n\nCountry\nYear\nYield\nProduction\n\n\n\n\nChina\n1961\n20787\n56217601\n\n\nChina\n1962\n23700\n65675288\n\n\nChina\n1963\n26833\n76439280\n\n\nChina\n1964\n28289\n85853780\n\n\nChina\n1965\n29667\n90705630\n\n\nChina\n1966\n31445\n98403990\n\n\n\n\n\n\n\n5.2 Plotting the slopegraph\nThe following code chunk will be used to plot a slopegraph.\n\nDefaultChanging line colourHighlighting line colour\n\n\n\nrice %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;%\n  filter(Year %in% c(1970, 1980, 1990)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1970-1990\",\n                Caption = NULL)\n\n\n\n\n\n\n\nrice %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;%\n  filter(Year %in% c(1970, 1980, 1990)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1970-1990\",\n                Caption = NULL,\n                LineColor = \"grey\",\n                LineThickness = 0.8)\n\n\n\n\n\n\n\nrice %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;%\n  filter(Year %in% c(1970, 1980, 1990)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1970-1980\",\n                Caption = NULL,\n                LineColor = c(\"Malaysia\" = \"gold\", \"Thailand\" = \"blue\"),\n                LineThickness = 0.8)\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above\n\nFor effective data visualisation design, as.factor() is used convert the value type of Year field from numeric to factor.\nBy default, the line is colourful; after highlighting the specific line using LineColor, the other line will turn grey."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section1",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section1",
    "title": "Hands-on Exercise 6",
    "section": "3 Plotting Calendar Heatmap",
    "text": "3 Plotting Calendar Heatmap\nBy the end of this section, we will be able to plot a calender heatmap by using ggplot2 functions and its extension. On top of that, we will learn to derive specific date and time related field by using base R and lubridate packages.\n\n3.1 Importing the data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\nFirst, the code chunk below is used to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\nRows: 199999 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): source_country, tz\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.2 Examining the data structure\nkable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\n\ntz field stores time zone of the source IP address.\n\n\n\n3.3 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n3.4 Plotting the Calendar Heatmaps\nBefore plotting the heatmaps, the following code is used to count the total number of attacks on a particular hour and weekday:\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nLet us examine the data frame.\n\nkable(head(grouped))\n\n\n\n\nwkday\nhour\nn\n\n\n\n\nSaturday\n0\n1081\n\n\nSaturday\n1\n1053\n\n\nSaturday\n2\n1088\n\n\nSaturday\n3\n1130\n\n\nSaturday\n4\n1183\n\n\nSaturday\n5\n1226\n\n\n\n\n\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          linewidth = 0.1,\n          linetype = \"dashed\") + \ntheme_tufte(base_family = \"Helvetica\") +\ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"beige\",\n                    high = \"chocolate4\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\nThings to learn from the code chunk\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n3.5 Plotting Multiple Calendar Heatmaps\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, refer to the followings steps:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\nkable(head(attacks_by_country))\n\n\n\n\nsource_country\nn\npercent\n\n\n\n\nCN\n85243\n42.62171%\n\n\nUS\n48684\n24.34212%\n\n\nKR\n12648\n6.32403%\n\n\nNL\n8572\n4.28602%\n\n\nVN\n6340\n3.17002%\n\n\nTW\n3469\n1.73451%\n\n\n\n\n\nStep 2: Preparing the tidy data frame\nIn this step, attack records of the top 4 countries from attacks data frame are extracted and saved in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\nkable(head(top4_attacks))\n\n\n\n\nsource_country\nwkday\nhour\nn\n\n\n\n\nCN\nSaturday\n0\n438\n\n\nCN\nSaturday\n1\n401\n\n\nCN\nSaturday\n2\n358\n\n\nCN\nSaturday\n3\n487\n\n\nCN\nSaturday\n4\n457\n\n\nCN\nSaturday\n5\n429\n\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"beige\", \n                    high = \"chocolate4\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, \n       y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section2",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section2",
    "title": "Hands-on Exercise 6",
    "section": "4 Plotting Cycle Plot",
    "text": "4 Plotting Cycle Plot\nIn this section, we will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n4.1 Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\nAfter importing, we examine the first 5 row of the data by using head().\n\nhead(air,5)\n\n# A tibble: 5 × 36\n  `Month-Year`        `Republic of South Africa` Canada   USA Bangladesh Brunei\n  &lt;dttm&gt;                                   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 2000-01-01 00:00:00                       3291   5545 25906       2883   3749\n2 2000-02-01 00:00:00                       2357   6120 28262       2469   3236\n3 2000-03-01 00:00:00                       4036   6255 30439       2904   3342\n4 2000-04-01 00:00:00                       4241   4521 25378       2843   5117\n5 2000-05-01 00:00:00                       2841   3914 26163       2793   4152\n# ℹ 30 more variables: China &lt;dbl&gt;, `Hong Kong SAR (China)` &lt;dbl&gt;, India &lt;dbl&gt;,\n#   Indonesia &lt;dbl&gt;, Japan &lt;dbl&gt;, `South Korea` &lt;dbl&gt;, Kuwait &lt;dbl&gt;,\n#   Malaysia &lt;dbl&gt;, Myanmar &lt;dbl&gt;, Pakistan &lt;dbl&gt;, Philippines &lt;dbl&gt;,\n#   `Saudi Arabia` &lt;dbl&gt;, `Sri Lanka` &lt;dbl&gt;, Taiwan &lt;dbl&gt;, Thailand &lt;dbl&gt;,\n#   `United Arab Emirates` &lt;dbl&gt;, Vietnam &lt;dbl&gt;, `Belgium & Luxembourg` &lt;dbl&gt;,\n#   CIS &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Ireland &lt;dbl&gt;,\n#   Italy &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Spain &lt;dbl&gt;, Switzerland &lt;dbl&gt;, …\n\n\n\n\n4.2 Data Preparation\nStep 1: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nStep 2: Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam) from 2010 onwards.\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nStep 3: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n4.3 Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.background = element_rect(fill=\"white\", color= \"grey50\"),\n        panel.grid = element_line(color= \"grey80\", linetype = \"dashed\", linewidth = 0.1),\n        strip.background = element_rect(fill = \"grey50\"),\n        strip.text = element_text(colour = \"white\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section3",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section3",
    "title": "Hands-on Exercise 6",
    "section": "5 Plotting Slopegraph",
    "text": "5 Plotting Slopegraph\nIn this section we will learn how to plot a slopegraph by using CGPfunctions.Refer to Using newggslopegraph to learn more about the function and read more about newggslopegraph() and arguments.\n\n5.1 Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\nRows: 550 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (3): Year, Yield, Production\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nkable(head(rice))\n\n\n\n\nCountry\nYear\nYield\nProduction\n\n\n\n\nChina\n1961\n20787\n56217601\n\n\nChina\n1962\n23700\n65675288\n\n\nChina\n1963\n26833\n76439280\n\n\nChina\n1964\n28289\n85853780\n\n\nChina\n1965\n29667\n90705630\n\n\nChina\n1966\n31445\n98403990\n\n\n\n\n\n\n\n5.2 Plotting the slopegraph\nThe following code chunk will be used to plot a slopegraph.\n\nDefaultChanging line colourHighlighting line colour\n\n\n\nrice %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;%\n  filter(Year %in% c(1970, 1980, 1990)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1970-1990\",\n                Caption = NULL)\n\n\n\n\n\n\n\nrice %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;%\n  filter(Year %in% c(1970, 1980, 1990)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1970-1990\",\n                Caption = NULL,\n                LineColor = \"grey\",\n                LineThickness = 0.8)\n\n\n\n\n\n\n\nrice %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;%\n  filter(Year %in% c(1970, 1980, 1990)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1970-1980\",\n                Caption = NULL,\n                LineColor = c(\"Malaysia\" = \"gold\", \"Thailand\" = \"blue\"),\n                LineThickness = 0.8)\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above\n\nFor effective data visualisation design, as.factor() is used convert the value type of Year field from numeric to factor.\nBy default, the line is colourful; after highlighting the specific line using LineColor, the other line will turn grey."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#reference",
    "title": "Hands-on Exercise 6",
    "section": "7 Reference",
    "text": "7 Reference\n\nKam, T.S.(2023) Visualising and Analysing Time-oriented Data\nKam, T.S.(2023) Time on the Horizon: ggHoriPlot methods"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section4",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#Section4",
    "title": "Hands-on Exercise 6",
    "section": "6 Plotting horizon chart",
    "text": "6 Plotting horizon chart\nA horizon graph is an analytical graphical method specially designed for visualising large numbers of time-series. It aims to overcome the issue of visualising highly overlapping time-series as shown in the figure below.\nA horizon graph essentially an area chart that has been split into slices and the slices then layered on top of one another with the areas representing the highest (absolute) values on top. Each slice has a greater intensity of colour based on the absolute value it represents.\nIn this section, we will learn how to plot a horizon graph by using ggHoriPlot package.Refer to this link to learn more about the functions of ggHoriPlot package. Next, read geom_horizon() to learn more about the usage of its arguments. :::\n\n6.1 Data Import\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used.\nUse the code chunk below to import the AVERP.csv file into R environment.\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))\n\nRows: 7452 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Date, Consumer Items\ndbl (1): Values\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNext, we examine the first 5 row of data table using head()\n\nhead(averp,5)\n\n# A tibble: 5 × 3\n  Date       `Consumer Items`               Values\n  &lt;date&gt;     &lt;chr&gt;                           &lt;dbl&gt;\n1 2014-01-01 Wholemeal Bread (Per 400 Gram)   2.05\n2 2014-02-01 Wholemeal Bread (Per 400 Gram)   2.05\n3 2014-03-01 Wholemeal Bread (Per 400 Gram)   2.04\n4 2014-04-01 Wholemeal Bread (Per 400 Gram)   2.04\n5 2014-05-01 Wholemeal Bread (Per 400 Gram)   2.05\n\n\n\nThing to learn from the code chunk above\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\n6.2 Plotting the horizon graph\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'YlOrBr') +      #run colorspace::hcl_palettes() to list avai palettes\n  theme(panel.spacing.y=unit(0, \"lines\"), \n        strip.text.y = element_text(\n    size = 6, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=8),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), \n                 date_breaks = \"6 month\", \n                 date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "title": "In-class Exercise 6: Horizon Plot",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#installing-and-launching-r-packages",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#installing-and-launching-r-packages",
    "title": "In-class Exercise 6: Horizon Plot",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#importing-the-data",
    "title": "In-class Exercise 6: Horizon Plot",
    "section": "2 Importing the data",
    "text": "2 Importing the data\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))\n\n\nWhat do the code chunk above do?\n\nimport csv data file\nconvert date column from ‘chr’ to ‘date’\n\n\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')\n\n\n\n\n\n\nif the data consists of negative & positive values, use diverging colour\nif there is only intensity, use only one colour"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html",
    "title": "Hands-on Exercise 7a",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, we learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#overview",
    "title": "Hands-on Exercise 7a",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, we learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#getting-started",
    "title": "Hands-on Exercise 7a",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\nR packages for Choropleth mapping\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\ntmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#importing-data-into-r",
    "title": "Hands-on Exercise 7a",
    "section": "3 Importing Data into R",
    "text": "3 Importing Data into R\n\n3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n3.2 Importing Geospatial Data into R\nWe first uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\ljjiaa\\ISSS608\\Hands-on_Ex\\Hands-on_Ex7\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThen, we examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n3.4 Data Preparation\nBefore a thematic map can be prepared, we need to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.4.1 Data wrangling\nThe following code chunk were used to transpose the data table:\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+ rowSums(.[13:15]))%&gt;%\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`)/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\nWhat do the code do?\nThe above code use the following functions to perform the following task:\n\npivot_wider() of tidyr package\nmutate(), filter(), group_by() and select() of dplyr package\n\nTask\n\nFilters data for 2020.\nAggregates population sums by grouping variables (PA, SZ, AG).\nPivot the dataset to widen age groups into columns with their population sums.\nCalculates population sums for different age categories: young, economically active, and aged.\nComputes the total population and dependency ratio (sum of young and aged populations divided by the economically active population).\n\n\n\n\n3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n            .funs = list(~toupper(.))) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nThing to learn from the code chunk above:\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 7a",
    "section": "4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\nThing to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following tabset, tmap functions that used to plot these elements will be discussed.\n\nbase mapusing tm_polygons()using tm_fill()using tm_border()\n\n\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape() is used to define the input data (i.e mpsz_pop2020)\n\ntm_polygons() is used to draw the planning subzone polygons\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nThings to learn from tm_polygons()\nBy default,\n\nInterval binning used to draw the choropleth map is called “pretty”. Refer to sub-section 4.3 for a detailed discussion of the data classification methods supported by tmap.\nColour scheme used is YlOrRd of ColorBrewer. Refer to sub-section 4.4 for more color scheme.\nMissing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_borders(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shaded according to the respective dependecy values. However,there is no border to segregate different planning subzones.\n\n\nTo add the boundary of the planning subzones, tm_borders() will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 2,  alpha = 0.5, col = \"grey60\")\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe arguments for tm_borders() are:\n\nalpha - define transparency number between 0 (totally transparent) and 1 (not transparent) (Default=1).\ncol - border colour,\nlwd - border line width. The default is 1, and\nlty - border line type. The default is “solid”.\n\n\n\n\n\n\n4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks. More details can be found here.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n4.3.1 Plotting choropleth maps with built-in classification methods\n\njenksequalkmeans\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 8,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 8,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method (jenks tab) are more evenly distributed then equal data classification method.\n\n\nIn the code chunk below, kmeans data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 8,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIt is observed that when n increases, the maps show more detailed differentiation between areas based on the variable. This resulted in smaller range of value for each class.\n\n\n\n4.3.2 Plotting choropleth map with custome break\nBy using break argument in of tm_fill(), the breakpoints can be set explicitly. The breaks must be specified in increasing order with a minimum and maximum.\nCode chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\nnormalreverse\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n\n\n4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios.\nColour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\nMap LegendMap styleCartographic Furniture\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nSome examples of the style are classic, white, gray, natural, cobalt, col_blind, albatross, beaver, bw and watercolor.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"cobalt\")\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below. Once the code chunk is run, the next code run without specifying style will use “white” style.\n\ntmap_style(\"classic\")\n\n\n\n4.6 Drawing Small Multiple Choropleth Maps\nIn tmap, facet maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nncolstm_facets()tmap_arrange()\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection function to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7a.html#reference",
    "title": "Hands-on Exercise 7a",
    "section": "5 Reference",
    "text": "5 Reference\n\n5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html",
    "title": "Hands-on Exercise 7b",
    "section": "",
    "text": "Proportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap.\nBy the end of this hands-on exercise, following skills will be acquired by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#overview",
    "title": "Hands-on Exercise 7b",
    "section": "",
    "text": "Proportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap.\nBy the end of this hands-on exercise, following skills will be acquired by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#getting-started",
    "title": "Hands-on Exercise 7b",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nBefore we get started, we need to ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 7b",
    "section": "3 Geospatial Data Wrangling",
    "text": "3 Geospatial Data Wrangling\n\n3.1 The data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System.\n\n\n3.2 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nRows: 306 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): NAME, ADDRESS, OUTLET TYPE\ndbl (4): POSTCODE, XCOORD, YCOORD, Gp1Gp2 Winnings\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, we use list() to examine if the data file has been imported correctly.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame.\n\n\n3.3 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\n\nNotice that a new column called geometry has been added into the data frame.\nWe display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 7b",
    "section": "4 Drawing Proportional Symbol Map",
    "text": "4 Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\n\n4.1 Basic interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n4.2 Proportional symbol map\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n4.3 Using the colour visual attribute\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n4.4 Multiple plots\nThe argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, switch tmap’s Viewer back to plot mode by using the code chunk below.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7b.html#reference",
    "title": "Hands-on Exercise 7b",
    "section": "5 Reference",
    "text": "5 Reference\n\n5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html",
    "title": "Hands-on Exercise 7c",
    "section": "",
    "text": "In this in-class exercise, we will gain hands-on experience on using appropriate R methods to plot analytical maps.\nBy the end of this in-class exercise, we will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#overview",
    "title": "Hands-on Exercise 7c",
    "section": "",
    "text": "In this in-class exercise, we will gain hands-on experience on using appropriate R methods to plot analytical maps.\nBy the end of this in-class exercise, we will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#installing-and-loading-packages",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#installing-and-loading-packages",
    "title": "Hands-on Exercise 7c",
    "section": "2 Installing and loading packages",
    "text": "2 Installing and loading packages\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#importing-data",
    "title": "Hands-on Exercise 7c",
    "section": "3 Importing data",
    "text": "3 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 7c",
    "section": "4 Basic Choropleth Mapping",
    "text": "4 Basic Choropleth Mapping\n\n4.1 Visualising distribution of non-functional water point\nPlot a choropleth map showing the distribution of non-function water point by LGA\n\nwpfunctional &lt;- tm_shape(NGA_wp)+ \n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE) \n\ntotalwp &lt;- tm_shape(NGA_wp)+ \n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(wpfunctional,totalwp, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 7c",
    "section": "5 Choropleth Map for Rates",
    "text": "5 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n5.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n5.2 Plotting map of rate\n\ntm_shape(NGA_wp)+ \n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7c.html#extreme-value-maps",
    "title": "Hands-on Exercise 7c",
    "section": "6 Extreme Value Maps",
    "text": "6 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n6.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n6.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n6.1.2 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\nWhy writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n\n6.1.3 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"YlOrBr\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n6.1.4 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n6.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot(width = 0.5) +\n  theme_minimal()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n6.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n6.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n6.2.3 Test drive the newly created function\nLet’s test the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n6.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Oranges\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#overview",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 8",
    "section": "2 Installing and launching R packages",
    "text": "2 Installing and launching R packages\nIn this hands-on exercise, these R packages will be installed and launched:\nFor network data modelling and visualisation:\n\nigraph\ntidygraph\nggraph\nvisNetwork\n\nFor time data handling and wrangling:\n\ntidyverse\nlubridate\n\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#the-data",
    "title": "Hands-on Exercise 8",
    "section": "3 The Data",
    "text": "3 The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets:\n\ncontains the nodes data\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\ncontains the edges (also know as link) data\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n3.1 Importing network data from files\nIn this step, we import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n3.2 Reviewing the imported data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n3.3 Wrangling time\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nThings to learn from the code chunk above\n\nboth dmy() and wday() are functions of lubridate package.\n\ndmy() transforms the SentDate to Date data type.\n\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\n\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n3.4 Reviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n3.5 Wrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\nThings to learn from the code chunk above\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n3.6 Reviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 8",
    "section": "4 Creating network objects using tidygraph",
    "text": "4 Creating network objects using tidygraph\nIn this section, we will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nRefer to these two articles for more information:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n4.1 The tbl_graph object\n\n\n\n\ntbl_graph()\nas_tbl_graph()\n\n\n\n\nDescription\ncreates a tbl_graph network object from nodes and edges data.\nconverts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\n\nSupported Data/Objects\n\nA node data.frame\nAn edge data.frame\n\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\n4.2 The dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n4.3 Using tbl_graph() to build tidygraph data model\nIn this section, you will use tbl_graph() of tidygraph package to build an tidygraph’s network graph data.frame.\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n4.4 Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 1372 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n4.5 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 8",
    "section": "5 Plotting Static Network Graphs with ggraph package",
    "text": "5 Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes\n\nedges\n\nlayouts\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n5.1 Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\nThings to learn from the code chunk above\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired.\n\nBoth of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n5.2 Changing default theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n5.3 Changing the colour\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n5.4 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n \nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\nlayout argument is used to define the layout to be used.\n\n\n\n\n5.5 Modifying network nodes\nIn this section, we colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\ngeom_node_point() is equivalent in functionality to geo_point() of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\n5.6 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\ngeom_edge_link() draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n5.7 Plotting practice\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_fan2(aes(width=Weight,\n                    colour=node.Department), \n                 alpha=0.2,\n                 show.legend = FALSE,\n                 strength = 1.5) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\ngeom_edge_fan2() behaves like geom_edge_link() and draws a straight line if there are no parallel edges. But if parallel edges exists it will spread them out as arcs with different curvature.\nBy customizing colour, we colour the edges by referring to their respective departments."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#creating-facet-graphs",
    "title": "Hands-on Exercise 8",
    "section": "6 Creating facet graphs",
    "text": "6 Creating facet graphs\nIn visualising network data, faceting can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes.\nThere are three functions in ggraph to implement faceting, they are:\n\n\n\nFunction\nDescription\nQuick access\n\n\n\n\nfacet_edges()\n\nCreate small multiples based on edge attributes\nall nodes are displayed in every panel\n\nclick here\n\n\nfacet_nodes()\n\nCreate small multiples based on node attributes\neach panel represents a subset of nodes\nedges are only drawn in a panel if both terminal nodes are present\n\nclick here\n\n\nfacet_graph()\n\nCreate a grid of small multiples by node and/or edge attributes\nfaceting on two variables simultaneously.\n\n-\n\n\n\n\n6.1 Working with facet_edges()\n\nBasicChanging themeAdd frame\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n6.2 Working with facet_nodes()\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#section",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#section",
    "title": "Hands-on Exercise 8 (WIP)",
    "section": "7 22",
    "text": "7 22\n\nBasicAdd frame\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n####Changing theme\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n7.1 Working with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n7.2 Working with facet_graph()\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_graph(Weekday~Department)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#working-with-ggraphs-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#working-with-ggraphs-layouts",
    "title": "Hands-on Exercise 8 (WIP)",
    "section": "6 Working with ggraph’s layouts",
    "text": "6 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n \nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\nlayout argument is used to define the layout to be used.\n\n\n\n6.1 Modifying network nodes\nIn this section, we colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\ngeom_node_point() is equivalent in functionality to geo_point() of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\n6.2 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\ngeom_edge_link() draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n6.3 Plotting practice\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_fan2(aes(width=Weight,\n                    colour=node.Department), \n                 alpha=0.2,\n                 show.legend = FALSE,\n                 strength = 1.5) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\ngeom_edge_fan2() behaves like geom_edge_link() and draws a straight line if there are no parallel edges. But if parallel edges exists it will spread them out as arcs with different curvature.\nBy customizing colour, we colour the edges by referring to their respective departments."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#network-metrics-analysis",
    "title": "Hands-on Exercise 8",
    "section": "7 Network Metrics Analysis",
    "text": "7 Network Metrics Analysis\n\n7.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\n7.2 Visualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n7.3 Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 8",
    "section": "8 Building Interactive Network Graph with visNetwork",
    "text": "8 Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n8.1 Data preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n8.2 Plotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n8.3 Working with layout\nIn the code chunk below, nicely layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_nicely\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n8.4 Working with Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field. visNodes() is used to change the properties of the nodes. Refer to this link for the documentation.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_nicely\") %&gt;%\n  visNodes(shape = c(\"box\"),\n           opacity = 0.8,\n           size = 30,\n           label= \"label\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n8.5 Working with Edges\nIn the code run below visEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\nRefer to this link for the documentation.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_nicely\") %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 1.5)), \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visNodes(font = list(size = 20)) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n8.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n# Sort the nodes data frame by the 'id' column in ascending order\nGAStech_nodes &lt;- GAStech_nodes[order(GAStech_nodes$label), ]\n\n# create the visNetwork visualization\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_nicely\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(size = 20,\n           font = list(size = 20)) %&gt;%\n  visEdges(color = list(color = \"lightgray\", highlight = \"red\"), width = 2) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#data-preparation",
    "title": "Take-home Exercise 4",
    "section": "3 Data preparation",
    "text": "3 Data preparation\nAs this take home exercise serves as analysis of one of the module of the project, the data preparation steps is only performed once to ensure that same data set (weather_imputed_11stations.rds) are used across all members. Detailed preparation steps can be found in this link.\n\n3.1 Importing the data\nWe import the processed data set using read_rds(),\n\nweather &lt;- read_rds(\"data/weather_imputed_11stations.rds\")\n\n\nhead(weather)\n\n# A tibble: 6 × 15\n  Station    Date        Year Month   Day `Daily Rainfall Total (mm)`\n  &lt;chr&gt;      &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;\n1 Ang Mo Kio 2021-01-01  2021     1     1                        94.4\n2 Ang Mo Kio 2021-01-02  2021     1     2                       114. \n3 Ang Mo Kio 2021-01-03  2021     1     3                         5.2\n4 Ang Mo Kio 2021-01-04  2021     1     4                         0  \n5 Ang Mo Kio 2021-01-05  2021     1     5                         0  \n6 Ang Mo Kio 2021-01-06  2021     1     6                         0  \n# ℹ 9 more variables: `Mean Temperature (°C)` &lt;dbl&gt;,\n#   `Maximum Temperature (°C)` &lt;dbl&gt;, `Minimum Temperature (°C)` &lt;dbl&gt;,\n#   LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, Date_mine &lt;date&gt;, Month_Name &lt;fct&gt;, Week &lt;dbl&gt;,\n#   Weekday &lt;dbl&gt;\n\n\n\n\n3.2 Convert to tsibble data frame\nThe dataset comprises daily records of rainfall and temperature gathered from various weather stations across Singapore. Initially, this data is loaded in a ‘tibble’ format. However, for our forecasting analysis, we will be using fable package, which is part of the tidyverts ecosystem and requires data in a ‘tsibble’ format. To accommodate this requirement, we will convert our tibble dataframe into a tsibble using the following code.\n\nweather_tsbl &lt;- as_tsibble(weather, key = Station, index = Date)\nweather_tsbl\n\n# A tsibble: 12,045 x 15 [1D]\n# Key:       Station [11]\n   Station    Date        Year Month   Day `Daily Rainfall Total (mm)`\n   &lt;chr&gt;      &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;\n 1 Ang Mo Kio 2021-01-01  2021     1     1                        94.4\n 2 Ang Mo Kio 2021-01-02  2021     1     2                       114. \n 3 Ang Mo Kio 2021-01-03  2021     1     3                         5.2\n 4 Ang Mo Kio 2021-01-04  2021     1     4                         0  \n 5 Ang Mo Kio 2021-01-05  2021     1     5                         0  \n 6 Ang Mo Kio 2021-01-06  2021     1     6                         0  \n 7 Ang Mo Kio 2021-01-07  2021     1     7                         1.6\n 8 Ang Mo Kio 2021-01-08  2021     1     8                        12.6\n 9 Ang Mo Kio 2021-01-09  2021     1     9                        35.4\n10 Ang Mo Kio 2021-01-10  2021     1    10                       119. \n# ℹ 12,035 more rows\n# ℹ 9 more variables: `Mean Temperature (°C)` &lt;dbl&gt;,\n#   `Maximum Temperature (°C)` &lt;dbl&gt;, `Minimum Temperature (°C)` &lt;dbl&gt;,\n#   LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, Date_mine &lt;date&gt;, Month_Name &lt;fct&gt;, Week &lt;dbl&gt;,\n#   Weekday &lt;dbl&gt;\n\n\nThe list below shows 11 unique stations.\n\nStation &lt;- weather_tsbl %&gt;% distinct(Station)\n\ndatatable(Station, \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 12,scrollX=T))\n\n\n\n\n\n\n\nWhat is tsibble?\nA tsibble consists of a time index, key, and other measured variables in a data-centric format. In tsibble,\n\nIndex is a variable with inherent ordering from past to present.\nKey is a set of variables that define observational units over time.\nEach observation should be uniquely identified by index and key.\nEach observational unit should be measured at a common interval, if regularly spaced."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#prototype",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#prototype",
    "title": "Take-home Exercise 4 (WIP)",
    "section": "4 Prototype",
    "text": "4 Prototype\n\n4.1 Time Series Analysis\n\n4.1.1 Simple time series plot\n\n# Facet plot of different station\nweather %&gt;%\n  group_by(Station) %&gt;%                      # facet\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%          #specify period\n  plot_time_series(Date, \n                   `Mean Temperature (°C)`, \n                   .facet_ncol = 3, \n                   .smooth= FALSE) \n\n\n\n\n\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%     # specify station\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%    # specify period # for &gt;1yr not stationary\n  plot_time_series(Date, \n                   `Mean Temperature (°C)`, \n                   .smooth = TRUE,\n                   .plotly_slider = TRUE)\n\n\n\n\n\nThe following plot shows the time series in months.\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%\n  summarise_by_time(Date,\n                    .by = \"month\",           #aggregated by month\n                    `Mean Temperature (°C)` = round(mean(`Mean Temperature (°C)`),2)) %&gt;%\n  plot_time_series(Date, `Mean Temperature (°C)`, .smooth = TRUE,\n                   .plotly_slider = TRUE)\n\n\n\n\n\nThe following plot highlight the different months using different colour. (note: it can only be applied for a period less than 1 year)\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2023-01\", \"2023-12\") %&gt;%\n  plot_time_series(Date, `Mean Temperature (°C)`, \n                   .smooth = TRUE,\n                   .smooth_size = 0.8,\n                   .smooth_alpha = 0.8,\n                   .color_var = month(Date, label= TRUE),\n                   .plotly_slider = TRUE,\n                   .title = \"Mean temperature for Ang Mo Kio\",\n                   .y_lab = \"(°C)\",\n                   .color_lab = \"Month\") \n\n\n\n\n\n\n\n4.1.2 ACF Diagnostics\nACF and PACF plot of daily data for a specific station\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%\n  summarise_by_time(Date,\n                    .by = \"day\",              # day, month\n                    `Mean Temperature (°C)` = round(mean(`Mean Temperature (°C)`),2)) %&gt;%\n  plot_acf_diagnostics(Date, `Mean Temperature (°C)`,\n                       .lags = \"3 years\")     # period \n\n\n\n\n\nACF and PACF plot of monthly aggregated data for a specific station\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%\n  summarise_by_time(Date,\n                    .by = \"month\",              # day, month\n                    `Mean Temperature (°C)` = round(mean(`Mean Temperature (°C)`),2)) %&gt;%\n  plot_acf_diagnostics(Date, `Mean Temperature (°C)`,\n                       .lags = \"3 years\")     # period \n\n\n\n\n\nFacet ACF and PACF plot of monthly aggregated data\n\nweather %&gt;%\n  group_by(Station) %&gt;%                       # facet\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%\n  summarise_by_time(Date,\n                    .by = \"month\",\n                    `Mean Temperature (°C)` = round(mean(`Mean Temperature (°C)`),2)) %&gt;%\n  plot_acf_diagnostics(Date, `Mean Temperature (°C)`,\n                       .facet_ncol = 1,\n                       .lags = \"3 years\")\n\n\n\n\n\n\n\n4.1.3 Seasonal and Trend decomposition using Loess\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%           #using 1 year\n  plot_stl_diagnostics(Date, `Mean Temperature (°C)`,\n        # Set features to return, desired frequency and trend\n        .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),\n        .frequency   = \"auto\",\n        .trend       = \"auto\",\n        .interactive = FALSE)\n\n\n\n\n\n\n4.1.4 Seasonality Diagnostics\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2023-01\", \"2023-12\") %&gt;%            #using 1 year\n  plot_seasonal_diagnostics(Date, `Mean Temperature (°C)`,\n                            .feature_set = c(\"week\", \"month.lbl\", \"quarter\"),\n                            .geom = \"boxplot\",\n                            .interactive = TRUE)\n\n\n\n\n\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2023-01\", \"2023-12\") %&gt;%                  #using 1 year\n  plot_seasonal_diagnostics(Date, `Daily Rainfall Total (mm)`,\n                            .feature_set = c(\"week\", \"month.lbl\", \"quarter\"),\n                            .geom = \"boxplot\",\n                            .interactive = TRUE)\n\n\n\n\n\n\n\n4.1.5 Anomaly Detection\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2023-01\", \"2023-12\") %&gt;%           #using 1 year\n  anomalize(.date_var = Date, \n            .value = `Mean Temperature (°C)`,\n            .iqr_alpha = 0.10,                #Controls the width of the \"normal\" range\n            # .max_anomalies = 0.10,\n            # .message = FALSE\n) %&gt;%\n  plot_anomalies(Date,\n                 .ribbon_alpha = 0.15, \n                 .interactive = TRUE)\n\n\n\n\n\n\n\n\n4.2 Time series forecasting\nIn this section, we will perform forecasting on the weather data set using modeltime.\nFollowing steps will be used to build forecasting mode:\n\nSplit data into training and test sets\nCreate & Fit Multiple Models\nAdd fitted models to a Model Table\nCalibrate the models to a testing set.\nPerform Testing Set Forecast & Accuracy Evaluation\nRefit the models to Full Dataset & Forecast Forward\n\nStep 1 to Step 6 below is done using data set from 1 January 2021 to 31 December 2023, as using data set from 1 January 2023 to 31 December 2023.\n\n4.2.1 Step 1 - Split data into training and test sets\n\nweather_AMK &lt;- weather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2021-01\", \"2023-12\") \n\nweather_AMK %&gt;%          \n  plot_time_series(Date, `Mean Temperature (°C)`)\n\n\n\n\n\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%\n  summarise_by_time(Date,\n                    .by = \"day\",              # day, month\n                    `Mean Temperature (°C)` = round(mean(`Mean Temperature (°C)`),2)) %&gt;%\n  plot_acf_diagnostics(Date, `Mean Temperature (°C)`,\n                       .lags = \"3 years\")     # period \n\n\n\n\n\n\n# Split Data 80/20\nsplits &lt;- initial_time_split(weather_AMK, prop = 0.8)\n\n\nsplits\n\n&lt;Training/Testing/Total&gt;\n&lt;876/219/1095&gt;\n\n\n\n\n4.2.2 Step 2 - Create & Fit Multiple Models\nModel 1: Auto ARIMA\n\nmodel_fit_arima_no_boost &lt;- arima_reg() %&gt;%\n    set_engine(engine = \"auto_arima\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date, data = training(splits))\n\nModel 2: Boosted Auto ARIMA\n\nmodel_fit_arima_boosted &lt;- arima_boost(\n    min_n = 2,\n    learn_rate = 0.015\n) %&gt;%\n    set_engine(engine = \"auto_arima_xgboost\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date + \n          as.numeric(Date) + factor(month(Date, label = TRUE), ordered = F),\n        data = training(splits))\n\nModel 3: Exponential Smoothing\n\nmodel_fit_ets &lt;- exp_smoothing() %&gt;%\n    set_engine(engine = \"ets\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date , data = training(splits))\n\nModel 4: Linear Regression\n\nmodel_fit_lm &lt;- linear_reg() %&gt;%\n    set_engine(\"lm\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ as.numeric(Date) \n        + factor(month(Date, label = TRUE), ordered = FALSE),\n        data = training(splits))\n\n\n\n4.2.3 Step 3 - Add fitted models to a Model Table\n\nmodels_tbl &lt;- modeltime_table(\n    model_fit_arima_no_boost,\n    model_fit_arima_boosted,\n    model_fit_ets,\n    model_fit_lm\n)\nmodels_tbl\n\n# Modeltime Table\n# A tibble: 4 × 3\n  .model_id .model   .model_desc                   \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                         \n1         1 &lt;fit[+]&gt; ARIMA(2,1,2)                  \n2         2 &lt;fit[+]&gt; ARIMA(1,1,2) W/ XGBOOST ERRORS\n3         3 &lt;fit[+]&gt; ETS(A,N,N)                    \n4         4 &lt;fit[+]&gt; LM                            \n\n\n\n\n4.2.4 Step 4 - Calibrate the model to a testing set\n\ncalibration_tbl &lt;- models_tbl %&gt;%\n    modeltime_calibrate(new_data = testing(splits))\n\ncalibration_tbl\n\n# Modeltime Table\n# A tibble: 4 × 5\n  .model_id .model   .model_desc                    .type .calibration_data \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                          &lt;chr&gt; &lt;list&gt;            \n1         1 &lt;fit[+]&gt; ARIMA(2,1,2)                   Test  &lt;tibble [219 × 4]&gt;\n2         2 &lt;fit[+]&gt; ARIMA(1,1,2) W/ XGBOOST ERRORS Test  &lt;tibble [219 × 4]&gt;\n3         3 &lt;fit[+]&gt; ETS(A,N,N)                     Test  &lt;tibble [219 × 4]&gt;\n4         4 &lt;fit[+]&gt; LM                             Test  &lt;tibble [219 × 4]&gt;\n\n\n\n\n4.2.5 Step 5 - Testing Set Forecast & Accuracy Evaluation\nThere are 2 critical parts to an evaluation:\n\nVisualizing the Forecast vs Test Data Set\nEvaluating the Test (Out of Sample) Accuracy\n\n5A - Visualizing the Forecast Test Visualizing the Test Error is easy to do using the interactive plotly visualization (just toggle the visibility of the models using the Legend).\n\ncalibration_tbl %&gt;%\n    modeltime_forecast(\n        new_data    = testing(splits),\n        actual_data = weather_AMK\n    ) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )\n\n\n\n\n\n5B - Accuracy Metrics We can use modeltime_accuracy() to collect common accuracy metrics. The default reports the following metrics using yardstick functions:\nMAE - Mean absolute error, mae() MAPE - Mean absolute percentage error, mape() MASE - Mean absolute scaled error, mase() SMAPE - Symmetric mean absolute percentage error, smape() RMSE - Root mean squared error, rmse() RSQ - R-squared, rsq()\n\ncalibration_tbl %&gt;%\n    modeltime_accuracy() %&gt;%\n    table_modeltime_accuracy(\n        # .interactive = interactive\n    )\n\n\n\n\n\n\n\n\n4.2.6 Step 6 - Refit to Full Dataset & Forecast Forward\n\nrefit_tbl &lt;- calibration_tbl %&gt;%\n    modeltime_refit(data = weather_AMK)\n\nrefit_tbl %&gt;%\n    modeltime_forecast(h = \"10 days\", actual_data = weather_AMK) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )\n\n\n\n\n\n\n\n4.2.7 Repeat step 1 to 6 for 12 months\n\n4.2.7.1 Step 1 - Split data into training and test sets\n\nweather_AMK_2023 &lt;- weather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2023-01\", \"2023-12\") \n\nweather_AMK_2023 %&gt;%          \n  plot_time_series(Date, `Mean Temperature (°C)`)\n\n\n\n\n\n\nweather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2023-01\", \"2023-12\") %&gt;%\n  summarise_by_time(Date,\n                    .by = \"day\",              # day, month\n                    `Mean Temperature (°C)` = round(mean(`Mean Temperature (°C)`),2)) %&gt;%\n  plot_acf_diagnostics(Date, `Mean Temperature (°C)`,\n                       .lags = \"1 years\")     # period \n\n\n\n\n\n\n# Split Data 80/20\nsplits_2023 &lt;- initial_time_split(weather_AMK_2023, prop = 0.8)\n\n\nsplits_2023\n\n&lt;Training/Testing/Total&gt;\n&lt;292/73/365&gt;\n\n\n\n\n4.2.7.2 Step 2 - Create & Fit Multiple Models\nModel 1: Auto ARIMA\n\nmodel_fit_arima_no_boost_2023 &lt;- arima_reg() %&gt;%\n    set_engine(engine = \"auto_arima\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date, data = training(splits_2023))\n\nModel 2: Boosted Auto ARIMA\n\nmodel_fit_arima_boosted_2023 &lt;- arima_boost(\n    min_n = 2,\n    learn_rate = 0.015\n) %&gt;%\n    set_engine(engine = \"auto_arima_xgboost\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date + \n          as.numeric(Date) + factor(month(Date, label = TRUE), ordered = F),\n        data = training(splits_2023))\n\nModel 3: Exponential Smoothing\n\nmodel_fit_ets_2023 &lt;- exp_smoothing() %&gt;%\n    set_engine(engine = \"ets\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date , data = training(splits_2023))\n\nModel 4: Linear Regression\n\nmodel_fit_lm_2023 &lt;- linear_reg() %&gt;%\n    set_engine(\"lm\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ as.numeric(Date) \n        + factor(month(Date, label = TRUE), ordered = FALSE),\n        data = training(splits_2023))\n\n\n\n4.2.7.3 Step 3 - Add fitted models to a Model Table\n\nmodels_tbl_2023 &lt;- modeltime_table(\n    model_fit_arima_no_boost_2023,\n    model_fit_arima_boosted_2023,\n    model_fit_ets_2023,\n    model_fit_lm_2023\n)\nmodels_tbl_2023\n\n# Modeltime Table\n# A tibble: 4 × 3\n  .model_id .model   .model_desc                   \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                         \n1         1 &lt;fit[+]&gt; ARIMA(1,1,2)(1,0,0)[7]        \n2         2 &lt;fit[+]&gt; ARIMA(1,1,2) W/ XGBOOST ERRORS\n3         3 &lt;fit[+]&gt; ETS(A,N,N)                    \n4         4 &lt;fit[+]&gt; LM                            \n\n\n\n\n4.2.7.4 Step 4 - Calibrate the model to a testing set\n\ncalibration_tbl_2023 &lt;- models_tbl %&gt;%\n    modeltime_calibrate(new_data = testing(splits_2023))\n\ncalibration_tbl_2023\n\n# Modeltime Table\n# A tibble: 4 × 5\n  .model_id .model   .model_desc                    .type .calibration_data\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                          &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;fit[+]&gt; ARIMA(2,1,2)                   Test  &lt;tibble [73 × 4]&gt;\n2         2 &lt;fit[+]&gt; ARIMA(1,1,2) W/ XGBOOST ERRORS Test  &lt;tibble [73 × 4]&gt;\n3         3 &lt;fit[+]&gt; ETS(A,N,N)                     Test  &lt;tibble [73 × 4]&gt;\n4         4 &lt;fit[+]&gt; LM                             Test  &lt;tibble [73 × 4]&gt;\n\n\n\n\n4.2.7.5 Step 5 - Testing Set Forecast & Accuracy Evaluation\nThere are 2 critical parts to an evaluation:\n\nVisualizing the Forecast vs Test Data Set\nEvaluating the Test (Out of Sample) Accuracy\n\n5A - Visualizing the Forecast Test Visualizing the Test Error is easy to do using the interactive plotly visualization (just toggle the visibility of the models using the Legend).\n\ncalibration_tbl_2023 %&gt;%\n    modeltime_forecast(\n        new_data    = testing(splits_2023),\n        actual_data = weather_AMK_2023\n    ) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )\n\n\n\n\n\n5B - Accuracy Metrics We can use modeltime_accuracy() to collect common accuracy metrics. The default reports the following metrics using yardstick functions:\nMAE - Mean absolute error, mae() MAPE - Mean absolute percentage error, mape() MASE - Mean absolute scaled error, mase() SMAPE - Symmetric mean absolute percentage error, smape() RMSE - Root mean squared error, rmse() RSQ - R-squared, rsq()\n\ncalibration_tbl_2023 %&gt;%\n    modeltime_accuracy() %&gt;%\n    table_modeltime_accuracy(\n        # .interactive = interactive\n    )\n\n\n\n\n\n\n\n\n4.2.7.6 Step 6 - Refit to Full Dataset & Forecast Forward\n\nrefit_tbl_2023 &lt;- calibration_tbl_2023 %&gt;%\n    modeltime_refit(data = weather_AMK)\n\nrefit_tbl_2023 %&gt;%\n    modeltime_forecast(h = \"10 days\", actual_data = weather_AMK_2023) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )\n\n\n\n\n\n\n\n\n4.2.8 Comparing ARIMA\nThis section compares the result of auto arima and arima for the period of 1 January 2021 to 31 December 2023.\n\n# Auto arima\nmodel_fit_autoarima &lt;- arima_reg() %&gt;%\n    set_engine(engine = \"auto_arima\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date, data = training(splits))\n\nmodel_fit_autoarima\n\nparsnip model object\n\nSeries: outcome \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      0.7994  -0.0464  -1.3154  0.3494\ns.e.  0.2239   0.1193   0.2201  0.2042\n\nsigma^2 = 0.8802:  log likelihood = -1184.2\nAIC=2378.4   AICc=2378.47   BIC=2402.27\n\n\n\n# Manual arima\nmodel_fit_manualarima &lt;- arima_reg(\n        seasonal_period          = 12,\n        non_seasonal_ar          = 3,\n        non_seasonal_differences = 1,\n        non_seasonal_ma          = 3,\n        seasonal_ar              = 1,\n        seasonal_differences     = 0,\n        seasonal_ma              = 1\n    ) %&gt;%\n    set_engine(\"arima\") %&gt;%\n    fit(`Mean Temperature (°C)` ~ Date, data = training(splits))\n\nmodel_fit_manualarima\n\nparsnip model object\n\nSeries: outcome \nARIMA(3,1,3)(1,0,1)[12] \n\nCoefficients:\n          ar1     ar2     ar3      ma1      ma2     ma3    sar1     sma1\n      -0.1952  0.6429  0.0180  -0.3194  -0.8579  0.2483  0.4129  -0.3888\ns.e.   1.3228  0.2301  0.3318   1.3194   0.7054  0.5527  4.6926   4.6940\n\nsigma^2 = 0.8824:  log likelihood = -1183.32\nAIC=2384.65   AICc=2384.85   BIC=2427.61\n\n\n\nmodels_tbl_1 &lt;- modeltime_table(\n    model_fit_autoarima,\n    model_fit_manualarima\n)\nmodels_tbl_1\n\n# Modeltime Table\n# A tibble: 2 × 3\n  .model_id .model   .model_desc            \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                  \n1         1 &lt;fit[+]&gt; ARIMA(2,1,2)           \n2         2 &lt;fit[+]&gt; ARIMA(3,1,3)(1,0,1)[12]\n\n\n\ncalibration_tbl_1 &lt;- models_tbl_1 %&gt;%\n    modeltime_calibrate(new_data = testing(splits))\n\ncalibration_tbl_1\n\n# Modeltime Table\n# A tibble: 2 × 5\n  .model_id .model   .model_desc             .type .calibration_data \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;            \n1         1 &lt;fit[+]&gt; ARIMA(2,1,2)            Test  &lt;tibble [219 × 4]&gt;\n2         2 &lt;fit[+]&gt; ARIMA(3,1,3)(1,0,1)[12] Test  &lt;tibble [219 × 4]&gt;\n\n\n\ncalibration_tbl_1 %&gt;%\n    modeltime_forecast(\n        new_data    = testing(splits),\n        actual_data = weather_AMK\n    ) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )\n\n\n\n\n\n\ncalibration_tbl_1 %&gt;%\n    modeltime_accuracy() %&gt;%\n    table_modeltime_accuracy(\n        # .interactive = interactive\n    )\n\n\n\n\n\n\n\ncalibration_tbl_1 %&gt;%\n    modeltime_residuals() %&gt;%\n    plot_modeltime_residuals(.interactive = TRUE)\n\n\n\n\n\n\nrefit_tbl_1 &lt;- calibration_tbl_1 %&gt;%\n    modeltime_refit(data = weather_AMK)\n\nrefit_tbl_1 %&gt;%\n    modeltime_forecast(h = \"10 days\", actual_data = weather_AMK) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )\n\n\n\n\n\n\n\n4.2.9 Forecasting with STL Decomposition\n\nweather_decomposed &lt;- weather %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n  filter_by_time(Date, \"2021-01\", \"2023-12\") %&gt;%           #using 1 year\ntk_stl_diagnostics(.date_var = Date, .value = `Mean Temperature (°C)`,.frequency   = \"1 year\",.trend = \"auto\" ) \n\nweather_decomposed %&gt;%\n    plot_stl_diagnostics(Date, seasadj,\n        # Set features to return, desired frequency and trend\n        .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),\n        .frequency   = \"auto\",\n        .trend       = \"auto\",\n        .interactive = FALSE)\n\n\n\n\n\n# Split Data 80/20\nsplits_decom &lt;- initial_time_split(weather_decomposed, prop = 0.8)\nsplits_decom\n\n&lt;Training/Testing/Total&gt;\n&lt;876/219/1095&gt;\n\n\n\n# adf.test(weather_decomposed$seasadj)\n\n\n# Auto arima\nmodel_fit_autoarima_decom &lt;- arima_reg() %&gt;%\n    set_engine(engine = \"auto_arima\") %&gt;%\n    fit(seasadj ~ Date, data = training(splits_decom))\n\nmodel_fit_autoarima_decom\n\nparsnip model object\n\nSeries: outcome \nARIMA(1,0,2) with non-zero mean \n\nCoefficients:\n         ar1      ma1      ma2     mean\n      0.8451  -0.4677  -0.0807  27.8111\ns.e.  0.0410   0.0544   0.0409   0.0899\n\nsigma^2 = 0.8446:  log likelihood = -1167.17\nAIC=2344.33   AICc=2344.4   BIC=2368.21\n\n\n\ncalibration_tbl_decom &lt;- modeltime_table(\n    model_fit_autoarima_decom) %&gt;%\n    modeltime_calibrate(new_data = testing(splits_decom))\n\ncalibration_tbl_decom\n\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model   .model_desc                     .type .calibration_data \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                           &lt;chr&gt; &lt;list&gt;            \n1         1 &lt;fit[+]&gt; ARIMA(1,0,2) WITH NON-ZERO MEAN Test  &lt;tibble [219 × 4]&gt;\n\n\n\ncalibration_tbl_decom %&gt;%\n    modeltime_forecast(\n        new_data    = testing(splits_decom),\n        actual_data = weather_decomposed\n    ) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )\n\n\n\n\n\n\ncalibration_tbl_decom %&gt;%\n    modeltime_accuracy() %&gt;%\n    table_modeltime_accuracy(\n        # .interactive = interactive\n    )\n\n\n\n\n\n\n\ncalibration_tbl_decom %&gt;%\n    modeltime_residuals() %&gt;%\n    plot_modeltime_residuals(.interactive = TRUE)\n\n\n\n\n\n\nrefit_tbl_decom &lt;- calibration_tbl_decom %&gt;%\n    modeltime_refit(data = weather_decomposed)\n\nrefit_tbl_decom %&gt;%\n    modeltime_forecast(h = \"10 days\", actual_data = weather_decomposed) %&gt;%\n    plot_modeltime_forecast(\n      .legend_max_width = 25, # For mobile screens\n      # .interactive      = interactive,\n      .plotly_slider = TRUE\n    )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#ui-design",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#ui-design",
    "title": "Take-home Exercise 4 (WIP)",
    "section": "5 UI design",
    "text": "5 UI design"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#loading-r-packages",
    "title": "Take-home Exercise 4",
    "section": "2 Loading R packages",
    "text": "2 Loading R packages\nFirst and foremost, we will start by loading the R packages required.\n\npacman::p_load(tidyverse, ggplot2, tsibble, tsibbledata,  \n                fable, fabletools, feasts, \n               plotly, DT, fable.prophet)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#loading-r-packages",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "2 Loading R packages",
    "text": "2 Loading R packages\nFirst and foremost, we will start by loading the R packages required.\n\npacman::p_load(tidyverse, dplyr, lubridate, ggplot2,  \n                fable, fabletools, feasts, tsibble, tsibbledata, kableExtra,\n               plotly, DT, forecast)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#data-preparation",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "3 Data preparation",
    "text": "3 Data preparation\nAs this take home exercise serves as analysis of one of the module of the project, the data preparation steps is only performed once to ensure that same data set (weather_imputed_11stations.rds) are used across all members. Detailed preparation steps can be found in this link.\n\n3.1 Importing the data\nWe import the processed data set using read_rds(),\n\nweather &lt;- read_rds(\"data/weather_imputed_11stations.rds\")\n\n\nhead(weather)\n\n# A tibble: 6 × 15\n  Station    Date        Year Month   Day `Daily Rainfall Total (mm)`\n  &lt;chr&gt;      &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;\n1 Ang Mo Kio 2021-01-01  2021     1     1                        94.4\n2 Ang Mo Kio 2021-01-02  2021     1     2                       114. \n3 Ang Mo Kio 2021-01-03  2021     1     3                         5.2\n4 Ang Mo Kio 2021-01-04  2021     1     4                         0  \n5 Ang Mo Kio 2021-01-05  2021     1     5                         0  \n6 Ang Mo Kio 2021-01-06  2021     1     6                         0  \n# ℹ 9 more variables: `Mean Temperature (°C)` &lt;dbl&gt;,\n#   `Maximum Temperature (°C)` &lt;dbl&gt;, `Minimum Temperature (°C)` &lt;dbl&gt;,\n#   LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, Date_mine &lt;date&gt;, Month_Name &lt;fct&gt;, Week &lt;dbl&gt;,\n#   Weekday &lt;dbl&gt;\n\n\n\n\n3.2 Convert to tsibble data frame\nThe dataset comprises daily records of rainfall and temperature gathered from various weather stations across Singapore. Initially, this data is loaded in a ‘tibble’ format. However, for our forecasting analysis, we will be using fable package, which is part of the tidyverts ecosystem and requires data in a ‘tsibble’ format. To accommodate this requirement, we will convert our tibble dataframe into a tsibble using the following code.\n\nweather_tsbl &lt;- as_tsibble(weather, key = Station, index = Date)\nweather_tsbl\n\n# A tsibble: 12,045 x 15 [1D]\n# Key:       Station [11]\n   Station    Date        Year Month   Day `Daily Rainfall Total (mm)`\n   &lt;chr&gt;      &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;\n 1 Ang Mo Kio 2021-01-01  2021     1     1                        94.4\n 2 Ang Mo Kio 2021-01-02  2021     1     2                       114. \n 3 Ang Mo Kio 2021-01-03  2021     1     3                         5.2\n 4 Ang Mo Kio 2021-01-04  2021     1     4                         0  \n 5 Ang Mo Kio 2021-01-05  2021     1     5                         0  \n 6 Ang Mo Kio 2021-01-06  2021     1     6                         0  \n 7 Ang Mo Kio 2021-01-07  2021     1     7                         1.6\n 8 Ang Mo Kio 2021-01-08  2021     1     8                        12.6\n 9 Ang Mo Kio 2021-01-09  2021     1     9                        35.4\n10 Ang Mo Kio 2021-01-10  2021     1    10                       119. \n# ℹ 12,035 more rows\n# ℹ 9 more variables: `Mean Temperature (°C)` &lt;dbl&gt;,\n#   `Maximum Temperature (°C)` &lt;dbl&gt;, `Minimum Temperature (°C)` &lt;dbl&gt;,\n#   LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, Date_mine &lt;date&gt;, Month_Name &lt;fct&gt;, Week &lt;dbl&gt;,\n#   Weekday &lt;dbl&gt;\n\n\nThe list below shows 11 unique stations.\n\nStation &lt;- weather_tsbl %&gt;% distinct(Station)\n\ndatatable(Station, \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 12,scrollX=T))\n\n\n\n\n\n\n\nWhat is tsibble?\nA tsibble consists of a time index, key, and other measured variables in a data-centric format. In tsibble,\n\nIndex is a variable with inherent ordering from past to present.\nKey is a set of variables that define observational units over time.\nEach observation should be uniquely identified by index and key.\nEach observational unit should be measured at a common interval, if regularly spaced."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#time-series-analysis",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#time-series-analysis",
    "title": "Take-home Exercise 4 (using Fable packages)",
    "section": "4 Time series analysis",
    "text": "4 Time series analysis\n\n4.1 Daily data\n\nvariable &lt;- \"Minimum Temperature (°C)\"\n\n\n# Set limit for the y axis \nmin_y &lt;- min(weather_tsbl_amk[[variable]], na.rm = TRUE)\nmax_y &lt;- max(weather_tsbl_amk[[variable]], na.rm = TRUE)\nlower_limit &lt;- min_y - 0.20 * (max_y - min_y)\nupper_limit &lt;- max_y + 0.20 * (max_y - min_y)\n\n# plot line graph\nggplot_obj &lt;- weather_tsbl_amk %&gt;%\n  ggplot(aes(x = Date, y = !!sym(variable))) +\n  geom_line(colour = \"#2c3e50\",\n            linewidth = 0.5) +\n  geom_point(size = 0.6, alpha= 0.3, colour= \"grey\") +\n  labs(title = paste(variable, \"for Ang Mo Kio\"),\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  scale_y_continuous(limits = c(lower_limit, upper_limit))\n\n# Convert ggplot object to plotly for interactivity\nggplotly(ggplot_obj) %&gt;%\n  layout(xaxis = list(rangeslider = list(type = \"date\")))\n\n\n\n\n\nInteractive line plot shows the mean temperature of ang mo kio station. When the user hover through the point, a tool tip shows the date date slider is added to the bottom to let the user zoom into the period they are interested in.\n\nweather_tsbl_amk %&gt;%\n  gg_tsdisplay(`Mean Temperature (°C)`,\n               plot_type = \"partial\")+\n  labs(title = \"Mean temperature for Ang Mo Kio\",\n       x = \"\",\n       y = \"\")\n\n\nggseason &lt;- weather_tsbl_amk %&gt;%\n   gg_season(`Mean Temperature (°C)`)+\n  labs(title = \"Mean temperature for Ang Mo Kio\",\n       x = \"\",\n       y = \"\")\n\nplotly_ggseason &lt;- ggplotly(ggseason)\nplotly_ggseason\n\nACF of daily mean temperature\n\nACF &lt;- weather_tsbl_amk %&gt;%\n  ACF(`Mean Temperature (°C)`, lag_max = 100) %&gt;%\n  autoplot() +\n  labs(title = \"ACF plot of daily mean temperature of Ang Mo Kio\") +\n  theme_minimal()\n\nggplotly(ACF)\n\n\n\n\n\nACF plot of monthly average mean temperature\n\nweather_tsbl_mth_agg %&gt;%\n  ACF(avg_meantemp, lag_max = 24) %&gt;%\n  autoplot() +\n  labs(title = \"ACF plot of monthly average mean temperature of Ang Mo Kio\")\n\nSTL decomposition\n\na &lt;- weather_tsbl_amk %&gt;%\n  model(STL(`Mean Temperature (°C)` ~ season(window = \"periodic\"))) %&gt;%\n  components() %&gt;%\n  autoplot() \n\nggplotly(a)\n\n\n\n\n\n\nb &lt;- weather_tsbl_amk %&gt;%\n  model(STL(`Mean Temperature (°C)`)) %&gt;%\n  components() %&gt;%\n  autoplot() \n\nggplotly(b)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#time-series-forecasting",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#time-series-forecasting",
    "title": "Take-home Exercise 4 (using Fable packages)",
    "section": "5 Time series forecasting",
    "text": "5 Time series forecasting\n\n5.1 Split data into training and testing\n\n# Define the split point; for example, keeping the first 80% of rows for training\nsplit_point &lt;- nrow(weather_tsbl_amk) * 0.8\n\n# Create the training dataset (first 80% of the data)\ntrain_amk &lt;- weather_tsbl_amk %&gt;% \n  slice(1:floor(split_point))\n\n# Create the test dataset (remaining 20% of the data)\ntest_amk &lt;- weather_tsbl_amk %&gt;% \n  slice((floor(split_point) + 1):n())\n\n\ntest_amk\n\n# A tsibble: 219 x 15 [1D]\n# Key:       Station [1]\n   Station    Date        Year Month   Day `Daily Rainfall Total (mm)`\n   &lt;chr&gt;      &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;\n 1 Ang Mo Kio 2023-05-27  2023     5    27                         0  \n 2 Ang Mo Kio 2023-05-28  2023     5    28                         0  \n 3 Ang Mo Kio 2023-05-29  2023     5    29                         2.6\n 4 Ang Mo Kio 2023-05-30  2023     5    30                         0  \n 5 Ang Mo Kio 2023-05-31  2023     5    31                         0  \n 6 Ang Mo Kio 2023-06-01  2023     6     1                        41.2\n 7 Ang Mo Kio 2023-06-02  2023     6     2                         0  \n 8 Ang Mo Kio 2023-06-03  2023     6     3                         6.2\n 9 Ang Mo Kio 2023-06-04  2023     6     4                        10.4\n10 Ang Mo Kio 2023-06-05  2023     6     5                         0  \n# ℹ 209 more rows\n# ℹ 9 more variables: `Mean Temperature (°C)` &lt;dbl&gt;,\n#   `Maximum Temperature (°C)` &lt;dbl&gt;, `Minimum Temperature (°C)` &lt;dbl&gt;,\n#   LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, Date_mine &lt;date&gt;, Month_Name &lt;fct&gt;, Week &lt;dbl&gt;,\n#   Weekday &lt;dbl&gt;\n\n\n\n\n5.2 Create and fit multiple model to tesing set\n\n5.2.1 Combined model\nPreviously we plot all three model seperately. Now, we combine all the model in one plot.\n\n# Correct approach for defining multiple models:\ntrain_amk_fit &lt;- train_amk %&gt;%\n  model(\n    stla = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      ARIMA(season_adjust)\n    ),\n    \n    # stlsn = decomposition_model(\n    #   STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n    #   SNAIVE(season_adjust)\n    # ),\n    \n      stln = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      NAIVE(season_adjust)\n    ),\n    \n    sn = SNAIVE(`Mean Temperature (°C)` ~ lag(\"year\")\n    ),\n    \n    holtwinters = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\")),\n  stlets = decomposition_model(\n    STL(`Mean Temperature (°C)`), \n    ETS(season_adjust ~ season(\"N\")\n        ))\n  )\n\n# Forecasting\ntrain_amk_fc &lt;- forecast(train_amk_fit, h = \"6 months\")\n\n# Plotting the forecasts\nc &lt;- autoplot(train_amk, `Mean Temperature (°C)`) +\n  autolayer(train_amk_fc, series = \"Forecast\", level = NULL) +\n  labs(title = \"Forecast for mean temperature for Ang Mo Kio\",\n       x = \"\", y = \"\") +\n  theme_minimal() \n\nggplotly(c)\n\n\n\n\n\n\nd &lt;- autoplot(test_amk, `Mean Temperature (°C)`, series = \"Test Data\") +\n  autolayer(train_amk_fc, series = \"Forecast\", level = NULL) +\n  theme_minimal()\n\nggplotly(d)\n\n\n\n\n\n\n\n\n5.3 Testing set forcast & Accuracy Evaluation\n\naccuracy_metrics &lt;-\naccuracy(train_amk_fc, weather_tsbl_amk) %&gt;%\n  arrange(.model) %&gt;%\n  select(.model, .type, RMSE, MAE, MAPE, MASE)\n\n\n# Assuming 'accuracy_metrics' is your tibble\naccuracy_metrics_formatted &lt;- accuracy_metrics %&gt;%\n  mutate(across(c(RMSE, MAE, MAPE, MASE), round, 2))\n\n# Now, convert this formatted tibble to a nicely formatted table\naccuracy_metrics_formatted %&gt;%\n  kable(\"html\", digits = 2) %&gt;%  # \"html\" for HTML output, use \"latex\" for PDF output\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n.model\n.type\nRMSE\nMAE\nMAPE\nMASE\n\n\n\n\nholtwinters\nTest\n1.75\n1.38\n4.97\n1.22\n\n\nsn\nTest\n1.68\n1.39\n4.82\n1.22\n\n\nstla\nTest\n1.67\n1.32\n4.71\n1.16\n\n\nstlets\nTest\n1.40\n1.16\n4.01\n1.02\n\n\nstln\nTest\n1.30\n1.08\n3.80\n0.95\n\n\n\n\n\n\n\n\n\n5.4 apply to the whole data set\n\n# Refit models to the full dataset\nfull_fit &lt;- weather_tsbl_amk %&gt;%\n  model(\n    stla = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      ARIMA(season_adjust)\n    ),\n    \n    # stlsn = decomposition_model(\n    #   STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n    #   SNAIVE(season_adjust)\n    # ),\n    \n      stln = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      NAIVE(season_adjust)\n    ),\n    \n    sn = SNAIVE(`Mean Temperature (°C)` ~ lag(\"year\")\n    ),\n    \n    holtwinters = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n  )\n\n\n# Forecasting forward\nfuture_horizon &lt;- \"1 months\" # Adjust this to your forecast needs\nfull_forecast &lt;- forecast(full_fit, h = future_horizon)\nfull_forecast\n\n# A fable: 120 x 5 [1D]\n# Key:     Station, .model [4]\n   Station    .model Date       `Mean Temperature (°C)` .mean\n   &lt;chr&gt;      &lt;chr&gt;  &lt;date&gt;                      &lt;dist&gt; &lt;dbl&gt;\n 1 Ang Mo Kio stla   2024-01-01                N(27, 1)  26.7\n 2 Ang Mo Kio stla   2024-01-02              N(23, 1.3)  23.4\n 3 Ang Mo Kio stla   2024-01-03              N(23, 1.5)  22.9\n 4 Ang Mo Kio stla   2024-01-04              N(23, 1.6)  23.0\n 5 Ang Mo Kio stla   2024-01-05              N(23, 1.7)  23.2\n 6 Ang Mo Kio stla   2024-01-06              N(24, 1.7)  24.3\n 7 Ang Mo Kio stla   2024-01-07              N(24, 1.8)  24.3\n 8 Ang Mo Kio stla   2024-01-08              N(24, 1.8)  24.5\n 9 Ang Mo Kio stla   2024-01-09              N(25, 1.8)  24.5\n10 Ang Mo Kio stla   2024-01-10              N(25, 1.9)  24.5\n# ℹ 110 more rows\n\n\n\n# Plotting the forecasts along with the full dataset\nautoplot(weather_tsbl_amk, `Mean Temperature (°C)`) +\n  autolayer(full_forecast, series = \"Forecast\", level = NULL) +\n  labs(title = \"Full Dataset Forecast for Mean Temperature in Ang Mo Kio\",\n       x = \"Date\", y = \"Mean Temperature (°C)\") +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "pacman::p_load(tidyverse, naniar,imputeTS, DT, lubridate,\n               ggplot2, patchwork, ggthemes,\n               tseries, ggHoriPlot, forecast)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html#load-packages",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html#load-packages",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "pacman::p_load(tidyverse, naniar,imputeTS, DT, lubridate,\n               ggplot2, patchwork, ggthemes,\n               tseries, ggHoriPlot, forecast)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html#import-cleaned-data",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html#import-cleaned-data",
    "title": "Take-home Exercise 4",
    "section": "2 Import cleaned data",
    "text": "2 Import cleaned data\n\nweather_data &lt;- read_rds(\"data/weather_imputed.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html#data-wrangling",
    "title": "Take-home Exercise 4",
    "section": "3 Data Wrangling",
    "text": "3 Data Wrangling\nData: Ang Mo Kio\nTime period: 2021-2023\nVariable: Mean Temperature\n\n# Extract the temperature data into a variable *temp_data*\ntemp_data &lt;- weather_data %&gt;%\n  select(c(Station, Date, `Mean Temperature (°C)`, LAT, LONG))\n\n# Filter for a single station, e.g., \"Ang Mo Kio\"\nang_mo_kio_data &lt;- temp_data %&gt;% \n  filter(Station == \"Ang Mo Kio\")\n\n# Change Date to chr type for time series\nang_mo_kio_data$Date &lt;- as.character (ang_mo_kio_data$Date) \nstr(ang_mo_kio_data)\n\ntibble [1,095 × 5] (S3: tbl_df/tbl/data.frame)\n $ Station              : chr [1:1095] \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ Date                 : chr [1:1095] \"2021-01-01\" \"2021-01-02\" \"2021-01-03\" \"2021-01-04\" ...\n $ Mean Temperature (°C): num [1:1095] 24 23 23.9 25.1 26.9 26.9 24.4 25.3 25.2 23.7 ...\n $ LAT                  : num [1:1095] 1.38 1.38 1.38 1.38 1.38 ...\n $ LONG                 : num [1:1095] 104 104 104 104 104 ...\n\n\nSplit data into train test split (0.8)\n\n# Sort the data by Date if it's not already sorted\nang_mo_kio_data &lt;- ang_mo_kio_data[order(ang_mo_kio_data$Date), ]\n\n# Split data into training and testing sets based on the 'Date' column\ntrain_size &lt;- floor(nrow(ang_mo_kio_data) * 0.8)  # Calculate the size of the training set\ntrain_data &lt;- ang_mo_kio_data[1:train_size, ]\ntest_data &lt;- ang_mo_kio_data[(train_size + 1):nrow(ang_mo_kio_data), ]\n\nSelecting column Date and Mean Temperature (°C)\n\ntest_clean &lt;- test_data %&gt;% \n                select(Date, `Mean Temperature (°C)`)\n\ntrain_clean &lt;- train_data %&gt;% \n                select(Date, `Mean Temperature (°C)`)\n\nCreate time series object for data train\n\nts_train &lt;- ts(data = train_clean$`Mean Temperature (°C)`,\n                start = c(2021,01),\n                frequency = 365)\nautoplot(ts_train)\n\n\n\n\nCreate time series object for data test\n\nts_test &lt;- ts(data = test_clean$`Mean Temperature (°C)`,\n                start = c(2023,05,27),\n                frequency = 365)\nautoplot(ts_test)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html#exploratory-plot",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html#exploratory-plot",
    "title": "Take-home Exercise 4",
    "section": "4 Exploratory plot",
    "text": "4 Exploratory plot\n\n4.1 Decomposing a time series means separating it into its constituent components.\n\nclimate_dc &lt;- decompose(ts_train)\nplot(climate_dc)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html#create-holt-winters-model",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html#create-holt-winters-model",
    "title": "Take-home Exercise 4",
    "section": "5 Create Holt-Winters Model",
    "text": "5 Create Holt-Winters Model\n\nclimate_hw &lt;- HoltWinters(ts_train,seasonal = \"additive\")\n\n\n# Forecast model\nclimate_forecast &lt;- forecast(climate_hw, h=114)\n\n\nclimate_forecast\n\n          Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.4000       30.82203 29.39958 32.24448 28.64659 32.99747\n2023.4027       30.32202 28.74839 31.89565 27.91536 32.72868\n2023.4055       31.42544 29.71393 33.13695 28.80791 34.04297\n2023.4082       31.52903 29.68995 33.36811 28.71640 34.34166\n2023.4110       30.93603 28.97767 32.89439 27.94098 33.93108\n2023.4137       30.14652 28.07574 32.21730 26.97953 33.31350\n2023.4164       28.95771 26.78031 31.13511 25.62766 32.28776\n2023.4192       28.65749 26.37845 30.93653 25.17200 32.14299\n2023.4219       29.35645 26.98011 31.73278 25.72215 32.99074\n2023.4247       28.75649 26.28669 31.22630 24.97925 32.53374\n2023.4274       29.45854 26.89867 32.01840 25.54357 33.37351\n2023.4301       28.36011 25.71325 31.00697 24.31209 32.40813\n2023.4329       28.96168 26.23060 31.69276 24.78485 33.13851\n2023.4356       28.26479 25.45201 31.07758 23.96301 32.56658\n2023.4384       29.26847 26.37629 32.16065 24.84526 33.69168\n2023.4411       27.96809 24.99863 30.93755 23.42670 32.50948\n2023.4438       28.17093 25.12616 31.21570 23.51436 32.82751\n2023.4466       27.87049 24.75223 30.98876 23.10152 32.63947\n2023.4493       28.87304 25.68298 32.06311 23.99425 33.75183\n2023.4521       28.68026 25.41997 31.94055 23.69408 33.66645\n2023.4548       26.88639 23.55735 30.21542 21.79507 31.97770\n2023.4575       28.18974 24.79336 31.58612 22.99543 33.38406\n2023.4603       27.89099 24.42857 31.35341 22.59567 33.18630\n2023.4630       27.89520 24.36798 31.42243 22.50078 33.28963\n2023.4658       28.19921 24.60835 31.79007 22.70746 33.69096\n2023.4685       28.00323 24.34985 31.65662 22.41586 33.59061\n2023.4712       28.80624 25.09138 32.52110 23.12486 34.48763\n2023.4740       27.91127 24.13594 31.68660 22.13740 33.68514\n2023.4767       28.41482 24.57996 32.24967 22.54992 34.27972\n2023.4795       27.61731 23.72385 31.51077 21.66278 33.57185\n2023.4822       29.61976 25.66856 33.57097 23.57692 35.66261\n2023.4849       30.22168 26.21357 34.22979 24.09180 36.35155\n2023.4877       28.82382 24.75959 32.88804 22.60812 35.03951\n2023.4904       29.62723 25.50766 33.74680 23.32689 35.92757\n2023.4932       28.22940 24.05522 32.40359 21.84554 34.61327\n2023.4959       30.02925 25.80115 34.25734 23.56294 36.49556\n2023.4986       30.08979 25.80847 34.37111 23.54207 36.63751\n2023.5014       29.31283 24.97893 33.64673 22.68470 35.94095\n2023.5041       27.47035 23.08451 31.85620 20.76278 34.17793\n2023.5068       28.50381 24.06663 32.94099 21.71772 35.28990\n2023.5096       28.43714 23.94921 32.92508 21.57344 35.30085\n2023.5123       29.07276 24.53464 33.61088 22.13231 36.01321\n2023.5151       29.98769 25.39994 34.57545 22.97133 37.00405\n2023.5178       29.57032 24.93347 34.20717 22.47886 36.66178\n2023.5205       26.60023 21.91479 31.28567 19.43446 33.76600\n2023.5233       26.77955 22.04601 31.51308 19.54023 34.01886\n2023.5260       27.20947 22.42834 31.99061 19.89735 34.52159\n2023.5288       25.51512 20.68684 30.34339 18.13091 32.89932\n2023.5315       27.32035 22.44539 32.19530 19.86475 34.77595\n2023.5342       28.92149 24.00029 33.84268 21.39517 36.44780\n2023.5370       29.85916 24.89216 34.82616 22.26278 37.45553\n2023.5397       30.21748 25.20509 35.22987 22.55169 37.88327\n2023.5425       30.32111 25.26374 35.37848 22.58653 38.05569\n2023.5452       29.50119 24.39923 34.60315 21.69842 37.30396\n2023.5479       30.30788 25.16172 35.45403 22.43751 38.17824\n2023.5507       30.60495 25.41497 35.79493 22.66756 38.54234\n2023.5534       30.74438 25.51095 35.97782 22.74054 38.74823\n2023.5562       30.09909 24.82256 35.37562 22.02933 38.16885\n2023.5589       30.06860 24.74932 35.38788 21.93346 38.20373\n2023.5616       30.65515 25.29346 36.01683 22.45515 38.85514\n2023.5644       30.29291 24.88914 35.69667 22.02856 38.55725\n2023.5671       30.75463 25.30912 36.20014 22.42644 39.08282\n2023.5699       30.74354 25.25659 36.23048 22.35198 39.13509\n2023.5726       30.75063 25.22256 36.27869 22.29618 39.20507\n2023.5753       31.03941 25.47053 36.60830 22.52254 39.55629\n2023.5781       29.41853 23.80912 35.02793 20.83968 37.99737\n2023.5808       28.49534 22.84570 34.14497 19.85496 37.13571\n2023.5836       29.22306 23.53347 34.91264 20.52159 37.92452\n2023.5863       29.09654 23.36729 34.82578 20.33441 37.85867\n2023.5890       28.00411 22.23546 33.77275 19.18173 36.82648\n2023.5918       30.08033 24.27256 35.88810 21.19811 38.96255\n2023.5945       29.69508 23.84845 35.54172 20.75342 38.63674\n2023.5973       29.05902 23.17377 34.94426 20.05831 38.05972\n2023.6000       28.13480 22.21120 34.05840 19.07543 37.19417\n2023.6027       29.38051 23.41880 35.34222 20.26286 38.49816\n2023.6055       30.23174 24.23216 36.23133 21.05618 39.40731\n2023.6082       29.63236 23.59515 35.66957 20.39924 38.86548\n2023.6110       29.30550 23.23089 35.38011 20.01518 38.59581\n2023.6137       28.44122 22.32944 34.55300 19.09406 37.78838\n2023.6164       28.20788 22.05916 34.35660 18.80422 37.61154\n2023.6192       28.54348 22.35803 34.72893 19.08366 38.00330\n2023.6219       28.24901 22.02706 34.47097 18.73336 37.76467\n2023.6247       27.53065 21.27240 33.78890 17.95949 37.10181\n2023.6274       26.69219 20.39786 32.98653 17.06584 36.31854\n2023.6301       27.48398 21.15377 33.81419 17.80275 37.16520\n2023.6329       25.92668 19.56079 32.29257 16.19089 35.66246\n2023.6356       27.77490 21.37353 34.17626 17.98485 37.56494\n2023.6384       27.37857 20.94193 33.81522 17.53457 37.22258\n2023.6411       27.99320 21.52146 34.46494 18.09553 37.89087\n2023.6438       26.08377 19.57713 32.59041 16.13273 36.03482\n2023.6466       26.68237 20.14102 33.22373 16.67824 36.68651\n2023.6493       27.87721 21.30133 34.45310 17.82026 37.93416\n2023.6521       28.26420 21.65397 34.87444 18.15472 38.37369\n2023.6548       29.48655 22.84214 36.13096 19.32480 39.64829\n2023.6575       27.29827 20.61986 33.97668 17.08453 37.51201\n2023.6603       26.33735 19.62511 33.04958 16.07187 36.60282\n2023.6630       27.87345 21.12756 34.61934 17.55650 38.19040\n2023.6658       28.38988 21.61050 35.16926 18.02171 38.75804\n2023.6685       26.11257 19.29987 32.92528 15.69344 36.53170\n2023.6712       28.74694 21.90107 35.59280 18.27709 39.21679\n2023.6740       28.63468 21.75581 35.51355 18.11436 39.15500\n2023.6767       29.08381 22.17210 35.99553 18.51326 39.65437\n2023.6795       28.58092 21.63652 35.52533 17.96037 39.20147\n2023.6822       28.83782 21.86088 35.81476 18.16751 39.50813\n2023.6849       28.85124 21.84191 35.86057 18.13140 39.57108\n2023.6877       29.92604 22.88448 36.96760 19.15690 40.69518\n2023.6904       28.24324 21.16959 35.31690 17.42502 39.06146\n2023.6932       29.50928 22.40368 36.61488 18.64220 40.37635\n2023.6959       29.80376 22.66636 36.94116 18.88804 40.71947\n2023.6986       27.56661 20.39755 34.73567 16.60248 38.53074\n2023.7014       28.82157 21.62098 36.02215 17.80923 39.83391\n2023.7041       29.58664 22.35467 36.81860 18.52630 40.64698\n2023.7068       29.36477 22.10156 36.62799 18.25664 40.47290\n2023.7096       29.95328 22.65895 37.24761 18.79756 41.10900\n\n\n\n# Check accuracy\nforecast::accuracy(climate_forecast, ts_test)\n\n                      ME     RMSE       MAE         MPE     MAPE      MASE\nTraining set  0.00279363 1.108857 0.7116148 -0.05696726 2.597585 0.5478908\nTest set     -1.57212418 2.195795 1.8045386 -5.82483639 6.639015 1.3893615\n                   ACF1 Theil's U\nTraining set 0.03609098        NA\nTest set     0.59056765  2.408303\n\n\n\n5.0.1 Visualization of Holt-Winters Model\n\n# Visualization\nts_train %&gt;% \n  autoplot(series = \"Actual\") +\n  autolayer(climate_forecast$fitted, series = \"Train\") +\n  autolayer(climate_forecast$mean, series = \"Test\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html#create-arima",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html#create-arima",
    "title": "Take-home Exercise 4",
    "section": "6 Create ARIMA",
    "text": "6 Create ARIMA\n\n# Check stationarity. Must be stationary to proceed. (p-val &lt;0.05)\nadf.test(ts_train)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_train\nDickey-Fuller = -5.4569, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n# Create ARIMA on train\nclimate_auto &lt;- auto.arima(ts_train, seasonal = T)\nclimate_auto\n\nSeries: ts_train \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      0.7994  -0.0464  -1.3154  0.3494\ns.e.  0.2239   0.1193   0.2201  0.2042\n\nsigma^2 = 0.8802:  log likelihood = -1184.2\nAIC=2378.4   AICc=2378.47   BIC=2402.27\n\n\n\n# Forecast\nclimate_auto_f &lt;- forecast(climate_auto, h=114)\nclimate_auto_f\n\n          Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.4000       29.14213 27.93982 30.34445 27.30335 30.98092\n2023.4027       29.15909 27.82335 30.49483 27.11625 31.20193\n2023.4055       29.15212 27.74254 30.56170 26.99636 31.30789\n2023.4082       29.14576 27.68747 30.60405 26.91550 31.37602\n2023.4110       29.14100 27.64829 30.63371 26.85809 31.42390\n2023.4137       29.13749 27.61906 30.65592 26.81525 31.45973\n2023.4164       29.13490 27.59627 30.67353 26.78177 31.48804\n2023.4192       29.13300 27.57777 30.68822 26.75448 31.51151\n2023.4219       29.13160 27.56219 30.70100 26.73140 31.53179\n2023.4247       29.13056 27.54865 30.71247 26.71124 31.54989\n2023.4274       29.12980 27.53654 30.72306 26.69312 31.56648\n2023.4301       29.12924 27.52547 30.73302 26.67648 31.58200\n2023.4329       29.12883 27.51514 30.74252 26.66091 31.59675\n2023.4356       29.12853 27.50536 30.75169 26.64611 31.61094\n2023.4384       29.12830 27.49600 30.76061 26.63191 31.62470\n2023.4411       29.12814 27.48694 30.76934 26.61814 31.63814\n2023.4438       29.12802 27.47811 30.77792 26.60470 31.65133\n2023.4466       29.12793 27.46947 30.78639 26.59153 31.66432\n2023.4493       29.12786 27.46097 30.79475 26.57857 31.67715\n2023.4521       29.12781 27.45259 30.80304 26.56577 31.68985\n2023.4548       29.12778 27.44430 30.81126 26.55312 31.70244\n2023.4575       29.12775 27.43609 30.81942 26.54057 31.71493\n2023.4603       29.12773 27.42795 30.82752 26.52813 31.72733\n2023.4630       29.12772 27.41986 30.83557 26.51578 31.73966\n2023.4658       29.12771 27.41184 30.84358 26.50351 31.75191\n2023.4685       29.12770 27.40386 30.85154 26.49131 31.76409\n2023.4712       29.12769 27.39592 30.85947 26.47918 31.77621\n2023.4740       29.12769 27.38803 30.86735 26.46711 31.78827\n2023.4767       29.12769 27.38018 30.87520 26.45510 31.80028\n2023.4795       29.12768 27.37236 30.88301 26.44315 31.81222\n2023.4822       29.12768 27.36458 30.89078 26.43126 31.82411\n2023.4849       29.12768 27.35684 30.89852 26.41942 31.83595\n2023.4877       29.12768 27.34914 30.90623 26.40763 31.84773\n2023.4904       29.12768 27.34146 30.91390 26.39590 31.85946\n2023.4932       29.12768 27.33383 30.92153 26.38422 31.87114\n2023.4959       29.12768 27.32622 30.92914 26.37258 31.88277\n2023.4986       29.12768 27.31865 30.93671 26.36100 31.89436\n2023.5014       29.12768 27.31110 30.94425 26.34947 31.90589\n2023.5041       29.12768 27.30359 30.95176 26.33798 31.91737\n2023.5068       29.12768 27.29612 30.95924 26.32654 31.92881\n2023.5096       29.12768 27.28867 30.96669 26.31515 31.94020\n2023.5123       29.12768 27.28125 30.97411 26.30381 31.95155\n2023.5151       29.12768 27.27386 30.98150 26.29251 31.96285\n2023.5178       29.12768 27.26650 30.98886 26.28125 31.97411\n2023.5205       29.12768 27.25917 30.99619 26.27004 31.98532\n2023.5233       29.12768 27.25187 31.00349 26.25887 31.99648\n2023.5260       29.12768 27.24459 31.01076 26.24775 32.00761\n2023.5288       29.12768 27.23735 31.01801 26.23667 32.01869\n2023.5315       29.12768 27.23013 31.02523 26.22563 32.02973\n2023.5342       29.12768 27.22294 31.03242 26.21463 32.04072\n2023.5370       29.12768 27.21578 31.03958 26.20368 32.05168\n2023.5397       29.12768 27.20864 31.04672 26.19276 32.06259\n2023.5425       29.12768 27.20153 31.05383 26.18189 32.07347\n2023.5452       29.12768 27.19445 31.06091 26.17105 32.08430\n2023.5479       29.12768 27.18739 31.06797 26.16026 32.09510\n2023.5507       29.12768 27.18036 31.07500 26.14950 32.10585\n2023.5534       29.12768 27.17335 31.08201 26.13879 32.11657\n2023.5562       29.12768 27.16637 31.08899 26.12811 32.12725\n2023.5589       29.12768 27.15941 31.09595 26.11747 32.13789\n2023.5616       29.12768 27.15248 31.10288 26.10687 32.14849\n2023.5644       29.12768 27.14557 31.10979 26.09630 32.15906\n2023.5671       29.12768 27.13868 31.11667 26.08577 32.16959\n2023.5699       29.12768 27.13182 31.12354 26.07528 32.18008\n2023.5726       29.12768 27.12498 31.13037 26.06482 32.19054\n2023.5753       29.12768 27.11817 31.13719 26.05440 32.20096\n2023.5781       29.12768 27.11138 31.14398 26.04401 32.21134\n2023.5808       29.12768 27.10461 31.15075 26.03366 32.22169\n2023.5836       29.12768 27.09786 31.15749 26.02334 32.23201\n2023.5863       29.12768 27.09114 31.16422 26.01306 32.24229\n2023.5890       29.12768 27.08444 31.17092 26.00281 32.25254\n2023.5918       29.12768 27.07776 31.17760 25.99260 32.26276\n2023.5945       29.12768 27.07110 31.18425 25.98242 32.27294\n2023.5973       29.12768 27.06447 31.19089 25.97227 32.28309\n2023.6000       29.12768 27.05785 31.19751 25.96215 32.29321\n2023.6027       29.12768 27.05126 31.20410 25.95207 32.30329\n2023.6055       29.12768 27.04468 31.21067 25.94201 32.31334\n2023.6082       29.12768 27.03813 31.21723 25.93199 32.32336\n2023.6110       29.12768 27.03160 31.22376 25.92200 32.33335\n2023.6137       29.12768 27.02509 31.23027 25.91204 32.34331\n2023.6164       29.12768 27.01860 31.23676 25.90212 32.35324\n2023.6192       29.12768 27.01212 31.24323 25.89222 32.36314\n2023.6219       29.12768 27.00567 31.24968 25.88235 32.37301\n2023.6247       29.12768 26.99924 31.25612 25.87251 32.38284\n2023.6274       29.12768 26.99283 31.26253 25.86271 32.39265\n2023.6301       29.12768 26.98643 31.26892 25.85293 32.40243\n2023.6329       29.12768 26.98006 31.27530 25.84318 32.41218\n2023.6356       29.12768 26.97370 31.28165 25.83346 32.42190\n2023.6384       29.12768 26.96737 31.28799 25.82377 32.43159\n2023.6411       29.12768 26.96105 31.29431 25.81410 32.44125\n2023.6438       29.12768 26.95475 31.30061 25.80447 32.45089\n2023.6466       29.12768 26.94847 31.30689 25.79486 32.46050\n2023.6493       29.12768 26.94220 31.31315 25.78528 32.47008\n2023.6521       29.12768 26.93596 31.31940 25.77573 32.47963\n2023.6548       29.12768 26.92973 31.32563 25.76620 32.48915\n2023.6575       29.12768 26.92352 31.33184 25.75670 32.49865\n2023.6603       29.12768 26.91732 31.33803 25.74723 32.50812\n2023.6630       29.12768 26.91115 31.34421 25.73779 32.51757\n2023.6658       29.12768 26.90499 31.35037 25.72837 32.52699\n2023.6685       29.12768 26.89885 31.35651 25.71898 32.53638\n2023.6712       29.12768 26.89272 31.36263 25.70961 32.54575\n2023.6740       29.12768 26.88661 31.36874 25.70027 32.55509\n2023.6767       29.12768 26.88052 31.37483 25.69095 32.56441\n2023.6795       29.12768 26.87445 31.38091 25.68166 32.57370\n2023.6822       29.12768 26.86839 31.38697 25.67239 32.58296\n2023.6849       29.12768 26.86235 31.39301 25.66315 32.59220\n2023.6877       29.12768 26.85632 31.39904 25.65394 32.60142\n2023.6904       29.12768 26.85031 31.40505 25.64474 32.61061\n2023.6932       29.12768 26.84432 31.41104 25.63558 32.61978\n2023.6959       29.12768 26.83834 31.41702 25.62643 32.62892\n2023.6986       29.12768 26.83237 31.42298 25.61731 32.63804\n2023.7014       29.12768 26.82643 31.42893 25.60822 32.64714\n2023.7041       29.12768 26.82049 31.43486 25.59914 32.65621\n2023.7068       29.12768 26.81458 31.44078 25.59009 32.66526\n2023.7096       29.12768 26.80867 31.44668 25.58107 32.67429\n\n\n\n# Check accuracy\naccuracy(climate_auto_f, ts_test)\n\n                      ME      RMSE       MAE         MPE     MAPE      MASE\nTraining set  0.03777592 0.9354911 0.7423579  0.03391425 2.688186 0.5715608\nTest set     -1.55415777 1.8291975 1.6636117 -5.76272036 6.124617 1.2808582\n                     ACF1 Theil's U\nTraining set -0.002187051        NA\nTest set      0.454551561   2.00749\n\n\n\n6.1 Visualization of ARIMA Model\n\n# Visualization\nts_train %&gt;% \n  autoplot(series = \"Actual\") +\n  autolayer(climate_auto_f$fitted, series = \"Train\") +\n  autolayer(climate_auto_f$mean, series = \"Test\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/timeseries.html#create-stlm-model",
    "href": "Take-home_Ex/Take-home_Ex4/timeseries.html#create-stlm-model",
    "title": "Take-home Exercise 4",
    "section": "7 Create STLM Model",
    "text": "7 Create STLM Model\n\n# Create STLM\nclimate_stlm &lt;- stlm(y= ts_train,\n                     s.window = 356,\n                     method = \"arima\")\n\nsummary(climate_stlm$model)\n\nSeries: x \nARIMA(3,1,1) \n\nCoefficients:\n         ar1     ar2     ar3      ma1\n      0.4536  0.1477  0.0489  -0.9890\ns.e.  0.0344  0.0370  0.0343   0.0068\n\nsigma^2 = 0.5041:  log likelihood = -940.99\nAIC=1891.98   AICc=1892.05   BIC=1915.85\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.02123722 0.7079672 0.5637753 0.01390915 2.032402 0.4340655\n                     ACF1\nTraining set -0.004173344\n\n\n\n# Forecast\nclimate_stlm_forecast &lt;- forecast(climate_stlm, h = 114)\nclimate_stlm_forecast\n\n          Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.4000       29.39782 28.48793 30.30772 28.00626 30.78939\n2023.4027       29.29445 28.29116 30.29774 27.76005 30.82885\n2023.4055       29.58440 28.52630 30.64251 27.96617 31.20263\n2023.4082       30.09815 29.00628 31.19002 28.42827 31.76802\n2023.4110       29.80276 28.69226 30.91327 28.10440 31.50113\n2023.4137       29.35643 28.23473 30.47814 27.64093 31.07193\n2023.4164       28.45948 27.33085 29.58811 26.73339 30.18557\n2023.4192       28.71166 27.57862 29.84471 26.97882 30.44450\n2023.4219       29.31331 28.17734 30.44928 27.57600 31.05063\n2023.4247       28.66457 27.52657 29.80256 26.92416 30.40498\n2023.4274       29.31552 28.17606 30.45497 27.57287 31.05817\n2023.4301       28.36625 27.22569 29.50681 26.62191 30.11058\n2023.4329       28.66681 27.52538 29.80824 26.92114 30.41248\n2023.4356       28.46725 27.32510 29.60940 26.72049 30.21402\n2023.4384       28.51761 27.37484 29.66037 26.76990 30.26531\n2023.4411       27.86789 26.72459 29.01119 26.11936 29.61642\n2023.4438       28.41813 27.27433 29.56192 26.66885 30.16740\n2023.4466       27.81832 26.67408 28.96257 26.06835 29.56830\n2023.4493       28.41849 27.27382 29.56317 26.66787 30.16912\n2023.4521       27.66865 26.52356 28.81373 25.91740 29.41989\n2023.4548       27.11878 25.97330 28.26425 25.36693 28.87063\n2023.4575       28.16890 27.02304 29.31476 26.41646 29.92134\n2023.4603       26.81902 25.67278 27.96526 25.06600 28.57204\n2023.4630       27.56918 26.42257 28.71579 25.81560 29.32277\n2023.4658       27.31934 26.17237 28.46632 25.56519 29.07349\n2023.4685       28.01950 26.87216 29.16684 26.26479 29.77421\n2023.4712       28.16965 27.02195 29.31736 26.41439 29.92491\n2023.4740       28.11980 26.97174 29.26787 26.36399 29.87562\n2023.4767       28.31995 27.17153 29.46838 26.56359 30.07632\n2023.4795       27.22010 26.07132 28.36889 25.46319 28.97702\n2023.4822       28.22025 27.07111 29.36939 26.46279 29.97771\n2023.4849       28.37040 27.22090 29.51990 26.61239 30.12841\n2023.4877       28.42055 27.27069 29.57040 26.66200 30.17910\n2023.4904       28.12069 26.97048 29.27091 26.36160 29.87979\n2023.4932       27.77084 26.62027 28.92141 26.01120 29.53048\n2023.4959       29.37099 28.22006 30.52191 27.61080 31.13117\n2023.4986       29.27114 28.11986 30.42241 27.51040 31.03187\n2023.5014       29.07128 27.91965 30.22292 27.31001 30.83256\n2023.5041       27.92143 26.76944 29.07342 26.15961 29.68325\n2023.5068       27.72157 26.56923 28.87392 25.95921 29.48394\n2023.5096       28.12172 26.96902 29.27442 26.35882 29.88462\n2023.5123       29.07187 27.91881 30.22492 27.30842 30.83531\n2023.5151       28.97201 27.81860 30.12542 27.20802 30.73600\n2023.5178       29.37216 28.21840 30.52593 27.60763 31.13669\n2023.5205       26.87231 25.71819 28.02643 25.10723 28.63738\n2023.5233       27.32245 26.16798 28.47693 25.55684 29.08807\n2023.5260       27.67260 26.51777 28.82743 25.90644 29.43876\n2023.5288       26.82275 25.66756 27.97793 25.05605 28.58945\n2023.5315       27.87289 26.71736 29.02843 26.10565 29.64013\n2023.5342       28.67304 27.51715 29.82893 26.90526 30.44082\n2023.5370       29.37304 28.21680 30.52929 27.60472 31.14137\n2023.5397       29.77305 28.61645 30.92965 28.00419 31.54192\n2023.5425       29.77306 28.61611 30.93001 28.00365 31.54246\n2023.5452       29.37306 28.21576 30.53037 27.60312 31.14301\n2023.5479       28.42307 27.26541 29.58073 26.65258 30.19356\n2023.5507       29.17308 28.01506 30.33109 27.40205 30.94410\n2023.5534       28.77308 27.61472 29.93145 27.00152 30.54465\n2023.5562       28.82309 27.66437 29.98181 27.05098 30.59520\n2023.5589       28.17310 27.01402 29.33217 26.40045 29.94574\n2023.5616       28.47310 27.31368 29.63253 26.69992 30.24629\n2023.5644       28.12311 26.96333 29.28289 26.34938 29.89684\n2023.5671       29.57312 28.41299 30.73325 27.79885 31.34738\n2023.5699       29.32312 28.16264 30.48361 27.54832 31.09793\n2023.5726       28.82313 27.66229 29.98396 27.04779 30.59847\n2023.5753       29.52314 28.36195 30.68432 27.74725 31.29902\n2023.5781       28.57314 27.41160 29.73468 26.79672 30.34956\n2023.5808       27.92315 26.76126 29.08504 26.14619 29.70011\n2023.5836       27.87316 26.71091 29.03540 26.09566 29.65065\n2023.5863       27.32316 26.16057 28.48576 25.54513 29.10120\n2023.5890       26.62317 25.46022 27.78612 24.84459 28.40174\n2023.5918       28.62317 27.45988 29.78647 26.84406 30.40229\n2023.5945       29.02318 27.85953 30.18683 27.24353 30.80283\n2023.5973       28.57319 27.40919 29.73719 26.79300 30.35338\n2023.6000       28.42319 27.25884 29.58755 26.64247 30.20392\n2023.6027       29.17320 28.00850 30.33791 27.39194 30.95446\n2023.6055       29.67321 28.50815 30.83826 27.89141 31.45501\n2023.6082       29.52321 28.35781 30.68862 27.74088 31.30555\n2023.6110       29.12322 27.95746 30.28898 27.34035 30.90609\n2023.6137       27.82323 26.65712 28.98934 26.03982 29.60664\n2023.6164       27.82346 26.65700 28.98992 26.03952 29.60741\n2023.6192       28.42370 27.25689 29.59051 26.63922 30.20818\n2023.6219       27.67394 26.50677 28.84110 25.88892 29.45895\n2023.6247       28.02417 26.85666 29.19168 26.23862 29.80973\n2023.6274       27.42441 26.25655 28.59227 25.63832 29.21050\n2023.6301       27.87464 26.70643 29.04286 26.08802 29.66127\n2023.6329       26.62488 25.45632 27.79344 24.83772 28.41204\n2023.6356       26.97512 25.80620 28.14403 25.18742 28.76281\n2023.6384       27.87535 26.70609 29.04461 26.08712 29.66358\n2023.6411       26.97559 25.80598 28.14520 25.18682 28.76435\n2023.6438       26.37582 25.20586 27.54579 24.58652 28.16513\n2023.6466       26.72606 25.55575 27.89637 24.93622 28.51590\n2023.6493       26.57630 25.40563 27.74696 24.78592 28.36667\n2023.6521       26.82653 25.65552 27.99754 25.03563 28.61744\n2023.6548       27.77677 26.60541 28.94813 25.98533 29.56821\n2023.6575       26.62700 25.45530 27.79871 24.83503 28.41898\n2023.6603       26.32724 25.15518 27.49930 24.53473 28.11975\n2023.6630       27.97748 26.80507 29.14988 26.18443 29.77052\n2023.6658       28.22771 27.05496 29.40047 26.43414 30.02129\n2023.6685       26.07795 24.90484 27.25105 24.28384 27.87206\n2023.6712       28.32818 27.15473 29.50164 26.53354 30.12283\n2023.6740       27.92842 26.75462 29.10222 26.13325 29.72359\n2023.6767       27.77866 26.60451 28.95281 25.98295 29.57436\n2023.6795       28.17889 27.00439 29.35339 26.38265 29.97513\n2023.6822       27.77913 26.60428 28.95397 25.98235 29.57590\n2023.6849       28.32936 27.15417 29.50456 26.53206 30.12667\n2023.6877       28.07960 26.90406 29.25514 26.28176 29.87744\n2023.6904       27.87912 26.70323 29.05502 26.08075 29.67749\n2023.6932       28.12865 26.95241 29.30489 26.32975 29.92755\n2023.6959       28.77817 27.60159 29.95476 26.97874 30.57761\n2023.6986       27.02770 25.85076 28.20463 25.22773 28.82766\n2023.7014       27.57722 26.39994 28.75450 25.77672 29.37772\n2023.7041       28.52675 27.34912 29.70437 26.72572 30.32777\n2023.7068       28.82627 27.64829 30.00425 27.02471 30.62783\n2023.7096       28.97579 27.79747 30.15412 27.17370 30.77788\n\n\n\n# Check Accuracy\n\naccuracy(climate_stlm_forecast, ts_test)\n\n                      ME      RMSE       MAE         MPE     MAPE      MASE\nTraining set  0.02123722 0.7079672 0.5637753  0.01539662 2.040041 0.4340654\nTest set     -0.92999525 1.5477016 1.2977256 -3.49412016 4.766063 0.9991529\n                     ACF1 Theil's U\nTraining set -0.004173344        NA\nTest set      0.481711859  1.695064\n\n\n\n7.1 Visualization of STLM using ARIMA method\n\nts_train %&gt;% \n  autoplot(series = \"Actual\") +\n  autolayer(climate_stlm_forecast$fitted, series = \"Train\") +\n  autolayer(climate_stlm_forecast$mean, series = \"Test\") +\n  autolayer(ts_test, series = \"Actual\")\n\n\n\n\n\nts_test_trimmed &lt;- window(ts_test, start=start(ts_test), end=start(ts_test) + length(climate_stlm_forecast$mean) - 1)\n\n# Plotting\nts_train %&gt;%\n  autoplot(series = \"Actual\") +\n  autolayer(climate_stlm_forecast$fitted, series = \"Train\") +\n  autolayer(climate_stlm_forecast$mean, series = \"Test\") +\n  autolayer(ts_test_trimmed, series = \"Actual\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#create-and-fit-multiple-model-to-tesing-set",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#create-and-fit-multiple-model-to-tesing-set",
    "title": "Take-home Exercise 4 (using Fable packages)",
    "section": "6 Create and fit multiple model to tesing set",
    "text": "6 Create and fit multiple model to tesing set\n\nstlm_arimastlm_snaiveholt_fit\n\n\n\nstlm_arima &lt;- train_amk  %&gt;%\n  model(stlf = decomposition_model(\n    STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n    ARIMA(season_adjust)\n  ))\n\nstlm_forecast &lt;-\nstlm_arima %&gt;%\n  forecast()\n  \nstlm_forecast %&gt;%\n  autoplot(train_amk)\n\n\n\n\n\n# Visualization\ntrain_amk %&gt;% \n  autoplot(`Mean Temperature (°C)`, series = \"Actual\") +\n  autolayer(stlm_forecast, level = NULL)\n\n\n\n\n\nstlm_forecast\n\n# A fable: 14 x 5 [1D]\n# Key:     Station, .model [1]\n   Station    .model Date       `Mean Temperature (°C)` .mean\n   &lt;chr&gt;      &lt;chr&gt;  &lt;date&gt;                      &lt;dist&gt; &lt;dbl&gt;\n 1 Ang Mo Kio stlf   2023-05-27              N(29, 1.1)  29.1\n 2 Ang Mo Kio stlf   2023-05-28              N(29, 1.4)  29.1\n 3 Ang Mo Kio stlf   2023-05-29              N(29, 1.6)  29.3\n 4 Ang Mo Kio stlf   2023-05-30              N(29, 1.7)  28.8\n 5 Ang Mo Kio stlf   2023-05-31              N(29, 1.8)  28.7\n 6 Ang Mo Kio stlf   2023-06-01              N(29, 1.9)  28.8\n 7 Ang Mo Kio stlf   2023-06-02              N(29, 1.9)  29.4\n 8 Ang Mo Kio stlf   2023-06-03                N(29, 2)  29.3\n 9 Ang Mo Kio stlf   2023-06-04                N(29, 2)  29.2\n10 Ang Mo Kio stlf   2023-06-05              N(29, 2.1)  29.4\n11 Ang Mo Kio stlf   2023-06-06              N(29, 2.1)  29.4\n12 Ang Mo Kio stlf   2023-06-07              N(29, 2.1)  29.3\n13 Ang Mo Kio stlf   2023-06-08              N(29, 2.1)  29.3\n14 Ang Mo Kio stlf   2023-06-09              N(29, 2.2)  29.4\n\n\n\n# Assuming stlm_arima and train_amk are already defined and stlm_arima has been forecasted\nforecast_result &lt;- stlm_arima %&gt;%\n  forecast() \n\n# Extracting forecasted values\nforecasted_values &lt;- forecast_result %&gt;%\n  as_tibble() %&gt;%\n  select(Date, .mean) # .index is the time index, .mean is the forecasted value\n\n# Combine forecasted values with actual data for plotting\nactual_data &lt;- train_amk %&gt;%\n  as_tibble() %&gt;%\n  select(Time = Date, `Mean Temperature (°C)`)\n\n# Plotting with ggplot2\nggplot() +\n  geom_line(data = actual_data, aes(x = Time, y = `Mean Temperature (°C)`), color = \"black\") +\n  geom_line(data = forecasted_values, aes(x = Date, y = .mean), color = \"red\") +\n  labs(x = \"Time\", y = \"Temperature\", title = \"Temperature Forecast without Confidence Intervals\") +\n  theme_minimal()\n\n\n\n\n\naug &lt;- stlm_arima %&gt;%\n   augment()  \n\nautoplot(aug, .innov)\n\n\n\n\n\nstlm_arima %&gt;% gg_tsresiduals()\n\n\n\n\n\n\n\nstlm_snaive &lt;- train_amk  %&gt;%\n  model(stlf = decomposition_model(\n    STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n    SNAIVE(season_adjust)\n  ))\n\nstlm_snaive %&gt;%\n  forecast() %&gt;%\n  autoplot(train_amk)\n\n\n\n\n\n\n\nholt_fit &lt;- train_amk %&gt;%\n  model(HOLTWINTERS = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\"))) \n\nholt_fit %&gt;%\n  forecast() %&gt;%\n  autoplot(train_amk)\n\n\n\n\n\n\n\n\n6.0.1 Combined model\nPreviously we plot all three model seperately. Now, we combine all the model in one plot.\n\n# Correct approach for defining multiple models:\ntrain_amk_fit &lt;- train_amk %&gt;%\n  model(\n    stla = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      ARIMA(season_adjust)\n    ),\n    \n    # stlsn = decomposition_model(\n    #   STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n    #   SNAIVE(season_adjust)\n    # ),\n    \n      stln = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      NAIVE(season_adjust)\n    ),\n    \n    sn = SNAIVE(`Mean Temperature (°C)` ~ lag(\"year\")\n    ),\n    \n    holtwinters = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n  )\n\n# Forecasting\ntrain_amk_fc &lt;- forecast(train_amk_fit, h = \"6 months\")\n\n# Plotting the forecasts\nautoplot(weather_tsbl_amk, `Mean Temperature (°C)`) +\n  autolayer(train_amk_fc, series = \"Forecast\", level = NULL) +\n  labs(title = \"Forecast for mean temperature for Ang Mo Kio\",\n       x = \"\", y = \"\") +\n  theme_minimal() \n\n\n\n\n\n\n6.1 Testing set forcast & Accuracy Evaluation\n\naccuracy_metrics &lt;-\naccuracy(train_amk_fc, weather_tsbl_amk) %&gt;%\n  arrange(.model) %&gt;%\n  select(.model, .type, RMSE, MAE, MAPE, MASE)\n\n\n# Assuming 'accuracy_metrics' is your tibble\naccuracy_metrics_formatted &lt;- accuracy_metrics %&gt;%\n  mutate(across(c(RMSE, MAE, MAPE, MASE), round, 2))\n\n# Now, convert this formatted tibble to a nicely formatted table\naccuracy_metrics_formatted %&gt;%\n  kable(\"html\", digits = 2) %&gt;%  # \"html\" for HTML output, use \"latex\" for PDF output\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n.model\n.type\nRMSE\nMAE\nMAPE\nMASE\n\n\n\n\nholtwinters\nTest\n2.06\n1.68\n6.01\n1.46\n\n\nsn\nTest\n1.69\n1.40\n4.85\n1.22\n\n\nstla\nTest\n1.67\n1.33\n4.74\n1.16\n\n\nstln\nTest\n1.31\n1.10\n3.87\n0.96\n\n\n\n\n\n\n\n\n\n6.2 apply to the whole data set\n\n# Refit models to the full dataset\nfull_fit &lt;- weather_tsbl_amk %&gt;%\n  model(\n    stla = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      ARIMA(season_adjust)\n    ),\n    stlsn = decomposition_model(\n      STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n      SNAIVE(season_adjust)\n    ),\n    holtwinters = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n  )\n\n\n# Forecasting forward\nfuture_horizon &lt;- \"1 months\" # Adjust this to your forecast needs\nfull_forecast &lt;- forecast(full_fit, h = future_horizon)\nfull_forecast\n\n# A fable: 90 x 5 [1D]\n# Key:     Station, .model [3]\n   Station    .model Date       `Mean Temperature (°C)` .mean\n   &lt;chr&gt;      &lt;chr&gt;  &lt;date&gt;                      &lt;dist&gt; &lt;dbl&gt;\n 1 Ang Mo Kio stla   2024-01-01                N(27, 1)  26.8\n 2 Ang Mo Kio stla   2024-01-02              N(23, 1.3)  23.4\n 3 Ang Mo Kio stla   2024-01-03              N(23, 1.5)  22.9\n 4 Ang Mo Kio stla   2024-01-04              N(23, 1.6)  23.0\n 5 Ang Mo Kio stla   2024-01-05              N(23, 1.7)  23.2\n 6 Ang Mo Kio stla   2024-01-06              N(24, 1.8)  24.3\n 7 Ang Mo Kio stla   2024-01-07              N(24, 1.8)  24.3\n 8 Ang Mo Kio stla   2024-01-08              N(24, 1.8)  24.5\n 9 Ang Mo Kio stla   2024-01-09              N(25, 1.9)  24.6\n10 Ang Mo Kio stla   2024-01-10              N(25, 1.9)  24.5\n# ℹ 80 more rows\n\n\n\n# Plotting the forecasts along with the full dataset\nautoplot(weather_tsbl_amk, `Mean Temperature (°C)`) +\n  autolayer(full_forecast, series = \"Forecast\") +\n  labs(title = \"Full Dataset Forecast for Mean Temperature in Ang Mo Kio\",\n       x = \"Date\", y = \"Mean Temperature (°C)\") +\n  theme_minimal()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false    \n\n\n\n Back to top"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "pacman::p_load(sf, terra, gstat, viridis, tidyverse, tmap)\n\n\nrfstations &lt;- read_csv(\"data/aspatial/RainfallStation.csv\")\n\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1,5)) %&gt;%\n  group_by(Station)%&gt;%\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\nrfdata &lt;- rfdata %&gt;%\n  left_join(rfstations)\n\n\nrfdata_sf &lt;- st_as_sf(rfdata, \n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs =4326) %&gt;%\n  st_transform(crs=4314)\n\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs=4314)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\ljjiaa\\ISSS608\\In-class_Ex\\In-class_Ex7\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\ntmap_options(check.and.fix=TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019)+\n  tm_borders()+\ntm_shape(rfdata_sf)+\n  tm_dots(col='MONTHSUM')\n\n\n\n\n\ntmap_mode(\"plot\")                                                                      \n\n\ngrid &lt;- terra::rast(mpsz2019,\n                    nrows = 690,\n                    ncols =1075)\n\nxy &lt;- terra::xyFromCell(grid, 1:ncell(grid))\n\n\n\n\n Back to top"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex7/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false    \n\n\n\n Back to top"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the module of our proposed Shiny application and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nThe module that we are focusing on is the univariate forecasting of the data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#overview",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#overview",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the module of our proposed Shiny application and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nThe module that we are focusing on is the univariate forecasting of the data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#using-weekly-rainfall",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#using-weekly-rainfall",
    "title": "Take-home Exercise 4 (using Fable packages)",
    "section": "7 Using weekly rainfall",
    "text": "7 Using weekly rainfall\n\nggplot_obj_rainfall &lt;- weather_tsbl_week_rainfall %&gt;%\n  ggplot(aes(x = year_week, y = total_rainfall)) +\n  # geom_point() +\n  geom_line(colour = \"#2c3e50\",\n            linewidth = 0.5) +\n  labs(title = \"Total monthly rainfall for Ang Mo Kio\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal()\n\n# Convert ggplot object to plotly for interactivity\nggplot_obj_rainfall &lt;- ggplotly(ggplot_obj_rainfall) %&gt;%\n  layout(xaxis = list(rangeslider = list(type = \"date\")))\nggplot_obj_rainfall\n\n\n\n\n\n\nweather_tsbl_week_rainfall %&gt;%\n  ACF(`total_rainfall`, lag_max = 52) %&gt;%\n  autoplot() +\n  labs(title = \"ACF plot of total monthly rainfall of Ang Mo Kio\")\n\n\n\n\n\nweather_tsbl_week_rainfall %&gt;%\n  PACF(`total_rainfall`, lag_max = 52) %&gt;%\n  autoplot() +\n  labs(title = \"PACF plot of total monthly rainfall of Ang Mo Kio\")\n\n\n\n\n\n# Define the split point; for example, keeping the first 80% of rows for training\nsplit_point_rainfall &lt;- nrow(weather_tsbl_week_rainfall) * 0.8\n\n# Create the training dataset (first 80% of the data)\ntrain_amk_rainfall &lt;- weather_tsbl_week_rainfall %&gt;% \n  slice(1:floor(split_point_rainfall))\n\n# Create the test dataset (remaining 20% of the data)\ntest_amk_rainfall &lt;- weather_tsbl_week_rainfall %&gt;% \n  slice((floor(split_point_rainfall) + 1):n())\n\n\ntrain_amk_rainfall\n\n# A tsibble: 125 x 3 [1W]\n# Key:       Station [1]\n   Station    year_week total_rainfall\n   &lt;chr&gt;         &lt;week&gt;          &lt;dbl&gt;\n 1 Ang Mo Kio  2020 W53          214  \n 2 Ang Mo Kio  2021 W01          168. \n 3 Ang Mo Kio  2021 W02           16.2\n 4 Ang Mo Kio  2021 W03           56.8\n 5 Ang Mo Kio  2021 W04           32.4\n 6 Ang Mo Kio  2021 W05            0  \n 7 Ang Mo Kio  2021 W06           17.2\n 8 Ang Mo Kio  2021 W07            0.2\n 9 Ang Mo Kio  2021 W08            0.6\n10 Ang Mo Kio  2021 W09           18.6\n# ℹ 115 more rows\n\n\n\n# Correct approach for defining multiple models:\ntrain_amk_rainfall_fit &lt;- train_amk_rainfall %&gt;%\n  model(\n    stla = decomposition_model(\n      STL(total_rainfall ~ season(window = \"periodic\")),\n      ARIMA(season_adjust)\n    ),\n    \n    # stlsn = decomposition_model(\n    #   STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n    #   SNAIVE(season_adjust)\n    # ),\n    \n      stln = decomposition_model(\n      STL(total_rainfall ~ season(window = \"periodic\")),\n      NAIVE(season_adjust)\n    ),\n    \n    sn = SNAIVE(total_rainfall ~ lag(\"year\")\n    ),\n    \n    # holtwinters = ETS(total_rainfall ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n    # ),\n\nstlets = decomposition_model(\n    STL(total_rainfall), \n    ETS(season_adjust ~ season(\"N\")\n        )\n    )\n)\n    \n\n# Forecasting\ntrain_amk_rainfall_fc &lt;- forecast(train_amk_rainfall_fit, h = \"20 weeks\")\n\n# Plotting the forecasts\nautoplot(weather_tsbl_week_rainfall, total_rainfall) +\n  autolayer(train_amk_rainfall_fc, series = \"Forecast\", level = NULL) +\n  labs(title = \"Forecast for weekly rainfall for Ang Mo Kio\",\n       x = \"\", y = \"\") +\n  theme_minimal() \n\n\n\n\n\naccuracy_metrics &lt;-\naccuracy(train_amk_rainfall_fc, weather_tsbl_week_rainfall) %&gt;%\n  arrange(.model) %&gt;%\n  select(.model, .type, RMSE, MAE, MAPE, MASE)\n\n\n# Assuming 'accuracy_metrics' is your tibble\naccuracy_metrics_formatted &lt;- accuracy_metrics %&gt;%\n  mutate(across(c(RMSE, MAE, MAPE, MASE), round, 2))\n\n# Now, convert this formatted tibble to a nicely formatted table\naccuracy_metrics_formatted %&gt;%\n  kable(\"html\", digits = 2) %&gt;%  # \"html\" for HTML output, use \"latex\" for PDF output\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n.model\n.type\nRMSE\nMAE\nMAPE\nMASE\n\n\n\n\nsn\nTest\n50.84\n44.10\n1334.83\n0.85\n\n\nstla\nTest\n54.04\n45.99\n1244.66\n0.89\n\n\nstlets\nTest\n53.89\n46.10\n1242.97\n0.89\n\n\nstln\nTest\n61.76\n44.98\n508.71\n0.87"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#section-1-time-series-exploration",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#section-1-time-series-exploration",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "4 Section 1: Time Series Exploration",
    "text": "4 Section 1: Time Series Exploration\nIn this section, users can interactively explore time series data using a line graph equipped with a time slider. They have the option to select specific stations, variables, and time periods for analysis.\n\n# user defined parameter\nselected_stations &lt;- c(\"Ang Mo Kio\", \"Changi\", \"Tai Seng\") \nvariable_temp &lt;- \"Minimum Temperature (°C)\"\nvariable_rain &lt;- \"Daily Rainfall Total (mm)\"\nstart_date &lt;- \"2021-01-01\"\nend_date &lt;-  \"2023-12-31\"\n\n\n4.1 Panel 1a: line graph\n\nDaily viewWeekly: avarage TemperatureRainfall, weekly total\n\n\n\n# Filter data based on period\nfiltered_period &lt;- weather_tsbl %&gt;%\n  filter_index(start_date ~ end_date)\n\n# Filter period-filtered data based on multiple selected stations\nfiltered_period_mstn &lt;- filtered_period  %&gt;%\n  filter(Station %in% selected_stations) \n\n# Generate the plot based on filtered data\nplot_ly(filtered_period_mstn, \n             x = ~Date, \n             y = as.formula(paste0(\"~`\", variable_temp, \"`\")), \n             type = 'scatter', \n             mode = 'lines', \n             color = ~Station,\n             hoverinfo = 'text',\n             text = ~paste(\"&lt;b&gt;Station:&lt;/b&gt;\", Station,\n                           \"&lt;br&gt;&lt;b&gt;Date:&lt;/b&gt;\", Date,\n                           \"&lt;br&gt;&lt;b&gt;\", variable_temp, \":&lt;/b&gt;\", filtered_period_mstn[[variable_temp]])) %&gt;%\n  layout(title = paste(variable_temp, \"by Station\"),\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"\")) %&gt;%\n\n# Display the plot\n layout(xaxis = list(rangeslider = list(type = \"date\"))) \n\n\n\n\n\n\n\nAverage of the following variables: “Mean Temperature (°C)”, “Minimum Temperature (°C)”, “Maximum Temperature (°C)”\n\n# Summarise period-filtered temperature data based on weekly average\nsummarised_temp &lt;- filtered_period %&gt;%\n  group_by_key() %&gt;%\n  index_by(year_week = ~ yearweek(.)) %&gt;%\n  summarise(\n    avg_temp = round(mean(.data[[variable_temp]]),2), na.rm = TRUE)\n\n# Filter summarised temperature data based on multiple selected stations\nsummarised_temp_mstn &lt;- summarised_temp  %&gt;%\n  filter(Station %in% selected_stations) \n\n# Convert the year_week result to the first day of the week for plotting\nsummarised_temp_mstn &lt;- summarised_temp_mstn %&gt;%\n  mutate(week_start_date = floor_date(as.Date(year_week), unit = \"week\"))\n\n# Generate the plot based on filtered data\nplot_ly(summarised_temp_mstn, x = ~week_start_date, y = ~avg_temp,\n             type = 'scatter', mode = 'lines', color = ~Station,\n             hoverinfo = 'text',\n             text = ~paste(\"&lt;b&gt;Station:&lt;/b&gt;\", Station, \n                           \"&lt;br&gt;&lt;b&gt;Week Starting:&lt;/b&gt;\", week_start_date, \n                           \"&lt;br&gt;&lt;b&gt;Average\", variable_temp, \":&lt;/b&gt;\", avg_temp)) %&gt;%\n\n  layout(title = paste(\"Weekly Average\", variable_temp, \"by Station\"),\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"\")) %&gt;%\n  \n  # Display the plot\n  layout(xaxis = list(rangeslider = list(type = \"date\"))) \n\n\n\n\n\n\n\nTotal of “Daily Rainfall Total (mm)”\n\n# Summarise period-filtered rainfall data based on weekly total\nsummarised_rain &lt;- filtered_period %&gt;%\n  group_by_key() %&gt;%\n  index_by(year_week = ~ yearweek(.)) %&gt;%\n  summarise(\n    total_rainfall = sum(.data[[variable_rain]]), na.rm = TRUE)\n\n# Filter summarised rainfall data based on multiple selected stations\nsummarised_rain_mstn &lt;- summarised_rain  %&gt;%\n  filter(Station %in% selected_stations) \n\n# Convert the year_week result to the first day of the week for plotting\nsummarised_rain_mstn &lt;- summarised_rain_mstn %&gt;%\n  mutate(week_start_date = floor_date(as.Date(year_week), unit = \"week\"))\n\n# Generate the plot based on filtered data\nplot_ly(summarised_rain_mstn, x = ~week_start_date, y = ~total_rainfall,\n             type = 'scatter', mode = 'lines', color = ~Station,\n             hoverinfo = 'text',\n             text = ~paste(\"&lt;b&gt;Station:&lt;/b&gt;\", Station, \n                           \"&lt;br&gt;&lt;b&gt;Week Starting:&lt;/b&gt;\", week_start_date, \n                           \"&lt;br&gt;&lt;b&gt;Total rainfall :&lt;/b&gt;\", total_rainfall, \"mm\")) %&gt;%\n\n  layout(title = paste(\"Weekly\", variable_rain, \"by Station\"),\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"\")) %&gt;% \n  \n  # Display the plot\n  layout(xaxis = list(rangeslider = list(type = \"date\"))) \n\n\n\n\n\n\n\n\n\n\n4.2 Panel 1b: ACF & PACF\nACF(Autocorrelation function) measures the linear relationship between lagged values of a time series while PACF (Partial Autocorrelation Function) measures the correlation between observations with the effect of the intermediate observations removed. In this section, users can view the ACF and PACF to analyze the time-dependent characteristics of the selected time series data.\nUsers can choose a single station to generate the ACF and PACF plot. For the purpose of this exercise, we demonstrate using “Ang Mo Kio” station.\n\nsingle_station &lt;- \"Ang Mo Kio\"\n\n\nACF, dailyPACF, dailyACF, weekly average temperaturePACF, weekly total rainfall\n\n\n\nfiltered_period_sstn &lt;- filtered_period  %&gt;%\n  filter(Station %in% single_station) \n\nACF &lt;- filtered_period_sstn %&gt;%\n  ACF(filtered_period_sstn[[variable_temp]], lag_max = 100) %&gt;%\n  autoplot() +\n  labs(title = paste(\"ACF plot of\", variable_temp, \"for\", single_station)) +\n  theme_minimal()\n\nggplotly(ACF)\n\n\n\n\n\n\n\n\nfiltered_period_sstn &lt;- filtered_period  %&gt;%\n  filter(Station %in% single_station) \n\nPACF &lt;- filtered_period_sstn %&gt;%\n  PACF(filtered_period_sstn[[variable_temp]], lag_max = 100) %&gt;%\n  autoplot() +\n  labs(title = paste(\"PACF plot of\", variable_temp, \"for\", single_station)) +\n  theme_minimal()\n\nggplotly(PACF)\n\n\n\n\n\n\n\n\nsummarised_temp_sstn &lt;- summarised_temp  %&gt;%\n  filter(Station %in% single_station) \n\n\nACF &lt;- summarised_temp_sstn %&gt;%\n  ACF(avg_temp, lag_max = 50) %&gt;%\n  autoplot() +\n  labs(title = paste(\"ACF plot of\", variable_temp, \"for\", single_station)) +\n  theme_minimal()\n\nggplotly(ACF)\n\n\n\n\n\n\n\n\nsummarised_rain_sstn &lt;- summarised_rain  %&gt;%\n  filter(Station %in% single_station) \n\n\nPACF &lt;- summarised_rain_sstn %&gt;%\n  PACF(total_rainfall, lag_max = 50) %&gt;%\n  autoplot() +\n  labs(title = paste(\"ACF plot of Weekly\", variable_rain, \"for\", single_station)) +\n  theme_minimal()\n\nggplotly(PACF)\n\n\n\n\n\n\n\n\n\n\n4.3 UI design"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#section-2-time-series-decomposition",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#section-2-time-series-decomposition",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "5 Section 2: Time Series Decomposition",
    "text": "5 Section 2: Time Series Decomposition\nThis section introduces the STL method, a versatile and robust method that breaks down time series data into trend, seasonality, and remainder components.\nSTL is an acronym for “Seasonal and Trend decomposition using Loess”, while loess is a method for estimating nonlinear relationships. We will use STL to uncover deeper insights into our data, highlighting its importance in understanding and predicting trends.\nBelow are the sample plot of STL decomposition using different tuning parameter.\n\n5.1 Panel 2: STL Decomposition analysis.\n\nstl_formula_default &lt;- STL(`Mean Temperature (°C)`)\n\nstl_formula_tuned &lt;- STL(`Mean Temperature (°C)`\n                         ~ trend(window = 20) +        \n                           ~  season(window = 20))\n\n\n# STL decomposition plot\na &lt;- filtered_period_sstn %&gt;%\n  model(stl_formula_default) %&gt;%\n  components() %&gt;%\n  autoplot()\n\nggplotly(a) %&gt;% layout(width = 700, height = 700, \n                      plot_bgcolor=\"#edf2f7\")\n\n\n\n\n\n\n# STL decomposition plot\nb &lt;- filtered_period_sstn %&gt;%\n  model(stl_formula_tuned) %&gt;%\n  components() %&gt;%\n  autoplot()\n\nggplotly(b) %&gt;% layout(width = 700, height = 700, \n                      plot_bgcolor=\"#edf2f7\")\n\n\n\n\n\nThe STL decomposition plot consist of four panel. The bottom four panel shows breakdown of three components of STL, namely trend, seasonality, and remainder. These components can be added together to reconstruct the data shown in the top panel. The remainder component shown in the bottom panel is what is left over when the seasonal and trend-cycle components have been subtracted from the data.\n\n\n5.2 UI design"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#section-3-time-series-forecasting",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#section-3-time-series-forecasting",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "6 Section 3: Time Series Forecasting",
    "text": "6 Section 3: Time Series Forecasting\n\n6.1 Split data into training and testing\n\n# Define the split point; for example, keeping the first 80% of rows for training\nsplit_point &lt;- nrow(filtered_period_sstn) * 0.8\n\n# Create the training dataset (first 80% of the data)\ntrain_daily &lt;- filtered_period_sstn %&gt;% \n  slice(1:floor(split_point))\n\n# Create the test dataset (remaining 20% of the data)\ntest_daily &lt;- filtered_period_sstn %&gt;% \n  slice((floor(split_point) + 1):n())\n\n\n\n6.2 Create and fit multiple model to tesing set\nWe observed that our data exhibits seasonal patterns, and hence we select the models specifically designed to handle such seasonal variations.\n\n\nCode\n#Double check this part on HoltWinters using different packages\n\nts_train &lt;- ts(data = train_daily$`Mean Temperature (°C)`,\n                start = c(2021,01),\n                frequency = 365)\nautoplot(ts_train)\n\n\nts_test &lt;- ts(data = train_daily$`Mean Temperature (°C)`,\n                start = c(2023,05,27),\n                frequency = 365)\nautoplot(ts_test)\n\nclimate_hw &lt;- HoltWinters(ts_train,seasonal = \"additive\")\nclimate_forecast &lt;- forecast(climate_hw, h=114)\n\n\n\ntrain_daily_fit &lt;- train_daily %&gt;%\n  model(\n    # naïve forecast of the seasonally adjusted data\n    STLNaive = decomposition_model(stl_formula_default,\n                               NAIVE(season_adjust)),\n    \n    # auto arima forecast of the seasonally adjusted data\n    STLArima = decomposition_model(stl_formula_default, ARIMA(season_adjust)),\n\n    # Exponential Smoothing forecast of the seasonally adjusted data\n    STLETS = decomposition_model(stl_formula_default, ETS(season_adjust ~ season(\"N\"))),\n    \n    # Holt-Winters’ additive method\n    HoltWinters = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n    \n    # sn = SNAIVE(`Mean Temperature (°C)` ~ lag(\"year\"))\n  )\n\n# Forecasting\ntrain_daily_fc &lt;- forecast(train_daily_fit, h = \"180 days\")\n\n# Plotting the forecasts\nc &lt;- autoplot(train_daily, `Mean Temperature (°C)`) +\n  autolayer(train_daily_fc, level = NULL) +\n  labs(title = \"Forecast of mean temperature for Ang Mo Kio\",\n       x = \"\", y = \"\") +\n  theme_minimal() \n\nggplotly(c, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\nd &lt;- autoplot(test_daily, `Mean Temperature (°C)`, series = \"Test Data\") +\n  autolayer(train_daily_fc, series = \"Forecast\", level = NULL) +\n  labs(title = \"Testing data vs Forecast\",\n       x = \"\", y = \"\") +\n  theme_minimal()\n\nggplotly(d, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\n\n6.3 Testing set forcast & Accuracy Evaluation\n\naccuracy_metrics &lt;- accuracy(train_daily_fc, test_daily) %&gt;%\n    arrange(.model) %&gt;%\n  select(.model, .type, RMSE, MAE, MAPE, MASE) %&gt;%\n  mutate(across(c(RMSE, MAE, MAPE, MASE), round, 2))\n\ndatatable(accuracy_metrics, \n          class= \"hover\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))\n\n\n\n\n\n\n\n\n6.4 Refit to Full Dataset & Forecast Forward\n\n# Refit models to the full dataset\nfull_fit &lt;- filtered_period_sstn %&gt;%\n  model(\n    # naïve forecast of the seasonally adjusted data\n    STLNaive = decomposition_model(stl_formula_default,\n                               NAIVE(season_adjust)),\n    \n    # auto arima forecast of the seasonally adjusted data\n    STLArima = decomposition_model(stl_formula_default, ARIMA(season_adjust)),\n\n    # Exponential Smoothing forecast of the seasonally adjusted data\n    STLETS = decomposition_model(stl_formula_default, ETS(season_adjust ~ season(\"N\"))),\n    \n    # Holt-Winters’ additive method\n    HoltWinters = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n    \n    # sn = SNAIVE(`Mean Temperature (°C)` ~ lag(\"year\"))\n  )\n\n\n# Forecasting forward\nfuture_horizon &lt;- \"10 days\" # Adjust this to your forecast needs\nfull_forecast &lt;- forecast(full_fit, h = future_horizon)\nfull_forecast\n\n# A fable: 40 x 5 [1D]\n# Key:     Station, .model [4]\n   Station    .model   Date       `Mean Temperature (°C)` .mean\n   &lt;chr&gt;      &lt;chr&gt;    &lt;date&gt;                      &lt;dist&gt; &lt;dbl&gt;\n 1 Ang Mo Kio STLNaive 2024-01-01             N(26, 0.61)  25.7\n 2 Ang Mo Kio STLNaive 2024-01-02              N(25, 1.2)  24.7\n 3 Ang Mo Kio STLNaive 2024-01-03              N(26, 1.8)  25.8\n 4 Ang Mo Kio STLNaive 2024-01-04              N(26, 2.4)  25.7\n 5 Ang Mo Kio STLNaive 2024-01-05                N(28, 3)  27.7\n 6 Ang Mo Kio STLNaive 2024-01-06              N(28, 3.6)  28.1\n 7 Ang Mo Kio STLNaive 2024-01-07              N(27, 4.2)  27.0\n 8 Ang Mo Kio STLNaive 2024-01-08              N(27, 4.8)  27.4\n 9 Ang Mo Kio STLNaive 2024-01-09              N(27, 5.4)  26.9\n10 Ang Mo Kio STLNaive 2024-01-10                N(26, 6)  26.3\n# ℹ 30 more rows\n\n\n\ndf_forecast &lt;- as_tibble(full_forecast)\ndatatable(df_forecast)\n\n\n# Plotting the forecasts along with the full dataset\ne &lt;- autoplot(filtered_period_sstn, `Mean Temperature (°C)`) +\n  autolayer(full_forecast, series = \"Forecast\", level = NULL) +\n  labs(title = \"Full Dataset Forecast for Mean Temperature in Ang Mo Kio\",\n       x = \"\", y = \"\") +\n  theme_minimal()\n\nggplotly(e, tooltip = c(\"x\", \"y\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#kiv-using-weekly-rainfall",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#kiv-using-weekly-rainfall",
    "title": "Take-home Exercise 4 (using Fable packages)",
    "section": "7 KIV: Using weekly rainfall",
    "text": "7 KIV: Using weekly rainfall\n\nweather_tsbl_week_rainfall &lt;- weather_tsbl %&gt;%\n group_by_key() %&gt;%\n  index_by(year_week = ~ yearweek(.)) %&gt;%\n  filter(Station == \"Ang Mo Kio\") %&gt;%\n    summarise(\n    total_rainfall = sum(`Daily Rainfall Total (mm)`, na.rm = TRUE)\n  )\n\nweather_tsbl_week_rainfall\n\n# A tsibble: 157 x 3 [1W]\n# Key:       Station [1]\n   Station    year_week total_rainfall\n   &lt;chr&gt;         &lt;week&gt;          &lt;dbl&gt;\n 1 Ang Mo Kio  2020 W53          214  \n 2 Ang Mo Kio  2021 W01          168. \n 3 Ang Mo Kio  2021 W02           16.2\n 4 Ang Mo Kio  2021 W03           56.8\n 5 Ang Mo Kio  2021 W04           32.4\n 6 Ang Mo Kio  2021 W05            0  \n 7 Ang Mo Kio  2021 W06           17.2\n 8 Ang Mo Kio  2021 W07            0.2\n 9 Ang Mo Kio  2021 W08            0.6\n10 Ang Mo Kio  2021 W09           18.6\n# ℹ 147 more rows\n\n\n\nggplot_obj_rainfall &lt;- weather_tsbl_week_rainfall %&gt;%\n  ggplot(aes(x = year_week, y = total_rainfall)) +\n  # geom_point() +\n  geom_line(colour = \"#2c3e50\",\n            linewidth = 0.5) +\n  labs(title = \"Total weekly rainfall for Ang Mo Kio\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal()\n\n# Convert ggplot object to plotly for interactivity\nggplot_obj_rainfall &lt;- ggplotly(ggplot_obj_rainfall) %&gt;%\n  layout(xaxis = list(rangeslider = list(type = \"date\")))\nggplot_obj_rainfall\n\n\n\n\n\n\nweather_tsbl_week_rainfall %&gt;%\n  ACF(`total_rainfall`, lag_max = 52) %&gt;%\n  autoplot() +\n  labs(title = \"ACF plot of total monthly rainfall of Ang Mo Kio\")\n\n\n\n\n\nweather_tsbl_week_rainfall %&gt;%\n  PACF(`total_rainfall`, lag_max = 52) %&gt;%\n  autoplot() +\n  labs(title = \"PACF plot of total monthly rainfall of Ang Mo Kio\")\n\n\n\n\n\n# Define the split point; for example, keeping the first 80% of rows for training\nsplit_point_rainfall &lt;- nrow(weather_tsbl_week_rainfall) * 0.8\n\n# Create the training dataset (first 80% of the data)\ntrain_amk_rainfall &lt;- weather_tsbl_week_rainfall %&gt;% \n  slice(1:floor(split_point_rainfall))\n\n# Create the test dataset (remaining 20% of the data)\ntest_amk_rainfall &lt;- weather_tsbl_week_rainfall %&gt;% \n  slice((floor(split_point_rainfall) + 1):n())\n\n\ntrain_amk_rainfall\n\n# A tsibble: 125 x 3 [1W]\n# Key:       Station [1]\n   Station    year_week total_rainfall\n   &lt;chr&gt;         &lt;week&gt;          &lt;dbl&gt;\n 1 Ang Mo Kio  2020 W53          214  \n 2 Ang Mo Kio  2021 W01          168. \n 3 Ang Mo Kio  2021 W02           16.2\n 4 Ang Mo Kio  2021 W03           56.8\n 5 Ang Mo Kio  2021 W04           32.4\n 6 Ang Mo Kio  2021 W05            0  \n 7 Ang Mo Kio  2021 W06           17.2\n 8 Ang Mo Kio  2021 W07            0.2\n 9 Ang Mo Kio  2021 W08            0.6\n10 Ang Mo Kio  2021 W09           18.6\n# ℹ 115 more rows\n\n\n\n# Correct approach for defining multiple models:\ntrain_amk_rainfall_fit &lt;- train_amk_rainfall %&gt;%\n  model(\n    stla = decomposition_model(\n      STL(total_rainfall ~ season(window = \"periodic\")),\n      ARIMA(season_adjust)\n    ),\n    \n    # stlsn = decomposition_model(\n    #   STL(`Mean Temperature (°C)` ~ season(window = \"periodic\")),\n    #   SNAIVE(season_adjust)\n    # ),\n    \n      stln = decomposition_model(\n      STL(total_rainfall ~ season(window = \"periodic\")),\n      NAIVE(season_adjust)\n    ),\n    \n    sn = SNAIVE(total_rainfall ~ lag(\"year\")\n    ),\n    \n    # holtwinters = ETS(total_rainfall ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n    # ),\n\nstlets = decomposition_model(\n    STL(total_rainfall), \n    ETS(season_adjust ~ season(\"N\")\n        )\n    )\n)\n    \n\n# Forecasting\ntrain_amk_rainfall_fc &lt;- forecast(train_amk_rainfall_fit, h = \"20 weeks\")\n\n# Plotting the forecasts\nautoplot(weather_tsbl_week_rainfall, total_rainfall) +\n  autolayer(train_amk_rainfall_fc, series = \"Forecast\", level = NULL) +\n  labs(title = \"Forecast for weekly rainfall for Ang Mo Kio\",\n       x = \"\", y = \"\") +\n  theme_minimal() \n\n\n\n\n\naccuracy_metrics &lt;-\naccuracy(train_amk_rainfall_fc, weather_tsbl_week_rainfall) %&gt;%\n  arrange(.model) %&gt;%\n  select(.model, .type, RMSE, MAE, MAPE, MASE)\n\n\n# Assuming 'accuracy_metrics' is your tibble\naccuracy_metrics_formatted &lt;- accuracy_metrics %&gt;%\n  mutate(across(c(RMSE, MAE, MAPE, MASE), round, 2))\n\n# Now, convert this formatted tibble to a nicely formatted table\naccuracy_metrics_formatted %&gt;%\n  kable(\"html\", digits = 2) %&gt;%  # \"html\" for HTML output, use \"latex\" for PDF output\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n.model\n.type\nRMSE\nMAE\nMAPE\nMASE\n\n\n\n\nsn\nTest\n50.84\n44.10\n1334.83\n0.85\n\n\nstla\nTest\n54.04\n45.99\n1244.66\n0.89\n\n\nstlets\nTest\n53.89\n46.10\n1242.97\n0.89\n\n\nstln\nTest\n61.76\n44.98\n508.71\n0.87"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#testing-using-different-stl",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#testing-using-different-stl",
    "title": "Take-home Exercise 4 (using Fable packages)",
    "section": "8 Testing using different STL",
    "text": "8 Testing using different STL\n\n# stl_formula &lt;- STL(`Mean Temperature (°C)`~ trend(window = 7) +\n#                                             season(window = \"periodic\"))\nstl_formula &lt;- STL(`Mean Temperature (°C)`\n                   # ~ trend(window = 7) +\n                   ~  season(window = 20)\n                   # , robust = TRUE\n                   )\nfiltered_period_sstn %&gt;%\n  model(stl_formula) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\nfeat_stl\n\ntemperature_ts &lt;- ts(filtered_period_sstn$`Mean Temperature (°C)`, frequency = 12) # Example frequency\n\n# Apply STL decomposition and feature extraction\nfeatures &lt;- feat_stl(temperature_ts, .period = 12, s.window = 11)\nfeatures\n\n      trend_strength seasonal_strength_12     seasonal_peak_12 \n        6.714539e-01         2.241765e-01         3.000000e+00 \n  seasonal_trough_12            spikiness            linearity \n        1.100000e+01         5.196100e-07         3.661789e+00 \n           curvature           stl_e_acf1          stl_e_acf10 \n       -6.682912e-01         1.702629e-01         1.053648e-01 \n\n\n\nfiltered_period_sstn %&gt;%features(`Mean Temperature (°C)`, feat_stl)\n\n# A tibble: 1 × 10\n  Station    trend_strength seasonal_strength_week seasonal_peak_week\n  &lt;chr&gt;               &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n1 Ang Mo Kio          0.743                  0.217                  0\n# ℹ 6 more variables: seasonal_trough_week &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;\n\n\n\n# Correct approach for defining multiple models:\ntrain_daily_fit &lt;- train_daily %&gt;%\n  model(\n    stla = decomposition_model(\n      stl_formula,\n      ARIMA(season_adjust)\n    ),\n\n    \n      stln = decomposition_model(\n      stl_formula,\n      NAIVE(season_adjust)\n    ),\n    \n    sn = SNAIVE(`Mean Temperature (°C)` ~ lag(\"year\")\n    ),\n    \n    holtwinters = ETS(`Mean Temperature (°C)` ~ error(\"A\") + trend(\"A\") + season(\"A\")),\n    \n    stlets = decomposition_model(\n    stl_formula,\n    ETS(season_adjust ~ season(\"N\")\n        ))\n  )\n\n# Forecasting\ntrain_daily_fc &lt;- forecast(train_daily_fit, h = \"6 months\")\n\n# Plotting the forecasts\nc &lt;- autoplot(train_daily, `Mean Temperature (°C)`) +\n  autolayer(train_daily_fc, series = \"Forecast\", level = NULL) +\n  labs(title = \"Forecast for mean temperature for Ang Mo Kio\",\n       x = \"\", y = \"\") +\n  theme_minimal() \n\nggplotly(c)\n\n\n\n\n\n\nd &lt;- autoplot(test_daily, `Mean Temperature (°C)`, series = \"Test Data\") +\n  autolayer(train_daily_fc, series = \"Forecast\", level = NULL) +\n  theme_minimal()\n\nggplotly(d)\n\n\n\n\n\n\naccuracy_metrics &lt;- accuracy(train_daily_fc, test_daily) %&gt;%\n    arrange(.model) %&gt;%\n  select(.model, .type, RMSE, MAE, MAPE, MASE) %&gt;%\n  mutate(across(c(RMSE, MAE, MAPE, MASE), round, 2))\n\ndatatable(accuracy_metrics, \n          class= \"hover\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-weekly-rainfall",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-weekly-rainfall",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "7 Test: Using weekly rainfall",
    "text": "7 Test: Using weekly rainfall"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-weekly-temperature",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-weekly-temperature",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "8 Test: Using weekly temperature",
    "text": "8 Test: Using weekly temperature"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-different-stl-parameter",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-different-stl-parameter",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "9 Test: Using different STL parameter",
    "text": "9 Test: Using different STL parameter"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-different-time-period-1-year",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4_usingfable.html#test-using-different-time-period-1-year",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "10 Test: Using different Time Period (1 year)",
    "text": "10 Test: Using different Time Period (1 year)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the module of our proposed Shiny application and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nThe module that we are focusing on is the univariate forecasting of the data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#overview",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#overview",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the module of our proposed Shiny application and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nThe module that we are focusing on is the univariate forecasting of the data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#section-1-time-series-exploration",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#section-1-time-series-exploration",
    "title": "Take-home Exercise 4",
    "section": "4 Section 1: Time Series Exploration",
    "text": "4 Section 1: Time Series Exploration\nIn this section, users can interactively explore time series data using a line graph equipped with a time slider. They have the option to select specific stations, variables, and time periods for analysis.\n\n# user defined parameter\nselected_stations &lt;- c(\"Ang Mo Kio\", \"Changi\", \"Tai Seng\") \nvariable_temp &lt;- \"Minimum Temperature (°C)\"\nvariable_rain &lt;- \"Daily Rainfall Total (mm)\"\nstart_date &lt;- \"2021-01-01\"\nend_date &lt;-  \"2023-12-31\"\n\n\nDaily viewWeekly: avarage TemperatureRainfall, weekly total\n\n\n\n# Filter data based on period\nfiltered_period &lt;- weather_tsbl %&gt;%\n  filter_index(start_date ~ end_date)\n\n# Filter period-filtered data based on multiple selected stations\nfiltered_period_mstn &lt;- filtered_period  %&gt;%\n  filter(Station %in% selected_stations) \n\n# Generate the plot based on filtered data\nplot_ly(filtered_period_mstn, \n             x = ~Date, \n             y = as.formula(paste0(\"~`\", variable_temp, \"`\")), \n             type = 'scatter', \n             mode = 'lines', \n             color = ~Station,\n             hoverinfo = 'text',\n             text = ~paste(\"&lt;b&gt;Station:&lt;/b&gt;\", Station,\n                           \"&lt;br&gt;&lt;b&gt;Date:&lt;/b&gt;\", Date,\n                           \"&lt;br&gt;&lt;b&gt;\", variable_temp, \":&lt;/b&gt;\", filtered_period_mstn[[variable_temp]])) %&gt;%\n  layout(title = paste(variable_temp, \"by Station\"),\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"\")) %&gt;%\n\n# Display the plot\n layout(xaxis = list(rangeslider = list(type = \"date\"))) \n\n\n\n\n\n\n\nThe code chunk below plot line graph for weekly summarise view of the following variables: “Mean Temperature (°C)”, “Minimum Temperature (°C)”, “Maximum Temperature (°C)”\n\n# Summarise period-filtered temperature data based on weekly average\nsummarised_temp &lt;- filtered_period %&gt;%\n  group_by_key() %&gt;%\n  index_by(year_week = ~ yearweek(.)) %&gt;%\n  summarise(\n    avg_temp = round(mean(.data[[variable_temp]]),2), na.rm = TRUE)\n\n# Filter summarised temperature data based on multiple selected stations\nsummarised_temp_mstn &lt;- summarised_temp  %&gt;%\n  filter(Station %in% selected_stations) \n\n# Convert the year_week result to the first day of the week for plotting\nsummarised_temp_mstn &lt;- summarised_temp_mstn %&gt;%\n  mutate(week_start_date = floor_date(as.Date(year_week), unit = \"week\"))\n\n# Generate the plot based on filtered data\nplot_ly(summarised_temp_mstn, x = ~week_start_date, y = ~avg_temp,\n             type = 'scatter', mode = 'lines', color = ~Station,\n             hoverinfo = 'text',\n             text = ~paste(\"&lt;b&gt;Station:&lt;/b&gt;\", Station, \n                           \"&lt;br&gt;&lt;b&gt;Week Starting:&lt;/b&gt;\", week_start_date, \n                           \"&lt;br&gt;&lt;b&gt;Average\", variable_temp, \":&lt;/b&gt;\", avg_temp)) %&gt;%\n\n  layout(title = paste(\"Weekly Average\", variable_temp, \"by Station\"),\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"\")) %&gt;%\n  \n  # Display the plot\n  layout(xaxis = list(rangeslider = list(type = \"date\"))) \n\n\n\n\n\n\n\nThe code chunk below plot line graph for weekly summarise view of “Daily Rainfall Total (mm)”.\n\n# Summarise period-filtered rainfall data based on weekly total\nsummarised_rain &lt;- filtered_period %&gt;%\n  group_by_key() %&gt;%\n  index_by(year_week = ~ yearweek(.)) %&gt;%\n  summarise(\n    total_rainfall = sum(.data[[variable_rain]]), na.rm = TRUE)\n\n# Filter summarised rainfall data based on multiple selected stations\nsummarised_rain_mstn &lt;- summarised_rain  %&gt;%\n  filter(Station %in% selected_stations) \n\n# Convert the year_week result to the first day of the week for plotting\nsummarised_rain_mstn &lt;- summarised_rain_mstn %&gt;%\n  mutate(week_start_date = floor_date(as.Date(year_week), unit = \"week\"))\n\n# Generate the plot based on filtered data\nplot_ly(summarised_rain_mstn, x = ~week_start_date, y = ~total_rainfall,\n             type = 'scatter', mode = 'lines', color = ~Station,\n             hoverinfo = 'text',\n             text = ~paste(\"&lt;b&gt;Station:&lt;/b&gt;\", Station, \n                           \"&lt;br&gt;&lt;b&gt;Week Starting:&lt;/b&gt;\", week_start_date, \n                           \"&lt;br&gt;&lt;b&gt;Total rainfall :&lt;/b&gt;\", total_rainfall, \"mm\")) %&gt;%\n\n  layout(title = paste(\"Weekly\", variable_rain, \"by Station\"),\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"\")) %&gt;% \n  \n  # Display the plot\n  layout(xaxis = list(rangeslider = list(type = \"date\"))) \n\n\n\n\n\n\n\n\n\n4.1 UI design"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#section-2-time-series-decomposition",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#section-2-time-series-decomposition",
    "title": "Take-home Exercise 4",
    "section": "5 Section 2: Time Series Decomposition",
    "text": "5 Section 2: Time Series Decomposition\nThis section introduces the STL method, a versatile and robust method that breaks down time series data into trend, seasonality, and remainder components.\nSTL is an acronym for “Seasonal and Trend decomposition using Loess”, while loess is a method for estimating nonlinear relationships. We will use STL to uncover deeper insights into our data, highlighting its importance in understanding and predicting trends.\nBelow are the sample plot of STL decomposition using different tuning parameter.\n\n5.1 STL Decomposition analysis.\n\nsingle_station &lt;- \"Ang Mo Kio\"\n\nfiltered_period_sstn &lt;- filtered_period  %&gt;%\n  filter(Station %in% single_station) \n\n\nstl_formula_default &lt;- STL(`Mean Temperature (°C)`)\n\nstl_formula_tuned &lt;- STL(`Mean Temperature (°C)`\n                         ~ trend(window = 20) +        \n                           ~  season(window = 20))\n\n\nAuto STL decomposition plotManual STL decomposition plot\n\n\n\nstl_default &lt;- filtered_period_sstn %&gt;%\n  model(stl_formula_default) %&gt;%\n  components() \nstl_default\n\n# A dable: 1,095 x 9 [1D]\n# Key:     Station, .model [1]\n# :        Mean Temperature (°C) = trend + season_year + season_week +\n#   remainder\n   Station    .model         Date       Mean Temperature (°C…¹ trend season_week\n   &lt;chr&gt;      &lt;chr&gt;          &lt;date&gt;                      &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 Ang Mo Kio stl_formula_d… 2021-01-01                   24    27.6     -0.0267\n 2 Ang Mo Kio stl_formula_d… 2021-01-02                   23    27.6     -0.0949\n 3 Ang Mo Kio stl_formula_d… 2021-01-03                   23.9  27.6     -0.236 \n 4 Ang Mo Kio stl_formula_d… 2021-01-04                   25.1  27.6      0.200 \n 5 Ang Mo Kio stl_formula_d… 2021-01-05                   26.9  27.6      0.298 \n 6 Ang Mo Kio stl_formula_d… 2021-01-06                   26.9  27.6      0.145 \n 7 Ang Mo Kio stl_formula_d… 2021-01-07                   24.4  27.6     -0.294 \n 8 Ang Mo Kio stl_formula_d… 2021-01-08                   25.3  27.6     -0.0218\n 9 Ang Mo Kio stl_formula_d… 2021-01-09                   25.2  27.6     -0.0781\n10 Ang Mo Kio stl_formula_d… 2021-01-10                   23.7  27.6     -0.225 \n# ℹ 1,085 more rows\n# ℹ abbreviated name: ¹​`Mean Temperature (°C)`\n# ℹ 3 more variables: season_year &lt;dbl&gt;, remainder &lt;dbl&gt;, season_adjust &lt;dbl&gt;\n\n\n\nstl_default_tibble &lt;- as_tibble(stl_default) %&gt;%\n  select(Date, `Mean Temperature (°C)`, trend, season_week, season_year, remainder, season_adjust) %&gt;%\n  mutate(trend = round(trend, 2),\n         season_adjust = round(season_adjust, 2),\n         season_week = round(season_week, 4),\n         season_year = round(season_year, 4),\n         remainder = round(remainder, 4))\n\ndatatable(stl_default_tibble, \n          class= \"nowrap\", \n          rownames = FALSE, \n          filter = 'top',  # Enables filters at the top of each column\n          width=\"100%\", \n          options = list(pageLength = 10, scrollX = TRUE))\n\n\n\n\n\n\n\nplot_stl_default &lt;- stl_default %&gt;%\n  autoplot()\n\nggplotly(plot_stl_default) %&gt;% layout(width = 700, height = 700, \n                      plot_bgcolor=\"#edf2f7\")\n\n\n\n\n\n\n\n\nstl_tuned &lt;- filtered_period_sstn %&gt;%\n  model(stl_formula_tuned) %&gt;%\n  components() \nstl_default\n\n# A dable: 1,095 x 9 [1D]\n# Key:     Station, .model [1]\n# :        Mean Temperature (°C) = trend + season_year + season_week +\n#   remainder\n   Station    .model         Date       Mean Temperature (°C…¹ trend season_week\n   &lt;chr&gt;      &lt;chr&gt;          &lt;date&gt;                      &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 Ang Mo Kio stl_formula_d… 2021-01-01                   24    27.6     -0.0267\n 2 Ang Mo Kio stl_formula_d… 2021-01-02                   23    27.6     -0.0949\n 3 Ang Mo Kio stl_formula_d… 2021-01-03                   23.9  27.6     -0.236 \n 4 Ang Mo Kio stl_formula_d… 2021-01-04                   25.1  27.6      0.200 \n 5 Ang Mo Kio stl_formula_d… 2021-01-05                   26.9  27.6      0.298 \n 6 Ang Mo Kio stl_formula_d… 2021-01-06                   26.9  27.6      0.145 \n 7 Ang Mo Kio stl_formula_d… 2021-01-07                   24.4  27.6     -0.294 \n 8 Ang Mo Kio stl_formula_d… 2021-01-08                   25.3  27.6     -0.0218\n 9 Ang Mo Kio stl_formula_d… 2021-01-09                   25.2  27.6     -0.0781\n10 Ang Mo Kio stl_formula_d… 2021-01-10                   23.7  27.6     -0.225 \n# ℹ 1,085 more rows\n# ℹ abbreviated name: ¹​`Mean Temperature (°C)`\n# ℹ 3 more variables: season_year &lt;dbl&gt;, remainder &lt;dbl&gt;, season_adjust &lt;dbl&gt;\n\n\n\nstl_tuned_tibble &lt;- as_tibble(stl_tuned) %&gt;%\n  select(Date, `Mean Temperature (°C)`, trend, season_week, season_year, remainder, season_adjust) %&gt;%\n  mutate(trend = round(trend, 2),\n         season_adjust = round(season_adjust, 2),\n         season_week = round(season_week, 4),\n         season_year = round(season_year, 4),\n         remainder = round(remainder, 4))\n\ndatatable(stl_tuned_tibble, \n          class= \"nowrap\", \n          rownames = FALSE, \n          filter = 'top',  # Enables filters at the top of each column\n          width=\"100%\", \n          options = list(pageLength = 10, scrollX = TRUE))\n\n\n\n\n\n\n\nplot_stl_tuned &lt;- stl_tuned %&gt;%\n  autoplot()\n\nggplotly(plot_stl_tuned) %&gt;% layout(width = 700, height = 700, \n                      plot_bgcolor=\"#edf2f7\")\n\n\n\n\n\n\n\n\nThe STL decomposition plot consist of four panel. The bottom four panel shows breakdown of three components of STL, namely trend, seasonality, and remainder. These components can be added together to reconstruct the data shown in the top panel. The remainder component shown in the bottom panel is what is left over when the seasonal and trend-cycle components have been subtracted from the data.\n\n\n5.2 UI design"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#section-3-time-series-forecasting",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#section-3-time-series-forecasting",
    "title": "Take-home Exercise 4",
    "section": "6 Section 3: Time Series Forecasting",
    "text": "6 Section 3: Time Series Forecasting\n\n6.1 Split data into training and testing\n\n# Define the split point; for example, keeping the first 80% of rows for training\nsplit_point &lt;- nrow(filtered_period_sstn) * 0.8\n\n# Create the training dataset (first 80% of the data)\ntrain_daily &lt;- filtered_period_sstn %&gt;% \n  slice(1:floor(split_point))\n\n# Create the test dataset (remaining 20% of the data)\ntest_daily &lt;- filtered_period_sstn %&gt;% \n  slice((floor(split_point) + 1):n())\n\n\n\n6.2 Create and fit multiple model to tesing set\nWe observed that our data exhibits seasonal patterns, and hence we select the models specifically designed to handle such seasonal variations.\nAccording to Hyndman and Athanasopoulos (Forecasting: Principles and Practice, Chapter 5.7)\n\ntrain_daily_fit &lt;- train_daily %&gt;%\n  model(\n    # naïve forecast of the seasonally adjusted data\n    STLNaive = decomposition_model(stl_formula_default, NAIVE(season_adjust)),              \n    \n    # auto arima forecast of the seasonally adjusted data\n    STLArima = decomposition_model(stl_formula_default, ARIMA(season_adjust)),\n\n    # Exponential Smoothing forecast of the seasonally adjusted data\n    STLETS = decomposition_model(stl_formula_default, ETS(season_adjust ~ season(\"N\"))),\n    \n    # AUTO arima\n    AUTOARIMA = ARIMA(`Mean Temperature (°C)`),    \n    \n    # AUTO prophet\n    AUTOprophet = prophet(`Mean Temperature (°C)`),\n    \n    # Auto Exponential smoothing\n    AUTOETS = ETS(`Mean Temperature (°C)`)\n    \n  )\n\nforecast_horizon &lt;- nrow(test_daily)\n\n# Forecasting\ntrain_daily_fc &lt;- forecast(train_daily_fit, h = forecast_horizon)\n\n# Plotting the forecasts\nc &lt;- autoplot(train_daily, `Mean Temperature (°C)`) +\n  autolayer(train_daily_fc, level = NULL) +\n  labs(title = \"Forecast Validation for daily mean temperature of Ang Mo Kio Station\",\n       x = \"\", y = \"\") +\n  theme_minimal() \n\nggplotly(c, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\nd &lt;- autoplot(test_daily, `Mean Temperature (°C)`, series = \"Test Data\") +\n  autolayer(train_daily_fc, series = \"Forecast\", level = NULL) +\n  labs(title = \"Testing data vs Forecast\",\n       x = \"\", y = \"\") +\n  theme_minimal()\n\nggplotly(d, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\n\n6.3 Testing set forcast & Accuracy Evaluation\n\naccuracy_metrics &lt;- accuracy(train_daily_fc, test_daily) %&gt;%\n    arrange(.model) %&gt;%\n  select(.model, .type, RMSE, MAE, MAPE, MASE) %&gt;%\n  mutate(across(c(RMSE, MAE, MAPE, MASE), round, 2))\n\ndatatable(accuracy_metrics, \n          class= \"hover\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))\n\n\n\n\n\n\n\n\n6.4 Refit to Full Dataset & Forecast Forward\n\n# Refit models to the full dataset\nfull_fit &lt;- filtered_period_sstn %&gt;%\n  model(\n    # naïve forecast of the seasonally adjusted data\n    STLNaive = decomposition_model(stl_formula_default, NAIVE(season_adjust)),              \n    \n    # auto arima forecast of the seasonally adjusted data\n    STLArima = decomposition_model(stl_formula_default, ARIMA(season_adjust)),\n\n    # Exponential Smoothing forecast of the seasonally adjusted data\n    STLETS = decomposition_model(stl_formula_default, ETS(season_adjust ~ season(\"N\"))),\n    \n    # AUTO arima\n    AUTOARIMA = ARIMA(`Mean Temperature (°C)`),    \n    \n    # AUTO prophet\n    AUTOprophet = prophet(`Mean Temperature (°C)`),\n    \n    # Auto Exponential smoothing\n    AUTOETS = ETS(`Mean Temperature (°C)`)\n  )\n\n\n# Plotting the forecasts along with the full dataset\nfuture_horizon &lt;- \"1 month\" # Adjust this to your forecast needs\nfull_forecast &lt;- forecast(full_fit, h = future_horizon)\n\ne &lt;- autoplot(filtered_period_sstn, `Mean Temperature (°C)`) +\n  autolayer(full_forecast, series = \"Forecast\", level = NULL) +\n  labs(title = \"Future Forecast Plot for daily mean temperature of Ang Mo Kio Station\",\n       x = \"\", y = \"\") +\n  theme_minimal()\n\nggplotly(e, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\n# Table view\n\nfull_forecast_tibble &lt;- as_tibble(full_forecast)\n\n\ncol&lt;- full_forecast_tibble %&gt;% \n  select(.model, Date, .mean) %&gt;% \n  rename(Forecast = .mean) %&gt;%\n  mutate(Forecast = round(Forecast, 2),\n         .model = as.factor(.model))\n\n\ndatatable(col, \n          class= \"hover\", \n          rownames = FALSE, \n          filter = 'top',  # Enables filters at the top of each column\n          width=\"100%\", \n          options = list(pageLength = 5, scrollX = TRUE))\n\n\n\n\n\n\n\n\n6.5 UI design"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-weekly-rainfall",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-weekly-rainfall",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "7 Test: Using weekly rainfall",
    "text": "7 Test: Using weekly rainfall"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-weekly-temperature",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-weekly-temperature",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "8 Test: Using weekly temperature",
    "text": "8 Test: Using weekly temperature"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-different-stl-parameter",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-different-stl-parameter",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "9 Test: Using different STL parameter",
    "text": "9 Test: Using different STL parameter"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-different-time-period-1-year",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#test-using-different-time-period-1-year",
    "title": "Take-home Exercise 4 (Work-In-Progress)",
    "section": "10 Test: Using different Time Period (1 year)",
    "text": "10 Test: Using different Time Period (1 year)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#reference",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#reference",
    "title": "Take-home Exercise 4",
    "section": "7 Reference",
    "text": "7 Reference\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "title": "In-class Exercise 9",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph, layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n Back to top"
  }
]